{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc7af4a",
   "metadata": {},
   "source": [
    "# Evaluate Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b08726b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6199546b",
   "metadata": {},
   "source": [
    "#### Load the API key and libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639eb28e",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from src.Language_Evaluation import llm_language_evaluation\n",
    "from src.data_analysis import run_analysis\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed2010",
   "metadata": {
    "height": 30
   },
   "source": [
    "#### Load the Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb0f44e",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "PATH = 'data/Portuguese.csv'\n",
    "MODEL = \"Llama-2-13b\"\n",
    "TEMPERATURE = 0.7\n",
    "N_REPETITIONS = 10\n",
    "REASONING = False\n",
    "LANGUAGES = ['english', 'portuguese']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc4732a",
   "metadata": {},
   "source": [
    "#### Run The Experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4dd1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The model file 'Models/Llama-2-13b.gguf' already exists. Do you want to overwrite it? (yes/no):  No\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model installation aborted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from Models/Llama-2-13b.gguf (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q8_0     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:             blk.15.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:             blk.15.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:            blk.2.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.2.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.2.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:              blk.2.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:         blk.2.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.2.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:              blk.2.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:            blk.3.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.3.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:              blk.3.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:              blk.3.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:         blk.3.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.3.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:              blk.3.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:            blk.4.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.4.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:              blk.4.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:              blk.4.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:         blk.4.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:              blk.4.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:              blk.4.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:            blk.5.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:            blk.5.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:              blk.5.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:              blk.5.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:         blk.5.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:              blk.5.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:              blk.5.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:            blk.6.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:            blk.6.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:              blk.6.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:              blk.6.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:         blk.6.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:              blk.6.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:              blk.6.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:            blk.7.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.7.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.7.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.7.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:         blk.7.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:              blk.7.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:              blk.7.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:            blk.8.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:            blk.8.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:              blk.8.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:              blk.8.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:         blk.8.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:              blk.8.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:              blk.8.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:            blk.9.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:            blk.9.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:              blk.9.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:              blk.9.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:         blk.9.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:              blk.9.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:              blk.9.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:           blk.15.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:        blk.15.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.16.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.16.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.16.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.16.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.17.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:           blk.17.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:             blk.17.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:        blk.17.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:             blk.17.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.18.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:           blk.18.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:        blk.18.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:             blk.18.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.19.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:           blk.19.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:        blk.19.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:             blk.19.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.20.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:           blk.20.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:        blk.20.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:             blk.20.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.21.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:           blk.21.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:        blk.21.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:             blk.21.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.22.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:           blk.22.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:        blk.22.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:             blk.22.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.23.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:           blk.23.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:        blk.23.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:             blk.23.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.24.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:        blk.24.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:             blk.24.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.25.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:        blk.25.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:             blk.25.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.26.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:             blk.26.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:        blk.26.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:             blk.26.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.27.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:        blk.27.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:             blk.27.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.28.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:        blk.28.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:             blk.28.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.29.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:        blk.29.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:             blk.29.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:           blk.30.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:                    output.weight q8_0     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:           blk.32.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:           blk.32.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:             blk.32.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:        blk.32.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:             blk.32.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:           blk.33.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:           blk.33.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:             blk.33.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:        blk.33.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:             blk.33.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:           blk.34.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:           blk.34.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:             blk.34.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:        blk.34.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:             blk.34.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:           blk.35.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:           blk.35.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:             blk.35.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:        blk.35.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:             blk.35.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:           blk.36.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:           blk.36.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:             blk.36.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:        blk.36.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:             blk.36.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:           blk.37.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:           blk.37.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:             blk.37.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:        blk.37.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:             blk.37.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:           blk.38.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:           blk.38.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:             blk.38.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:        blk.38.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:             blk.38.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:           blk.39.ffn_down.weight q8_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:           blk.39.ffn_gate.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.ffn_up.weight q8_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:             blk.39.attn_k.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:        blk.39.attn_output.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.attn_q.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:             blk.39.attn_v.weight q8_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  18:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q8_0:  282 tensors\n",
      "llm_load_print_meta: format           = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = mostly Q8_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 12.88 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.12 MB\n",
      "llm_load_tensors: mem required  = 13189.98 MB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "llama_new_context_with_model: compute buffer total size = 80.88 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Question 1: \n",
      "Language: english\n",
      "Question: \n",
      "In which ocular region are caliciform cells physiologically found?\n",
      "a) Cornea.\n",
      "b) Corneoscleral limbus.\n",
      "c) Gray line.\n",
      "d) Semilunar fold.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.69 ms /    27 runs   (    0.58 ms per token,  1721.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3616.50 ms /   200 tokens (   18.08 ms per token,    55.30 tokens per second)\n",
      "llama_print_timings:        eval time =  3135.57 ms /    26 runs   (  120.60 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =  6831.15 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n",
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.21 ms /    21 runs   (    0.58 ms per token,  1719.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2546.53 ms /    21 runs   (  121.26 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  2606.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.78 ms /    36 runs   (    0.58 ms per token,  1732.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4414.04 ms /    36 runs   (  122.61 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  4517.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.51 ms /    27 runs   (    0.57 ms per token,  1740.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3271.69 ms /    27 runs   (  121.17 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3348.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1737.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   842.57 ms /     7 runs   (  120.37 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:       total time =   861.92 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.89 ms /    24 runs   (    0.58 ms per token,  1727.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2914.70 ms /    24 runs   (  121.45 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  2981.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.57 ms /    27 runs   (    0.58 ms per token,  1734.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3254.69 ms /    27 runs   (  120.54 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:       total time =  3331.07 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.99 ms /     7 runs   (    0.57 ms per token,  1753.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   839.41 ms /     7 runs   (  119.92 ms per token,     8.34 tokens per second)\n",
      "llama_print_timings:       total time =   859.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.36 ms /    25 runs   (    0.57 ms per token,  1740.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3032.74 ms /    25 runs   (  121.31 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3103.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.60 ms /    27 runs   (    0.58 ms per token,  1730.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3300.00 ms /    27 runs   (  122.22 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3375.93 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Em qual região ocular células caliciformes são fisiologicamente encontradas?\n",
      "a)Córnea.\n",
      "b)Limbo corneoescleral.\n",
      "c)Linha cinzenta.\n",
      "d)Prega semilunar.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     7.29 ms /    12 runs   (    0.61 ms per token,  1645.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1588.30 ms /    63 tokens (   25.21 ms per token,    39.66 tokens per second)\n",
      "llama_print_timings:        eval time =  1333.49 ms /    11 runs   (  121.23 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  2956.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1731.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   854.29 ms /     7 runs   (  122.04 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =   873.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1731.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   852.59 ms /     7 runs   (  121.80 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =   871.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.61 ms /    36 runs   (    0.57 ms per token,  1746.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4383.94 ms /    36 runs   (  121.78 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  4485.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n",
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1747.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   867.93 ms /     7 runs   (  123.99 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =   886.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1735.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   865.09 ms /     7 runs   (  123.58 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =   884.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1733.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   843.46 ms /     7 runs   (  120.49 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:       total time =   862.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.72 ms /    40 runs   (    0.57 ms per token,  1760.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4887.22 ms /    40 runs   (  122.18 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  4999.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.41 ms /    32 runs   (    0.58 ms per token,  1737.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3932.01 ms /    32 runs   (  122.88 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  4022.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.93 ms /    12 runs   (    0.58 ms per token,  1732.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1469.63 ms /    12 runs   (  122.47 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  1503.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 2: \n",
      "Language: english\n",
      "Question: \n",
      "Mark the alternative that best correlates the histological characteristics with the respective ocular tissues:\n",
      "\n",
      "I. Monolayer of cells tightly joined together by junctional complexes.\n",
      "II. Parallel and regular striations observed under optical microscopy, perpendicular to the epithelium.\n",
      "III. It contains bipolar cells, amacrine cells, horizontal cells and Muller cells.\n",
      "IV. It contains magnocellular, parvocellular and coniocellular cells.\n",
      "\n",
      "A. Photoreceptors.\n",
      "B. Retinal pigmented epithelium.\n",
      "C. Retinal ganglionic layer.\n",
      "D. Inner nuclear layer.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.25 ms /    39 runs   (    0.57 ms per token,  1752.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2497.29 ms /   158 tokens (   15.81 ms per token,    63.27 tokens per second)\n",
      "llama_print_timings:        eval time =  4650.78 ms /    38 runs   (  122.39 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  7259.80 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.13 ms /    39 runs   (    0.57 ms per token,  1762.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4758.35 ms /    39 runs   (  122.01 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  4869.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.12 ms /    39 runs   (    0.57 ms per token,  1762.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4763.55 ms /    39 runs   (  122.14 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  4875.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    90.25 ms /   157 runs   (    0.57 ms per token,  1739.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 19474.11 ms /   157 runs   (  124.04 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time = 19938.63 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.40 ms /    39 runs   (    0.57 ms per token,  1741.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4822.94 ms /    39 runs   (  123.67 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  4934.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    83.70 ms /   145 runs   (    0.58 ms per token,  1732.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 17839.76 ms /   145 runs   (  123.03 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 18267.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #6: \n",
      "{'response': 'B'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    56.12 ms /    96 runs   (    0.58 ms per token,  1710.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11788.21 ms /    96 runs   (  122.79 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 12068.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    61.61 ms /   107 runs   (    0.58 ms per token,  1736.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13165.58 ms /   107 runs   (  123.04 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 13523.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.20 ms /    39 runs   (    0.57 ms per token,  1756.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4757.38 ms /    39 runs   (  121.98 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  4869.24 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    33.91 ms /    58 runs   (    0.58 ms per token,  1710.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7219.95 ms /    58 runs   (  124.48 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =  7388.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Assinale a alternativa que melhor correlaciona as características histológicas com os respectivos tecidos oculares:\n",
      "\n",
      "I. Monocamada de células fortemente unidas por complexos juncionais.\n",
      "II. Estriações paralelas e regulares observadas à microscopia óptica, perpendiculares ao epitélio.\n",
      "III. Contém células bipolares, células amácrinas, células horizontais e células de Muller.\n",
      "IV. Contém células magnocelulares, parvocelulares e coniocelulares.\n",
      "\n",
      "A. Fotorreceptores.\n",
      "B. Epitélio pigmentado da pigmentado da retina.\n",
      "C. Camada ganglionar retiniana.\n",
      "D. Camada nuclear interna.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    37.37 ms /    64 runs   (    0.58 ms per token,  1712.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3810.76 ms /   196 tokens (   19.44 ms per token,    51.43 tokens per second)\n",
      "llama_print_timings:        eval time =  7840.10 ms /    63 runs   (  124.45 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time = 11838.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1746.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   855.53 ms /     7 runs   (  122.22 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =   875.57 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    99.05 ms /   171 runs   (    0.58 ms per token,  1726.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 21104.02 ms /   171 runs   (  123.42 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 21617.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.37 ms /    11 runs   (    0.58 ms per token,  1727.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1346.01 ms /    11 runs   (  122.36 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  1377.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    54.84 ms /    94 runs   (    0.58 ms per token,  1713.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11621.04 ms /    94 runs   (  123.63 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 11900.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    72.91 ms /   125 runs   (    0.58 ms per token,  1714.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 15410.08 ms /   125 runs   (  123.28 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 15782.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.05 ms /     7 runs   (    0.58 ms per token,  1729.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   855.08 ms /     7 runs   (  122.15 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =   875.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.48 ms /    41 runs   (    0.57 ms per token,  1746.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5012.41 ms /    41 runs   (  122.25 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  5129.92 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.13 ms /    28 runs   (    0.58 ms per token,  1736.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3474.30 ms /    28 runs   (  124.08 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  3554.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.09 ms /     7 runs   (    0.58 ms per token,  1712.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   856.60 ms /     7 runs   (  122.37 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =   876.86 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 3: \n",
      "Language: english\n",
      "Question: \n",
      "Order the three cell names found in the corneal epithelium, starting with the most superficial, followed by the intermediate and the deep.\n",
      "a) Flat, wing, basal.\n",
      "b) wing, basal, flat.\n",
      "c) Basal, flat, wing.\n",
      "d) wing, flat, basal.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.91 ms /    25 runs   (    0.56 ms per token,  1797.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1570.68 ms /    79 tokens (   19.88 ms per token,    50.30 tokens per second)\n",
      "llama_print_timings:        eval time =  2926.28 ms /    24 runs   (  121.93 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  4569.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n",
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    41.98 ms /    73 runs   (    0.58 ms per token,  1738.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8837.59 ms /    73 runs   (  121.06 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  9051.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    38.54 ms /    68 runs   (    0.57 ms per token,  1764.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8282.72 ms /    68 runs   (  121.80 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  8480.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.48 ms /    33 runs   (    0.56 ms per token,  1785.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4087.59 ms /    33 runs   (  123.87 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  4181.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.74 ms /    25 runs   (    0.55 ms per token,  1819.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3035.80 ms /    25 runs   (  121.43 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3105.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.67 ms /    25 runs   (    0.55 ms per token,  1829.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3034.73 ms /    25 runs   (  121.39 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3105.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.80 ms /    36 runs   (    0.55 ms per token,  1818.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4339.93 ms /    36 runs   (  120.55 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:       total time =  4442.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.48 ms /    35 runs   (    0.56 ms per token,  1796.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4255.24 ms /    35 runs   (  121.58 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  4354.02 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.54 ms /    25 runs   (    0.54 ms per token,  1846.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3064.23 ms /    25 runs   (  122.57 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3133.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.40 ms /    35 runs   (    0.55 ms per token,  1804.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4340.52 ms /    35 runs   (  124.01 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  4442.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Ordene as três denominações celulares encontradas no epitélio da córnea, iniciando pelo mais superficial, seguido do intermediário e do profundo.\n",
      "a)Plana, alada, basal.\n",
      "b)Alada, basal, plana.\n",
      "c)Basal, plana, alada.\n",
      "d)Alada, plana, basal.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    47.33 ms /    82 runs   (    0.58 ms per token,  1732.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1581.18 ms /    96 tokens (   16.47 ms per token,    60.71 tokens per second)\n",
      "llama_print_timings:        eval time =  9911.43 ms /    81 runs   (  122.36 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time = 11733.12 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.90 ms /    37 runs   (    0.56 ms per token,  1770.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4457.60 ms /    37 runs   (  120.48 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:       total time =  4562.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.86 ms /    28 runs   (    0.57 ms per token,  1765.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3393.68 ms /    28 runs   (  121.20 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3473.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.21 ms /    20 runs   (    0.56 ms per token,  1783.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2453.50 ms /    20 runs   (  122.68 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  2509.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.01 ms /    30 runs   (    0.57 ms per token,  1763.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3660.43 ms /    30 runs   (  122.01 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3744.82 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.42 ms /    36 runs   (    0.57 ms per token,  1762.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4434.41 ms /    36 runs   (  123.18 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  4537.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    53.77 ms /    94 runs   (    0.57 ms per token,  1748.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11485.01 ms /    94 runs   (  122.18 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time = 11756.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.36 ms /    20 runs   (    0.57 ms per token,  1761.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2452.06 ms /    20 runs   (  122.60 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  2508.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.56 ms /    29 runs   (    0.57 ms per token,  1751.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3527.72 ms /    29 runs   (  121.65 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3609.85 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   125.03 ms /   217 runs   (    0.58 ms per token,  1735.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 26535.61 ms /   217 runs   (  122.28 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time = 27188.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 4: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding Descemet's membrane of the cornea, it is correct to state:\n",
      "a) Endothelial cells do not participate in its formation.\n",
      "b) Its thickness in the adult is about 30 µm.\n",
      "c) Its most anterior portion is of embryonic origin.\n",
      "d) Its thickness reduces with age.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.43 ms /    24 runs   (    0.60 ms per token,  1663.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1910.71 ms /    83 tokens (   23.02 ms per token,    43.44 tokens per second)\n",
      "llama_print_timings:        eval time =  2801.20 ms /    23 runs   (  121.79 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  4782.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.02 ms /    40 runs   (    0.58 ms per token,  1737.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4846.27 ms /    40 runs   (  121.16 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  4960.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    38.95 ms /    68 runs   (    0.57 ms per token,  1745.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8226.88 ms /    68 runs   (  120.98 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  8423.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.35 ms /    25 runs   (    0.57 ms per token,  1742.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3064.12 ms /    25 runs   (  122.56 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3135.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.49 ms /    48 runs   (    0.57 ms per token,  1745.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5834.93 ms /    48 runs   (  121.56 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  5971.98 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n",
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.83 ms /    17 runs   (    0.58 ms per token,  1729.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2071.06 ms /    17 runs   (  121.83 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  2119.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.63 ms /    29 runs   (    0.57 ms per token,  1744.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3533.33 ms /    29 runs   (  121.84 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3615.75 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.77 ms /    24 runs   (    0.57 ms per token,  1743.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2898.96 ms /    24 runs   (  120.79 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  2967.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.43 ms /    48 runs   (    0.57 ms per token,  1749.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5887.82 ms /    48 runs   (  122.66 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  6024.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.02 ms /    40 runs   (    0.58 ms per token,  1737.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4855.83 ms /    40 runs   (  121.40 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  4970.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Sobre a membrana de Descemet da córnea, é correto afirmar:\n",
      "a)As células endoteliais não participam da sua formação.\n",
      "b)Sua espessura no adulto é de cerca de 30 µm.\n",
      "c)Sua porção mais anterior é de origem embrionária.\n",
      "d)Sua espessura reduz-se com a idade.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.46 ms /     6 runs   (    0.58 ms per token,  1734.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1845.33 ms /   103 tokens (   17.92 ms per token,    55.82 tokens per second)\n",
      "llama_print_timings:        eval time =   660.78 ms /     5 runs   (  132.16 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:       total time =  2523.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting respose to json: {:response:\"A\"}\n",
      "Generating new response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    33.63 ms /    58 runs   (    0.58 ms per token,  1724.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7099.03 ms /    58 runs   (  122.40 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  7269.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.94 ms /     7 runs   (    0.56 ms per token,  1774.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   841.96 ms /     7 runs   (  120.28 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:       total time =   861.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   122.16 ms /   211 runs   (    0.58 ms per token,  1727.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 25872.20 ms /   211 runs   (  122.62 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time = 26513.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.92 ms /     7 runs   (    0.56 ms per token,  1787.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   852.08 ms /     7 runs   (  121.73 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =   871.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.97 ms /     7 runs   (    0.57 ms per token,  1765.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   838.96 ms /     7 runs   (  119.85 ms per token,     8.34 tokens per second)\n",
      "llama_print_timings:       total time =   859.01 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.90 ms /     7 runs   (    0.56 ms per token,  1794.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   838.55 ms /     7 runs   (  119.79 ms per token,     8.35 tokens per second)\n",
      "llama_print_timings:       total time =   857.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n",
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.91 ms /     7 runs   (    0.56 ms per token,  1791.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   849.36 ms /     7 runs   (  121.34 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =   869.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.87 ms /     7 runs   (    0.55 ms per token,  1806.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   856.68 ms /     7 runs   (  122.38 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =   875.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.89 ms /     7 runs   (    0.56 ms per token,  1801.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   845.26 ms /     7 runs   (  120.75 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =   864.88 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    34.04 ms /    59 runs   (    0.58 ms per token,  1733.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7162.53 ms /    59 runs   (  121.40 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  7332.51 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 5: \n",
      "Language: english\n",
      "Question: \n",
      "About the lipidic layer of the lacrimal film, choose the right answer. a) its purpose is to stabilize the lacrimal film secreted by the meibomian and manz glans b) Cholesterol esters are within its components, and one of the functions is to delay the lacrimal film evaporation c) In meibomian gland dysfunction, the fusion point decrease, leading to stagnation d) can be evaluated by the lacrimal film break up time, keeping intact less than five seconds in healthy eyes.\n",
      "Test #0: \n",
      "{'response': 'B'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    37.05 ms /    64 runs   (    0.58 ms per token,  1727.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2549.81 ms /   129 tokens (   19.77 ms per token,    50.59 tokens per second)\n",
      "llama_print_timings:        eval time =  7778.50 ms /    63 runs   (  123.47 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 10514.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    36.87 ms /    64 runs   (    0.58 ms per token,  1735.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7789.21 ms /    64 runs   (  121.71 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  7973.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    49.51 ms /    77 runs   (    0.64 ms per token,  1555.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9542.95 ms /    77 runs   (  123.93 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  9776.83 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    38.22 ms /    60 runs   (    0.64 ms per token,  1569.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7390.86 ms /    60 runs   (  123.18 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  7573.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    40.80 ms /    61 runs   (    0.67 ms per token,  1495.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7435.95 ms /    61 runs   (  121.90 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  7624.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    41.20 ms /    61 runs   (    0.68 ms per token,  1480.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7424.91 ms /    61 runs   (  121.72 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  7619.80 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    64.67 ms /   107 runs   (    0.60 ms per token,  1654.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13093.62 ms /   107 runs   (  122.37 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time = 13417.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #7: \n",
      "{'response': 'B'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    35.33 ms /    61 runs   (    0.58 ms per token,  1726.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7512.68 ms /    61 runs   (  123.16 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  7688.76 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.84 ms /    17 runs   (    0.58 ms per token,  1727.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2082.93 ms /    17 runs   (  122.53 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  2132.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    35.22 ms /    61 runs   (    0.58 ms per token,  1731.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7493.10 ms /    61 runs   (  122.84 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  7671.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Sobre a camada lipídica do filme lacrimal, assinale a alternativa correta. a)Tem por propósito básico estabilizar o filme e é secretada pelas glândulas de Meibomius e de Manz.  b)Ésteres de colesterol estão entre seus componentes e uma de suas funções é retardar a evaporação do filme lacrimal. c)Na disfunção das glândulas de Meibomius, o ponto de fusão de sua secreção diminui, colaborando para a estagnação dessas substâncias. d)Pode ser avaliada pelo tempo de ruptura do filme lacrimal, permanecendo intacta por menos de cinco segundos em olhos saudáveis.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.16 ms /     7 runs   (    0.59 ms per token,  1681.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3175.39 ms /   183 tokens (   17.35 ms per token,    57.63 tokens per second)\n",
      "llama_print_timings:        eval time =   733.49 ms /     6 runs   (  122.25 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3929.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.94 ms /    31 runs   (    0.58 ms per token,  1728.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3815.52 ms /    31 runs   (  123.08 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  3905.58 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.12 ms /     7 runs   (    0.59 ms per token,  1699.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   856.29 ms /     7 runs   (  122.33 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =   876.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.12 ms /     7 runs   (    0.59 ms per token,  1701.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   858.69 ms /     7 runs   (  122.67 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =   878.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1746.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   865.06 ms /     7 runs   (  123.58 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =   885.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    99.79 ms /   165 runs   (    0.60 ms per token,  1653.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 20399.58 ms /   165 runs   (  123.63 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 21102.03 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.51 ms /     7 runs   (    0.64 ms per token,  1550.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   844.19 ms /     7 runs   (  120.60 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =   891.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.40 ms /     7 runs   (    0.63 ms per token,  1590.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   854.49 ms /     7 runs   (  122.07 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =   901.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   119.55 ms /   184 runs   (    0.65 ms per token,  1539.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 22588.27 ms /   184 runs   (  122.76 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 23874.51 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #9: \n",
      "{'response': 'A'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 6: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding the external ocular muscles, choose the correct answer. a) in the Hering's law, the innervation is equally distributed to the agosist and antagonist muscles, for a given ocular version. b) The persecutory movements will automatically occur, slowly, with the displacement of the fixation point c) In the sherrington's law, the contraction stimulus is distributed between the agonists of both eyes, so that it occur symmetrically. d) Saccadic movements are characterized by abrupq, uncoordinated, reflex movments.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.49 ms /     7 runs   (    0.64 ms per token,  1559.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   850.96 ms /     7 runs   (  121.57 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =   897.68 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   149.36 ms /   230 runs   (    0.65 ms per token,  1539.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2621.36 ms /   137 tokens (   19.13 ms per token,    52.26 tokens per second)\n",
      "llama_print_timings:        eval time = 28294.96 ms /   229 runs   (  123.56 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 32550.15 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    52.74 ms /    80 runs   (    0.66 ms per token,  1516.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9755.98 ms /    80 runs   (  121.95 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time = 10301.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    73.09 ms /   111 runs   (    0.66 ms per token,  1518.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13669.01 ms /   111 runs   (  123.14 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 14430.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    50.80 ms /    78 runs   (    0.65 ms per token,  1535.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9609.65 ms /    78 runs   (  123.20 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 10139.58 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.68 ms /    24 runs   (    0.65 ms per token,  1531.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2929.95 ms /    24 runs   (  122.08 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3090.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    71.19 ms /   109 runs   (    0.65 ms per token,  1531.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13429.68 ms /   109 runs   (  123.21 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 14176.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    40.26 ms /    70 runs   (    0.58 ms per token,  1738.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8634.25 ms /    70 runs   (  123.35 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  8836.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    48.01 ms /    83 runs   (    0.58 ms per token,  1728.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10253.03 ms /    83 runs   (  123.53 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 10497.24 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    40.48 ms /    70 runs   (    0.58 ms per token,  1729.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8662.86 ms /    70 runs   (  123.76 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  8870.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    38.11 ms /    66 runs   (    0.58 ms per token,  1731.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8137.51 ms /    66 runs   (  123.30 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  8329.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Sobre os músculos oculares externos, assinale a alternativa correta.\n",
      "a)Na lei de Hering, a inervação se distribui igualmente para os músculos agonista e antagonista, para determinada versão.\n",
      "b)Os movimentos persecutórios se dão automaticamente, de maneira lenta, ao deslocamento do ponto de fixação.\n",
      "c)Na lei de Sherrington, o estímulo de contração é distribuído entre os agonistas de ambos os olhos, para que ela ocorra simetricamente.\n",
      "d)Os movimentos sacádicos são caracterizados por movimentos bruscos, descoordenados, de natureza totalmente reflexa.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.18 ms /     7 runs   (    0.60 ms per token,  1673.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3749.31 ms /   178 tokens (   21.06 ms per token,    47.48 tokens per second)\n",
      "llama_print_timings:        eval time =   733.47 ms /     6 runs   (  122.25 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  4503.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.12 ms /    47 runs   (    0.58 ms per token,  1733.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5802.17 ms /    47 runs   (  123.45 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  5938.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    45.55 ms /    79 runs   (    0.58 ms per token,  1734.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9760.49 ms /    79 runs   (  123.55 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  9990.98 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    43.69 ms /    75 runs   (    0.58 ms per token,  1716.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9216.05 ms /    75 runs   (  122.88 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  9433.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    36.81 ms /    64 runs   (    0.58 ms per token,  1738.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7867.78 ms /    64 runs   (  122.93 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  8052.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Os movimentos sacádicos são caracterizados por movimentos bruscos, descoordenados, de natureza totalmente reflexa.'}\n",
      "Test #5: \n",
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    38.58 ms /    67 runs   (    0.58 ms per token,  1736.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8314.18 ms /    67 runs   (  124.09 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  8508.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.18 ms /    47 runs   (    0.58 ms per token,  1729.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5971.68 ms /    47 runs   (  127.06 ms per token,     7.87 tokens per second)\n",
      "llama_print_timings:       total time =  6109.07 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    77.33 ms /   134 runs   (    0.58 ms per token,  1732.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16516.48 ms /   134 runs   (  123.26 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 16917.86 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    34.93 ms /    60 runs   (    0.58 ms per token,  1717.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7458.84 ms /    60 runs   (  124.31 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =  7634.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    44.09 ms /    76 runs   (    0.58 ms per token,  1723.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9423.94 ms /    76 runs   (  124.00 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  9647.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 7: \n",
      "Language: english\n",
      "Question: \n",
      "About the cornea, it is correct to state. a) The arrangement of corneal collagen fibrils is unrelated to its transparency. b) Pleomorphism of endothelial cells is characterized by the variation in the size of these cells. c) anterior stroma have a large number of mitochondria, responsible for the production of energy that supplies the endothelial pump. d) The endothelium behaves as a permeable non-selective barrier between the aqueous humor and the substantia propria.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    37.36 ms /    64 runs   (    0.58 ms per token,  1713.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2195.79 ms /   127 tokens (   17.29 ms per token,    57.84 tokens per second)\n",
      "llama_print_timings:        eval time =  7805.51 ms /    63 runs   (  123.90 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time = 10191.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    96.07 ms /   166 runs   (    0.58 ms per token,  1727.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 20475.54 ms /   166 runs   (  123.35 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 20977.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    41.14 ms /    71 runs   (    0.58 ms per token,  1725.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8649.83 ms /    71 runs   (  121.83 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  8856.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    41.20 ms /    71 runs   (    0.58 ms per token,  1723.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8787.49 ms /    71 runs   (  123.77 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  8994.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    65.73 ms /   114 runs   (    0.58 ms per token,  1734.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14059.90 ms /   114 runs   (  123.33 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 14399.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.17 ms /    49 runs   (    0.57 ms per token,  1739.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5977.31 ms /    49 runs   (  121.99 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  6116.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #6: \n",
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    65.77 ms /   114 runs   (    0.58 ms per token,  1733.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14132.52 ms /   114 runs   (  123.97 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time = 14462.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    46.44 ms /    80 runs   (    0.58 ms per token,  1722.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9852.26 ms /    80 runs   (  123.15 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 10083.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n",
      "{'response': 'd'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    38.74 ms /    67 runs   (    0.58 ms per token,  1729.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8181.00 ms /    67 runs   (  122.10 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  8373.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.52 ms /    48 runs   (    0.59 ms per token,  1682.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5894.02 ms /    48 runs   (  122.79 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  6032.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Sobre a córnea, é correto afirmar:\n",
      "a)A disposição das fibrilas de colágeno corneanas não tem relação com a sua transparência.\n",
      "b)Pleomorfismo das células endoteliais é caracterizado pela variação do tamanho dessas células.\n",
      "c)As células do estroma anterior possuem grande quantidade de mitocôndrias, responsáveis pela produção de energia que supre a bomba endotelial.\n",
      "d)O endotélio comporta-se como uma barreira permeável não-seletiva entre o humor aquoso e a substância própria.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.17 ms /     7 runs   (    0.60 ms per token,  1678.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3194.99 ms /   162 tokens (   19.72 ms per token,    50.70 tokens per second)\n",
      "llama_print_timings:        eval time =   724.85 ms /     6 runs   (  120.81 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  3939.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.37 ms /    11 runs   (    0.58 ms per token,  1728.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1362.66 ms /    11 runs   (  123.88 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  1393.75 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    34.31 ms /    59 runs   (    0.58 ms per token,  1719.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7284.00 ms /    59 runs   (  123.46 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  7455.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    43.04 ms /    71 runs   (    0.61 ms per token,  1649.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8736.65 ms /    71 runs   (  123.05 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  8943.38 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n",
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   118.85 ms /   205 runs   (    0.58 ms per token,  1724.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 25335.50 ms /   205 runs   (  123.59 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 25947.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.67 ms /    27 runs   (    0.58 ms per token,  1722.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3308.67 ms /    27 runs   (  122.54 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3385.51 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.07 ms /     7 runs   (    0.58 ms per token,  1722.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   852.38 ms /     7 runs   (  121.77 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =   872.26 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.08 ms /     7 runs   (    0.58 ms per token,  1716.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   853.49 ms /     7 runs   (  121.93 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =   873.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.07 ms /     7 runs   (    0.58 ms per token,  1719.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   853.37 ms /     7 runs   (  121.91 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =   873.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    33.39 ms /    56 runs   (    0.60 ms per token,  1677.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6933.32 ms /    56 runs   (  123.81 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  7096.07 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 8: \n",
      "Language: english\n",
      "Question: \n",
      "Choose the alternative that correctly fills in the gaps. Levocycloversion corresponds to ____________ of the right eye and ________ of the left eye and is triggered when the head is tilted towards the shoulder___________.\n",
      "a) Excicloduction / incicloduction / left.\n",
      "b) Incicloduction / excicloduction / left.\n",
      "c) Excicloduction / incicloduction / law.\n",
      "d) Incicloduction / excicloduction / right.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.32 ms /    24 runs   (    0.56 ms per token,  1801.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2296.37 ms /   126 tokens (   18.23 ms per token,    54.87 tokens per second)\n",
      "llama_print_timings:        eval time =  2796.27 ms /    23 runs   (  121.58 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  5161.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    60.44 ms /   106 runs   (    0.57 ms per token,  1753.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13097.26 ms /   106 runs   (  123.56 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 13406.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.17 ms /    24 runs   (    0.55 ms per token,  1822.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2935.57 ms /    24 runs   (  122.32 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3003.21 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.27 ms /    24 runs   (    0.55 ms per token,  1808.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2947.81 ms /    24 runs   (  122.83 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  3015.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    45.62 ms /    81 runs   (    0.56 ms per token,  1775.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9945.26 ms /    81 runs   (  122.78 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 10178.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.85 ms /    29 runs   (    0.55 ms per token,  1829.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3622.06 ms /    29 runs   (  124.90 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time =  3703.86 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    44.00 ms /    77 runs   (    0.57 ms per token,  1750.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9527.89 ms /    77 runs   (  123.74 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  9751.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.16 ms /    24 runs   (    0.55 ms per token,  1824.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2913.62 ms /    24 runs   (  121.40 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  2980.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    46.48 ms /    82 runs   (    0.57 ms per token,  1764.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10204.14 ms /    82 runs   (  124.44 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time = 10439.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.22 ms /    24 runs   (    0.55 ms per token,  1815.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2935.60 ms /    24 runs   (  122.32 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3002.12 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Assinale a alternativa que preenche corretamente as lacunas. A levocicloversão corresponde a ____________ do olho direito e ________ do olho esquerdo e é desencadeada quando se inclina a cabeça para o ombro___________.\n",
      "a)Exciclodução / inciclodução / esquerdo.\n",
      "b)Inciclodução / exciclodução / esquerdo.\n",
      "c)Exciclodução / inciclodução / direito.\n",
      "d)Inciclodução / exciclodução / direito.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.45 ms /    27 runs   (    0.57 ms per token,  1747.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3044.48 ms /   149 tokens (   20.43 ms per token,    48.94 tokens per second)\n",
      "llama_print_timings:        eval time =  3210.35 ms /    26 runs   (  123.48 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  6331.82 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.12 ms /    27 runs   (    0.56 ms per token,  1785.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3334.43 ms /    27 runs   (  123.50 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  3410.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    83.38 ms /   146 runs   (    0.57 ms per token,  1750.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 18098.22 ms /   146 runs   (  123.96 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time = 18528.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.13 ms /    27 runs   (    0.56 ms per token,  1784.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3307.43 ms /    27 runs   (  122.50 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3384.15 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.36 ms /    24 runs   (    0.56 ms per token,  1796.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2960.15 ms /    24 runs   (  123.34 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  3027.32 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n",
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    41.51 ms /    73 runs   (    0.57 ms per token,  1758.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9002.82 ms /    73 runs   (  123.33 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  9212.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    29.26 ms /    52 runs   (    0.56 ms per token,  1777.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6385.12 ms /    52 runs   (  122.79 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  6533.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n",
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.77 ms /    28 runs   (    0.56 ms per token,  1775.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3488.17 ms /    28 runs   (  124.58 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =  3567.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.28 ms /    24 runs   (    0.55 ms per token,  1806.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2967.98 ms /    24 runs   (  123.67 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  3034.95 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.17 ms /    27 runs   (    0.56 ms per token,  1779.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3329.31 ms /    27 runs   (  123.31 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  3405.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 9: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding the vitreous body cells, mark the correct alternative.\n",
      "a) They are more numerous in the adult than in the embryonic vitreous.\n",
      "b) They are mainly represented by lymphocytes and mature neutrophils.\n",
      "c) They are abundant in the region of the vitreous base.\n",
      "d) They are absent in the newborn.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.53 ms /    25 runs   (    0.58 ms per token,  1720.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1685.17 ms /    87 tokens (   19.37 ms per token,    51.63 tokens per second)\n",
      "llama_print_timings:        eval time =  2929.94 ms /    24 runs   (  122.08 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  4686.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.88 ms /    17 runs   (    0.58 ms per token,  1720.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2089.15 ms /    17 runs   (  122.89 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  2136.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.78 ms /    24 runs   (    0.57 ms per token,  1742.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2936.98 ms /    24 runs   (  122.37 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3004.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.98 ms /    24 runs   (    0.58 ms per token,  1716.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2970.94 ms /    24 runs   (  123.79 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  3039.92 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n",
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    40.91 ms /    68 runs   (    0.60 ms per token,  1662.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8363.16 ms /    68 runs   (  122.99 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  8563.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.82 ms /    24 runs   (    0.58 ms per token,  1736.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2926.64 ms /    24 runs   (  121.94 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  2994.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.21 ms /    25 runs   (    0.69 ms per token,  1452.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3072.85 ms /    25 runs   (  122.91 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  3149.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n",
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.85 ms /    24 runs   (    0.58 ms per token,  1733.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2940.45 ms /    24 runs   (  122.52 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3007.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    40.26 ms /    70 runs   (    0.58 ms per token,  1738.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8608.95 ms /    70 runs   (  122.99 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  8809.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.37 ms /    25 runs   (    0.57 ms per token,  1740.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3029.39 ms /    25 runs   (  121.18 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3099.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Com relação às células do corpo vítreo, assinale a alternativa correta.\n",
      "a)São mais numerosos no adulto, comparativamente às do vítreo embrionário.\n",
      "b)São representadas principalmente por linfócitos e neutrófilos maduros.\n",
      "c)São abundantes na região da base vítrea.\n",
      "d)Estão ausentes no recém-nascido.\n",
      "Test #0: \n",
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.30 ms /    39 runs   (    0.57 ms per token,  1749.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2273.65 ms /   108 tokens (   21.05 ms per token,    47.50 tokens per second)\n",
      "llama_print_timings:        eval time =  4643.97 ms /    38 runs   (  122.21 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  7029.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.43 ms /    46 runs   (    0.57 ms per token,  1740.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5672.74 ms /    46 runs   (  123.32 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  5803.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    40.71 ms /    71 runs   (    0.57 ms per token,  1743.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8766.15 ms /    71 runs   (  123.47 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  8974.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.73 ms /    55 runs   (    0.58 ms per token,  1733.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6757.21 ms /    55 runs   (  122.86 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  6916.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.38 ms /    39 runs   (    0.57 ms per token,  1742.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4810.59 ms /    39 runs   (  123.35 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  4921.80 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.02 ms /     7 runs   (    0.57 ms per token,  1740.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   855.68 ms /     7 runs   (  122.24 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =   875.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.83 ms /    19 runs   (    0.57 ms per token,  1755.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2322.25 ms /    19 runs   (  122.22 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  2375.68 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1746.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   870.50 ms /     7 runs   (  124.36 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =   889.83 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.95 ms /    40 runs   (    0.57 ms per token,  1742.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4900.92 ms /    40 runs   (  122.52 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  5014.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.99 ms /     7 runs   (    0.57 ms per token,  1753.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   854.55 ms /     7 runs   (  122.08 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =   873.82 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 10: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding the physiology of the oculomotor muscles, mark the correct alternative.\n",
      "a) Muscle contraction is independent of calcium availability.\n",
      "b) The muscle fiber is a multinucleated cell.\n",
      "c) Each muscle contraction is the result of a unique cycle of formation and destruction of actin-myosin bridges.\n",
      "d) Muscle relaxation rely mainly on sodium availability.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    62.85 ms /   108 runs   (    0.58 ms per token,  1718.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2470.22 ms /   102 tokens (   24.22 ms per token,    41.29 tokens per second)\n",
      "llama_print_timings:        eval time = 13088.83 ms /   107 runs   (  122.33 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time = 15879.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.96 ms /    24 runs   (    0.58 ms per token,  1719.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2935.44 ms /    24 runs   (  122.31 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3003.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.08 ms /    28 runs   (    0.57 ms per token,  1741.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3416.99 ms /    28 runs   (  122.04 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3496.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.23 ms /    28 runs   (    0.58 ms per token,  1724.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3508.98 ms /    28 runs   (  125.32 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:       total time =  3590.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.46 ms /    24 runs   (    0.60 ms per token,  1659.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2963.82 ms /    24 runs   (  123.49 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  3034.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.62 ms /    27 runs   (    0.58 ms per token,  1728.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3307.06 ms /    27 runs   (  122.48 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3385.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    54.76 ms /    95 runs   (    0.58 ms per token,  1734.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11606.88 ms /    95 runs   (  122.18 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time = 11883.26 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   130.76 ms /   227 runs   (    0.58 ms per token,  1736.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 27911.05 ms /   227 runs   (  122.96 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 28606.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n",
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.62 ms /    27 runs   (    0.58 ms per token,  1728.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3322.58 ms /    27 runs   (  123.06 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3399.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.58 ms /    27 runs   (    0.58 ms per token,  1733.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3262.30 ms /    27 runs   (  120.83 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  3338.63 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Com relação à fisiologia dos músculos oculomotores, assinale a alternativa correta.\n",
      "a)A contração muscular independe da disponibilidade de cálcio.\n",
      "b)A fibra muscular é uma célula multinucleada.\n",
      "c)Cada contração muscular é resultado de um ciclo único de formação e destruição de pontes de actina-miosina.\n",
      "d)O relaxamento muscular depende principalmente da disponibilidade de sódio.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    91.05 ms /   151 runs   (    0.60 ms per token,  1658.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2680.24 ms /   123 tokens (   21.79 ms per token,    45.89 tokens per second)\n",
      "llama_print_timings:        eval time = 18594.87 ms /   150 runs   (  123.97 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time = 21742.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.19 ms /    28 runs   (    0.58 ms per token,  1729.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3426.08 ms /    28 runs   (  122.36 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3506.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    29.62 ms /    51 runs   (    0.58 ms per token,  1721.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6211.57 ms /    51 runs   (  121.80 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  6360.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.46 ms /    20 runs   (    0.57 ms per token,  1745.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2463.20 ms /    20 runs   (  123.16 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  2521.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    39.27 ms /    68 runs   (    0.58 ms per token,  1731.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8428.59 ms /    68 runs   (  123.95 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  8629.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.85 ms /    55 runs   (    0.58 ms per token,  1726.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6783.64 ms /    55 runs   (  123.34 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  6946.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.14 ms /    28 runs   (    0.58 ms per token,  1735.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3444.84 ms /    28 runs   (  123.03 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3525.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    33.35 ms /    58 runs   (    0.58 ms per token,  1739.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7254.94 ms /    58 runs   (  125.09 ms per token,     7.99 tokens per second)\n",
      "llama_print_timings:       total time =  7423.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.73 ms /    27 runs   (    0.58 ms per token,  1716.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3336.78 ms /    27 runs   (  123.58 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  3415.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    33.45 ms /    58 runs   (    0.58 ms per token,  1733.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7150.27 ms /    58 runs   (  123.28 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  7317.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 11: \n",
      "Language: english\n",
      "Question: \n",
      "With regard to the lens, mark the correct alternative.\n",
      "a) Although its volume increases with age, its weight remains constant.\n",
      "b) The proteolysis of its fibers is the main mechanism of its cellular growth.\n",
      "c) The synthesis of its proteins takes place during the differentiation of the cell into fiber.\n",
      "d) Its protein synthesis is continuous throughout life, maintaining stable plasticity and elasticity from childhood to aging.\n",
      " \n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.60 ms /    27 runs   (    0.61 ms per token,  1626.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2194.06 ms /   107 tokens (   20.51 ms per token,    48.77 tokens per second)\n",
      "llama_print_timings:        eval time =  3274.80 ms /    26 runs   (  125.95 ms per token,     7.94 tokens per second)\n",
      "llama_print_timings:       total time =  5549.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.12 ms /    24 runs   (    0.59 ms per token,  1699.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2920.42 ms /    24 runs   (  121.68 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  2989.80 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    36.00 ms /    62 runs   (    0.58 ms per token,  1722.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7567.03 ms /    62 runs   (  122.05 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  7748.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.52 ms /    25 runs   (    0.58 ms per token,  1721.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3019.69 ms /    25 runs   (  120.79 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  3091.86 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    29.96 ms /    51 runs   (    0.59 ms per token,  1702.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6322.56 ms /    51 runs   (  123.97 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  6473.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd} Its protein synthesis is continuous throughout life, maintaining stable plasticity and elasticity from childhood to aging.'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.44 ms /    49 runs   (    0.58 ms per token,  1722.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6064.98 ms /    49 runs   (  123.78 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  6209.95 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.14 ms /    28 runs   (    0.58 ms per token,  1734.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3445.85 ms /    28 runs   (  123.07 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3526.95 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    35.62 ms /    62 runs   (    0.57 ms per token,  1740.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7558.84 ms /    62 runs   (  121.92 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  7739.48 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'D'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.18 ms /    28 runs   (    0.58 ms per token,  1730.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3416.80 ms /    28 runs   (  122.03 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3497.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.35 ms /    49 runs   (    0.58 ms per token,  1728.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6022.40 ms /    49 runs   (  122.91 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  6166.63 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Com relação ao cristalino, assinale a alternativa correta.\n",
      "a)Embora seu volume aumente com a idade, seu peso mantém-se constante.\n",
      "b)A proteólise de suas fibras é o principal mecanismo de seu crescimento celular.\n",
      "c)A síntese de suas proteínas processa-se durante a diferenciação da célula em fibra.\n",
      "d)Sua síntese proteica é continua ao longo da vida, mantendo plasticidade e elasticidade estáveis da infância ao envelhecimento.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.33 ms /    27 runs   (    0.60 ms per token,  1653.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2402.68 ms /   141 tokens (   17.04 ms per token,    58.68 tokens per second)\n",
      "llama_print_timings:        eval time =  3232.61 ms /    26 runs   (  124.33 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =  5718.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.21 ms /     7 runs   (    0.60 ms per token,  1664.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   848.56 ms /     7 runs   (  121.22 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =   869.32 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.78 ms /    27 runs   (    0.58 ms per token,  1711.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3309.92 ms /    27 runs   (  122.59 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3388.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    83.96 ms /   145 runs   (    0.58 ms per token,  1727.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 17837.79 ms /   145 runs   (  123.02 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 18275.83 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.67 ms /    53 runs   (    0.58 ms per token,  1727.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6508.33 ms /    53 runs   (  122.80 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  6661.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.06 ms /    33 runs   (    0.58 ms per token,  1731.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4029.23 ms /    33 runs   (  122.10 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  4123.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.51 ms /    27 runs   (    0.57 ms per token,  1741.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3389.38 ms /    27 runs   (  125.53 ms per token,     7.97 tokens per second)\n",
      "llama_print_timings:       total time =  3466.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   130.55 ms /   226 runs   (    0.58 ms per token,  1731.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 27929.94 ms /   226 runs   (  123.58 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 28617.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    40.62 ms /    70 runs   (    0.58 ms per token,  1723.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8634.03 ms /    70 runs   (  123.34 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  8835.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.94 ms /    55 runs   (    0.58 ms per token,  1721.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6724.74 ms /    55 runs   (  122.27 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  6881.63 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 12: \n",
      "Language: english\n",
      "Question: \n",
      "A patient with arthrosis has been using high doses of chloroquine for years. It is believed that this drug can cause malfunction of the retinal pigment epithelium, since:\n",
      "a) It hinders the synthesis of melanin.\n",
      "b) It hinders phagocytosis of the disks of the photoreceptor outer segments.\n",
      "c) It hinders the transport of intracellular ions.\n",
      "d) It favors intense heat exchange between the photoreceptors and the choroid.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.62 ms /    25 runs   (    0.58 ms per token,  1710.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2732.51 ms /   130 tokens (   21.02 ms per token,    47.58 tokens per second)\n",
      "llama_print_timings:        eval time =  2976.83 ms /    24 runs   (  124.03 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  5781.03 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.61 ms /    25 runs   (    0.58 ms per token,  1711.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3043.66 ms /    25 runs   (  121.75 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3115.83 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n",
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.28 ms /    28 runs   (    0.58 ms per token,  1719.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3419.61 ms /    28 runs   (  122.13 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3499.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    76.27 ms /   132 runs   (    0.58 ms per token,  1730.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16284.15 ms /   132 runs   (  123.36 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 16670.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    56.12 ms /    97 runs   (    0.58 ms per token,  1728.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11959.31 ms /    97 runs   (  123.29 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 12240.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.94 ms /    24 runs   (    0.58 ms per token,  1721.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2946.55 ms /    24 runs   (  122.77 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  3014.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   102.19 ms /   170 runs   (    0.60 ms per token,  1663.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 20860.53 ms /   170 runs   (  122.71 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 21373.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.58 ms /    50 runs   (    0.57 ms per token,  1749.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6093.06 ms /    50 runs   (  121.86 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  6234.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.80 ms /    24 runs   (    0.58 ms per token,  1738.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2950.30 ms /    24 runs   (  122.93 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3018.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    46.29 ms /    80 runs   (    0.58 ms per token,  1728.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9752.26 ms /    80 runs   (  121.90 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  9982.48 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Paciente com artrose faz uso de altas doses de cloroquina há anos. Acredita-se que essa droga pode causar mau funcionamento do epitélio pigmentar retiniano, uma vez que:\n",
      "a)Impede a síntese de melanina.\n",
      "b)Impede a fagocitose dos discos dos segmentos externos dos fotorreceptores.\n",
      "c)Impede o transporte de íons intracelulares.\n",
      "d)Favorece a intensa troca de calor entre os fotorreceptores e a coroide.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    70.21 ms /   120 runs   (    0.59 ms per token,  1709.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2651.00 ms /   143 tokens (   18.54 ms per token,    53.94 tokens per second)\n",
      "llama_print_timings:        eval time = 14661.60 ms /   119 runs   (  123.21 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 17668.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    83.79 ms /   144 runs   (    0.58 ms per token,  1718.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 17783.50 ms /   144 runs   (  123.50 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 18213.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    68.31 ms /   116 runs   (    0.59 ms per token,  1698.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14396.72 ms /   116 runs   (  124.11 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time = 14741.80 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    65.05 ms /   113 runs   (    0.58 ms per token,  1737.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13906.52 ms /   113 runs   (  123.07 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 14237.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.07 ms /     7 runs   (    0.58 ms per token,  1719.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   859.36 ms /     7 runs   (  122.77 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =   880.01 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.00 ms /     7 runs   (    0.57 ms per token,  1750.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   855.28 ms /     7 runs   (  122.18 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =   874.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    65.41 ms /   113 runs   (    0.58 ms per token,  1727.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14007.31 ms /   113 runs   (  123.96 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time = 14346.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    49.68 ms /    86 runs   (    0.58 ms per token,  1731.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10566.23 ms /    86 runs   (  122.86 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 10815.01 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   111.82 ms /   193 runs   (    0.58 ms per token,  1725.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 23788.81 ms /   193 runs   (  123.26 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 24371.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    70.19 ms /   121 runs   (    0.58 ms per token,  1723.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14833.27 ms /   121 runs   (  122.59 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time = 15193.32 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 13: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding the blood flow in the choroid, it is possible to state:\n",
      "a) It is responsible for all nutrition of the retina.\n",
      "b) It is responsible for 95% of the glucose consumed in the inner part of the retina.\n",
      "c)Participates in the thermal regulation of photoreceptors.\n",
      "d) The pre-capillary sphincters prevent the hyperflow of blood in noble areas such as the macula.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.24 ms /    24 runs   (    0.59 ms per token,  1685.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2294.83 ms /   110 tokens (   20.86 ms per token,    47.93 tokens per second)\n",
      "llama_print_timings:        eval time =  2815.82 ms /    23 runs   (  122.43 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  5180.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.23 ms /    28 runs   (    0.58 ms per token,  1725.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3525.06 ms /    28 runs   (  125.90 ms per token,     7.94 tokens per second)\n",
      "llama_print_timings:       total time =  3605.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.49 ms /    32 runs   (    0.58 ms per token,  1730.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3927.81 ms /    32 runs   (  122.74 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  4019.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.89 ms /    24 runs   (    0.58 ms per token,  1727.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2948.10 ms /    24 runs   (  122.84 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  3016.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.46 ms /    25 runs   (    0.58 ms per token,  1728.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3079.36 ms /    25 runs   (  123.17 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  3151.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.99 ms /    24 runs   (    0.58 ms per token,  1715.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2941.59 ms /    24 runs   (  122.57 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3010.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n",
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.51 ms /    25 runs   (    0.58 ms per token,  1723.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3030.35 ms /    25 runs   (  121.21 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3101.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.91 ms /    24 runs   (    0.58 ms per token,  1725.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2932.06 ms /    24 runs   (  122.17 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3000.48 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.97 ms /    24 runs   (    0.58 ms per token,  1718.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2932.26 ms /    24 runs   (  122.18 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3000.51 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n",
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Com relação ao fluxo sanguíneo na coroide, podemos afirmar:\n",
      "a)É responsável por toda nutrição da retina.\n",
      "b)É responsável por 95% da glicose consumida na parte interna da retina.\n",
      "c)Participa da regulação térmica dos fotorreceptores.\n",
      "d)Os esfíncteres pré-capilares previnem o hiperfluxo de sangue em áreas nobres como a mácula.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.51 ms /    25 runs   (    0.58 ms per token,  1722.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3052.40 ms /    25 runs   (  122.10 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3123.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.15 ms /     7 runs   (    0.59 ms per token,  1687.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2284.93 ms /   124 tokens (   18.43 ms per token,    54.27 tokens per second)\n",
      "llama_print_timings:        eval time =   748.43 ms /     6 runs   (  124.74 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time =  3053.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     5.23 ms /     9 runs   (    0.58 ms per token,  1720.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1121.82 ms /     9 runs   (  124.65 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time =  1147.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.28 ms /    28 runs   (    0.58 ms per token,  1719.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3440.82 ms /    28 runs   (  122.89 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  3520.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.66 ms /    47 runs   (    0.61 ms per token,  1640.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5761.17 ms /    47 runs   (  122.58 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  5899.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    86.70 ms /   126 runs   (    0.69 ms per token,  1453.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 15397.24 ms /   126 runs   (  122.20 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time = 15810.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.64 ms /    28 runs   (    0.63 ms per token,  1587.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3431.34 ms /    28 runs   (  122.55 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3515.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.50 ms /    24 runs   (    0.60 ms per token,  1655.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2977.50 ms /    24 runs   (  124.06 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  3047.82 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.10 ms /    24 runs   (    0.59 ms per token,  1702.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2915.08 ms /    24 runs   (  121.46 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  2984.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.86 ms /    27 runs   (    0.59 ms per token,  1702.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3273.69 ms /    27 runs   (  121.25 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3352.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.96 ms /    27 runs   (    0.59 ms per token,  1691.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3285.72 ms /    27 runs   (  121.69 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3363.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 14: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding the embryology of the choroid, it is correct to state:\n",
      "a) The choriocapillaris is the first to be formed, with subsequent formation of the great vessels.\n",
      "b) The basal lamina originate the Bruch's membrane, which lies between the choriocapillaris and Sattler's layer.\n",
      "c) The intermediate vascular layer is the last to form and develops from the ciliary body towards the equator.\n",
      "d) The choroidal stroma is essentially produced from mesoderm cells.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    32.33 ms /    54 runs   (    0.60 ms per token,  1670.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2417.78 ms /   133 tokens (   18.18 ms per token,    55.01 tokens per second)\n",
      "llama_print_timings:        eval time =  6539.78 ms /    53 runs   (  123.39 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  9116.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.12 ms /     7 runs   (    0.59 ms per token,  1699.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   842.92 ms /     7 runs   (  120.42 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:       total time =   862.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.60 ms /    27 runs   (    0.58 ms per token,  1731.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3259.45 ms /    27 runs   (  120.72 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  3336.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.57 ms /    54 runs   (    0.58 ms per token,  1710.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6642.54 ms /    54 runs   (  123.01 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  6800.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.61 ms /    25 runs   (    0.58 ms per token,  1710.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3105.36 ms /    25 runs   (  124.21 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =  3178.26 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.28 ms /    28 runs   (    0.58 ms per token,  1720.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3401.31 ms /    28 runs   (  121.48 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  3482.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.77 ms /    53 runs   (    0.58 ms per token,  1722.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6475.50 ms /    53 runs   (  122.18 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  6628.57 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n",
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.19 ms /    28 runs   (    0.58 ms per token,  1729.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3448.92 ms /    28 runs   (  123.18 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  3530.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.66 ms /    28 runs   (    0.60 ms per token,  1680.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3534.16 ms /    28 runs   (  126.22 ms per token,     7.92 tokens per second)\n",
      "llama_print_timings:       total time =  3615.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.94 ms /    17 runs   (    0.58 ms per token,  1710.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2089.68 ms /    17 runs   (  122.92 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  2138.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Sobre a embriologia da coroide, é correto afirmar:\n",
      "a)A coriocapilar é a primeira a se formar, com posterior formação dos grandes vasos.\n",
      "b)A lâmina basal dá origem à membrana de Bruch que se situa entre o coriocapilar e a camada de Sattler.\n",
      "c)A camada vascular intermediária é a última a se formar e desenvolve-se do corpo ciliar em direção ao equador.\n",
      "d)O estroma da coroide é essencialmente produzido a partir de células do mesoderma.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    40.36 ms /    67 runs   (    0.60 ms per token,  1660.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2795.00 ms /   154 tokens (   18.15 ms per token,    55.10 tokens per second)\n",
      "llama_print_timings:        eval time =  8137.95 ms /    66 runs   (  123.30 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 11131.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    61.17 ms /   106 runs   (    0.58 ms per token,  1732.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13089.72 ms /   106 runs   (  123.49 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 13401.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.06 ms /     7 runs   (    0.58 ms per token,  1724.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   862.98 ms /     7 runs   (  123.28 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =   882.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n",
      "Error converting respose to json: {:response:\"b\"}\n",
      "Generating new response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.43 ms /     6 runs   (    0.57 ms per token,  1751.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   724.04 ms /     6 runs   (  120.67 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =   740.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.08 ms /     7 runs   (    0.58 ms per token,  1717.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   857.49 ms /     7 runs   (  122.50 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =   877.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n",
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.00 ms /     7 runs   (    0.57 ms per token,  1752.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   869.42 ms /     7 runs   (  124.20 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =   889.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1732.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   857.43 ms /     7 runs   (  122.49 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =   876.92 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    49.47 ms /    86 runs   (    0.58 ms per token,  1738.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10567.49 ms /    86 runs   (  122.88 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 10820.86 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    52.15 ms /    90 runs   (    0.58 ms per token,  1725.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11055.19 ms /    90 runs   (  122.84 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 11319.02 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.71 ms /    41 runs   (    0.58 ms per token,  1728.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5067.22 ms /    41 runs   (  123.59 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  5185.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    57.13 ms /    98 runs   (    0.58 ms per token,  1715.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12137.01 ms /    98 runs   (  123.85 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time = 12424.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 15: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding ocular embryology, it is correct to state:\n",
      "a) Corneal epithelium arises from neural crest cells.\n",
      "b) At birth, the cornea and sclera have curvature radius.\n",
      "c) Bowman's layer appears between the first and second month of pregnancy.\n",
      "d) The cornea develops from the ectodermal surface and neural crest cells.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.08 ms /    24 runs   (    0.59 ms per token,  1705.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1971.08 ms /    97 tokens (   20.32 ms per token,    49.21 tokens per second)\n",
      "llama_print_timings:        eval time =  2812.16 ms /    23 runs   (  122.27 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  4852.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.94 ms /    45 runs   (    0.58 ms per token,  1735.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5512.66 ms /    45 runs   (  122.50 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  5642.26 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #2: \n",
      "{'response': 'd'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.87 ms /    19 runs   (    0.57 ms per token,  1748.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2320.63 ms /    19 runs   (  122.14 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  2375.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.28 ms /    37 runs   (    0.58 ms per token,  1738.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4546.38 ms /    37 runs   (  122.88 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  4652.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.01 ms /    47 runs   (    0.57 ms per token,  1740.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5784.16 ms /    47 runs   (  123.07 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  5920.02 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #5: \n",
      "{'response': 'd) The cornea develops from the ectodermal surface and neural crest cells.'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.02 ms /    37 runs   (    0.60 ms per token,  1680.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4586.24 ms /    37 runs   (  123.95 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  4694.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.15 ms /    47 runs   (    0.58 ms per token,  1731.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5742.84 ms /    47 runs   (  122.19 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  5878.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd} The cornea develops from the ectodermal surface and neural crest cells.'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.25 ms /    37 runs   (    0.57 ms per token,  1740.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4513.94 ms /    37 runs   (  122.00 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  4621.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.86 ms /    47 runs   (    0.57 ms per token,  1750.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5757.38 ms /    47 runs   (  122.50 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  5893.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.53 ms /    36 runs   (    0.57 ms per token,  1753.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4452.15 ms /    36 runs   (  123.67 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  4555.57 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Com relação à embriologia ocular, é correto afirmar:\n",
      "a)O epitélio da córnea origina-se a partir das células da crista neural.\n",
      "b)Ao nascimento, córnea e esclera apresentam o mesmo raio de curvatura.\n",
      "c)A camada de Bowman surge entre o primeiro e o segundo mês de gestação.\n",
      "d)A córnea desenvolve-se a partir da superfície ectodérmica e das células da crista neural.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.89 ms /    38 runs   (    0.63 ms per token,  1590.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2549.22 ms /   130 tokens (   19.61 ms per token,    51.00 tokens per second)\n",
      "llama_print_timings:        eval time =  4511.35 ms /    37 runs   (  121.93 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  7181.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    36.46 ms /    63 runs   (    0.58 ms per token,  1727.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7867.66 ms /    63 runs   (  124.88 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time =  8051.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) A córnea desenvolve-se a partir da superfície ectodérmica e das células da crista neural.'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    34.08 ms /    59 runs   (    0.58 ms per token,  1731.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7244.19 ms /    59 runs   (  122.78 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  7415.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) A córnea desenvolve-se a partir da superfície ectodérmica e das células da crista neural.'}\n",
      "Test #3: \n",
      "{'response': 'd'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    35.15 ms /    61 runs   (    0.58 ms per token,  1735.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7496.10 ms /    61 runs   (  122.89 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  7674.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    36.31 ms /    63 runs   (    0.58 ms per token,  1735.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7723.11 ms /    63 runs   (  122.59 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  7906.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.52 ms /    38 runs   (    0.57 ms per token,  1765.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4661.83 ms /    38 runs   (  122.68 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  4771.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd} A córnea desenvolve-se a partir da superfície ectodérmica e das células da crista neural.'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    33.37 ms /    58 runs   (    0.58 ms per token,  1737.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7102.67 ms /    58 runs   (  122.46 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  7269.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1743.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   848.87 ms /     7 runs   (  121.27 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =   868.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    33.46 ms /    58 runs   (    0.58 ms per token,  1733.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7101.06 ms /    58 runs   (  122.43 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  7268.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.93 ms /    31 runs   (    0.58 ms per token,  1729.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3811.15 ms /    31 runs   (  122.94 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3900.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 16: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding the autosomal dominant inheritance diseases, it is correct to state:\n",
      "a) In general, they are observed in several gestations.\n",
      "b) Men tend to be more affected than women.\n",
      "c) The severity of the disease is always the same among affected people from the same family.\n",
      "d) Only females transmit the mutation.\n",
      "Test #0: \n",
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.92 ms /    24 runs   (    0.58 ms per token,  1723.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1902.38 ms /    80 tokens (   23.78 ms per token,    42.05 tokens per second)\n",
      "llama_print_timings:        eval time =  2804.45 ms /    23 runs   (  121.93 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  4776.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.86 ms /    24 runs   (    0.58 ms per token,  1731.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3008.12 ms /    24 runs   (  125.34 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:       total time =  3077.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    57.62 ms /   100 runs   (    0.58 ms per token,  1735.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12316.82 ms /   100 runs   (  123.17 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 12615.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.39 ms /    25 runs   (    0.58 ms per token,  1736.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3041.37 ms /    25 runs   (  121.65 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3113.26 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.81 ms /    17 runs   (    0.58 ms per token,  1732.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2060.65 ms /    17 runs   (  121.21 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  2108.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n",
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.94 ms /    24 runs   (    0.58 ms per token,  1721.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2915.13 ms /    24 runs   (  121.46 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  2985.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.80 ms /    17 runs   (    0.58 ms per token,  1735.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2090.65 ms /    17 runs   (  122.98 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  2140.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.81 ms /    17 runs   (    0.58 ms per token,  1732.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2069.31 ms /    17 runs   (  121.72 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  2117.88 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.79 ms /    17 runs   (    0.58 ms per token,  1736.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2064.78 ms /    17 runs   (  121.46 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  2113.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.82 ms /    17 runs   (    0.58 ms per token,  1730.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2077.53 ms /    17 runs   (  122.21 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  2126.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Sobre as doenças de herança autossômica dominante, é correto afirmar:\n",
      "a)Em geral, são observadas em várias gestações.\n",
      "b)Os homens costumam ser mais afetados do que as mulheres.\n",
      "c)A gravidade da doença é sempre a mesma entre as pessoas afetadas da mesma família.\n",
      "d)Apenas as mulheres transmitem a mutação.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    44.38 ms /    76 runs   (    0.58 ms per token,  1712.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1984.10 ms /   109 tokens (   18.20 ms per token,    54.94 tokens per second)\n",
      "llama_print_timings:        eval time =  9172.88 ms /    75 runs   (  122.31 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time = 11384.86 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.66 ms /    34 runs   (    0.58 ms per token,  1729.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4128.87 ms /    34 runs   (  121.44 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  4228.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.52 ms /    48 runs   (    0.57 ms per token,  1744.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5934.59 ms /    48 runs   (  123.64 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  6074.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.28 ms /    49 runs   (    0.58 ms per token,  1732.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6010.16 ms /    49 runs   (  122.66 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  6156.03 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n",
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.09 ms /     7 runs   (    0.58 ms per token,  1713.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   909.59 ms /     7 runs   (  129.94 ms per token,     7.70 tokens per second)\n",
      "llama_print_timings:       total time =   930.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1743.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   876.67 ms /     7 runs   (  125.24 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:       total time =   897.48 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting respose to json: {'response': 'c'}\n",
      "Generating new response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   145.06 ms /   252 runs   (    0.58 ms per token,  1737.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 31120.25 ms /   252 runs   (  123.49 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 31898.32 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n",
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.49 ms /    52 runs   (    0.59 ms per token,  1705.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6389.61 ms /    52 runs   (  122.88 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  6542.54 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1735.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   860.06 ms /     7 runs   (  122.87 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =   879.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.62 ms /    48 runs   (    0.58 ms per token,  1737.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5854.89 ms /    48 runs   (  121.98 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  5994.67 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c)} A gravidade da doença é sempre a mesma entre as pessoas afetadas da mesma família.'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.54 ms /    27 runs   (    0.58 ms per token,  1737.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3310.26 ms /    27 runs   (  122.60 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3388.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 17: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding mitochondrial inheritance diseases, it is correct to state:\n",
      "a) Daltonism is an example of a disease linked to mitochondrial DNA.\n",
      "b) Inheritance occurs only through maternal lineage.\n",
      "c) Male patients assisted by the disease will transmit the phenotype to all offspring.\n",
      "d) Leber's hereditary optic neuropathy is an example of a disease linked to mitochondrial DNA, it usually causes congenital blindness and affects patients of both sexes in the same proportion.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.92 ms /    17 runs   (    0.58 ms per token,  1714.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2786.39 ms /   127 tokens (   21.94 ms per token,    45.58 tokens per second)\n",
      "llama_print_timings:        eval time =  1963.84 ms /    16 runs   (  122.74 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  4799.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #1: \n",
      "{'response': 'd'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   139.22 ms /   240 runs   (    0.58 ms per token,  1723.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 29656.74 ms /   240 runs   (  123.57 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 30400.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    40.98 ms /    70 runs   (    0.59 ms per token,  1708.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8740.96 ms /    70 runs   (  124.87 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time =  8946.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.78 ms /    17 runs   (    0.58 ms per token,  1738.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2084.73 ms /    17 runs   (  122.63 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  2133.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.60 ms /    20 runs   (    0.58 ms per token,  1723.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2445.96 ms /    20 runs   (  122.30 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  2503.15 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.35 ms /    17 runs   (    0.61 ms per token,  1642.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2131.63 ms /    17 runs   (  125.39 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:       total time =  2181.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #6: \n",
      "{'response': 'd'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    42.37 ms /    73 runs   (    0.58 ms per token,  1723.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8950.89 ms /    73 runs   (  122.61 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  9164.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.22 ms /    17 runs   (    0.60 ms per token,  1662.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2067.89 ms /    17 runs   (  121.64 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  2116.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.89 ms /    24 runs   (    0.58 ms per token,  1727.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2915.29 ms /    24 runs   (  121.47 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  2984.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.81 ms /    24 runs   (    0.58 ms per token,  1737.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2932.54 ms /    24 runs   (  122.19 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3001.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Sobre as doenças de padrão de herança mitocondrial, é correto afirmar:\n",
      "a)Daltonismo é um exemplo de doença ligada ao DNA mitocondrial.\n",
      "b)A herança ocorre apenas pela linhagem materna.\n",
      "c)Pacientes do sexo masculino afetados pela doença, transmitirão o fenótipo para toda a prole.\n",
      "d)A neuropatia óptica hereditária de Leber é um exemplo de doença ligada ao DNA mitocondrial, causa geralmente cegueira congênita e afeta na mesma proporção pacientes de ambos os sexos.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.11 ms /     7 runs   (    0.59 ms per token,  1702.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3223.15 ms /   164 tokens (   19.65 ms per token,    50.88 tokens per second)\n",
      "llama_print_timings:        eval time =   745.21 ms /     6 runs   (  124.20 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =  3988.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.12 ms /     7 runs   (    0.59 ms per token,  1697.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   847.17 ms /     7 runs   (  121.02 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =   867.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.10 ms /     7 runs   (    0.59 ms per token,  1708.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   860.89 ms /     7 runs   (  122.98 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =   881.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.05 ms /     7 runs   (    0.58 ms per token,  1727.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   863.44 ms /     7 runs   (  123.35 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =   884.48 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.18 ms /     7 runs   (    0.60 ms per token,  1675.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   871.04 ms /     7 runs   (  124.43 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =   891.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.11 ms /     7 runs   (    0.59 ms per token,  1701.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   860.47 ms /     7 runs   (  122.92 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =   880.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #6: \n",
      "{'response': 'd'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.07 ms /     7 runs   (    0.58 ms per token,  1719.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   858.39 ms /     7 runs   (  122.63 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =   879.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   117.62 ms /   203 runs   (    0.58 ms per token,  1725.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 25088.76 ms /   203 runs   (  123.59 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 25709.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #8: \n",
      "{'response': 'd'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1731.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   852.01 ms /     7 runs   (  121.72 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =   871.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1730.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   847.07 ms /     7 runs   (  121.01 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =   867.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 18: \n",
      "Language: english\n",
      "Question: \n",
      "Which of the lenses below best corrects simple myopic astigmatism?\n",
      "a)-3.00 spherical diopter + 1.00 cylindrical diopter X 90°.\n",
      "b) +1.00 spherical diopter -1.00 cylindrical diopter X 90°.\n",
      "c)-3.00 spherical diopter + 3.00 cylindrical diopter X 180°.\n",
      "d) 0.00 spherical diopter +1.00 cylindrical diopter X 180°.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.71 ms /    50 runs   (    0.55 ms per token,  1804.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2462.53 ms /   138 tokens (   17.84 ms per token,    56.04 tokens per second)\n",
      "llama_print_timings:        eval time =  6029.50 ms /    49 runs   (  123.05 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  8638.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.82 ms /    52 runs   (    0.55 ms per token,  1804.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6406.75 ms /    52 runs   (  123.21 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  6557.32 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n",
      "{'response': 'B'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.11 ms /    28 runs   (    0.54 ms per token,  1853.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3438.32 ms /    28 runs   (  122.80 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  3518.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.71 ms /    50 runs   (    0.55 ms per token,  1804.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6231.80 ms /    50 runs   (  124.64 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time =  6376.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b)'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.77 ms /    52 runs   (    0.55 ms per token,  1807.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6420.77 ms /    52 runs   (  123.48 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  6570.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b)'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.41 ms /    27 runs   (    0.53 ms per token,  1873.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3296.31 ms /    27 runs   (  122.09 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3372.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.76 ms /    53 runs   (    0.58 ms per token,  1722.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6490.41 ms /    53 runs   (  122.46 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  6648.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b)'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.28 ms /    27 runs   (    0.60 ms per token,  1658.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3361.54 ms /    27 runs   (  124.50 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =  3444.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.89 ms /    52 runs   (    0.56 ms per token,  1800.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6408.29 ms /    52 runs   (  123.24 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  6561.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.87 ms /    52 runs   (    0.56 ms per token,  1801.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6402.96 ms /    52 runs   (  123.13 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  6557.02 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b)'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Qual das lentes abaixo melhor corrige um astigmatismo miópico simples?\n",
      "a)-3,00 dioptria esférica + 1,00 dioptria cilindrica X 90°.\n",
      "b)+1,00 dioptria esférica -1,00 dioptria cilindrica X 90°.\n",
      "c)-3,00 dioptria esférica + 3,00 dioptria cilindrica X 180°.\n",
      "d)0,00 dioptria esférica +1,00 dioptria cilindrica X 180°.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.10 ms /    20 runs   (    0.60 ms per token,  1653.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2424.67 ms /   157 tokens (   15.44 ms per token,    64.75 tokens per second)\n",
      "llama_print_timings:        eval time =  2311.49 ms /    19 runs   (  121.66 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  4796.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #1: \n",
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     5.17 ms /     9 runs   (    0.57 ms per token,  1739.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1119.54 ms /     9 runs   (  124.39 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =  1145.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    37.11 ms /    65 runs   (    0.57 ms per token,  1751.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7962.16 ms /    65 runs   (  122.49 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  8153.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.76 ms /    47 runs   (    0.57 ms per token,  1756.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5741.40 ms /    47 runs   (  122.16 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  5877.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c)-3,00 dioptria esférica + 3,00 dioptria cilindrica X 180°'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.85 ms /     7 runs   (    0.55 ms per token,  1816.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   858.14 ms /     7 runs   (  122.59 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =   878.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.66 ms /    37 runs   (    0.56 ms per token,  1790.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4516.88 ms /    37 runs   (  122.08 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  4623.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c)-3,00 dioptria esférica + 3,00 dioptria cilindrica X 180°'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.61 ms /    24 runs   (    0.57 ms per token,  1763.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2979.51 ms /    24 runs   (  124.15 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  3049.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    73.03 ms /   126 runs   (    0.58 ms per token,  1725.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 15539.46 ms /   126 runs   (  123.33 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 15919.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    35.77 ms /    63 runs   (    0.57 ms per token,  1761.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7795.76 ms /    63 runs   (  123.74 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  7981.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.76 ms /    29 runs   (    0.58 ms per token,  1730.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3553.21 ms /    29 runs   (  122.52 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3638.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 19: \n",
      "Language: english\n",
      "Question: \n",
      "What is the focal length of the circle of least confusion on Sturm's conoid of a +1.00 OD +2.00 DC X 180° spherocylindrical lens?\n",
      "a) 0 m.\n",
      "b) 0.5 m.\n",
      "c) 1 m.\n",
      "d) 2 m.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.16 ms /    27 runs   (    0.56 ms per token,  1781.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1961.77 ms /    84 tokens (   23.35 ms per token,    42.82 tokens per second)\n",
      "llama_print_timings:        eval time =  3182.51 ms /    26 runs   (  122.40 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  5223.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.95 ms /    29 runs   (    0.55 ms per token,  1818.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3571.28 ms /    29 runs   (  123.15 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  3655.83 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n",
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.28 ms /    12 runs   (    0.52 ms per token,  1912.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1442.13 ms /    12 runs   (  120.18 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:       total time =  1476.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.85 ms /    29 runs   (    0.55 ms per token,  1829.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3545.28 ms /    29 runs   (  122.25 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3628.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.55 ms /    30 runs   (    0.55 ms per token,  1812.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3675.36 ms /    30 runs   (  122.51 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3762.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.25 ms /    30 runs   (    0.54 ms per token,  1845.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3665.15 ms /    30 runs   (  122.17 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3751.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.68 ms /     7 runs   (    0.53 ms per token,  1903.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   853.01 ms /     7 runs   (  121.86 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =   872.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n",
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.94 ms /    29 runs   (    0.55 ms per token,  1819.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3544.31 ms /    29 runs   (  122.22 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3627.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.51 ms /    30 runs   (    0.55 ms per token,  1817.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3646.14 ms /    30 runs   (  121.54 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  3732.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.01 ms /    34 runs   (    0.56 ms per token,  1788.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4232.66 ms /    34 runs   (  124.49 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =  4332.21 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Qual é a distância focal do círculo de menor confusão no conoide de Sturm de uma lente esferocilíndrica de +1,00 DE +2,00 DC X 180°?\n",
      "a)0 metros.\n",
      "b)0,5 metros.\n",
      "c)1 metros.\n",
      "d)2 metros.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     5.64 ms /    10 runs   (    0.56 ms per token,  1773.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2181.16 ms /    87 tokens (   25.07 ms per token,    39.89 tokens per second)\n",
      "llama_print_timings:        eval time =  1104.78 ms /     9 runs   (  122.75 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  3315.03 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting respose to json: {:response:\"c\")1 metros.\"}\n",
      "Generating new response...\n",
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.23 ms /    22 runs   (    0.56 ms per token,  1799.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2678.25 ms /    22 runs   (  121.74 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  2742.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    45.19 ms /    79 runs   (    0.57 ms per token,  1748.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9673.60 ms /    79 runs   (  122.45 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  9907.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.22 ms /    33 runs   (    0.55 ms per token,  1811.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4038.91 ms /    33 runs   (  122.39 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  4134.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n",
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.54 ms /    12 runs   (    0.55 ms per token,  1834.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1485.67 ms /    12 runs   (  123.81 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  1520.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.50 ms /    12 runs   (    0.54 ms per token,  1845.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1488.14 ms /    12 runs   (  124.01 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  1522.60 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.87 ms /     7 runs   (    0.55 ms per token,  1806.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   856.05 ms /     7 runs   (  122.29 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =   875.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n",
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.37 ms /    29 runs   (    0.53 ms per token,  1886.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3552.27 ms /    29 runs   (  122.49 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3636.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.50 ms /    12 runs   (    0.54 ms per token,  1846.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1463.83 ms /    12 runs   (  121.99 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  1497.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.88 ms /     7 runs   (    0.55 ms per token,  1804.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   877.93 ms /     7 runs   (  125.42 ms per token,     7.97 tokens per second)\n",
      "llama_print_timings:       total time =   897.51 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.55 ms /    12 runs   (    0.55 ms per token,  1830.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1458.99 ms /    12 runs   (  121.58 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  1493.15 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 20: \n",
      "Language: english\n",
      "Question: \n",
      "What is the focal length of the circle of least confusion on Sturm's conoid of a +1.00 OD +2.00 DC X 180° spherocylindrical lens?\n",
      "a) 0 meters.\n",
      "b) 0.5 meters.\n",
      "c) 1 meters.\n",
      "d) 2 meters.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.64 ms /     7 runs   (    0.52 ms per token,  1922.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2083.86 ms /    84 tokens (   24.81 ms per token,    40.31 tokens per second)\n",
      "llama_print_timings:        eval time =   739.57 ms /     6 runs   (  123.26 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  2843.38 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.78 ms /    29 runs   (    0.54 ms per token,  1838.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3529.26 ms /    29 runs   (  121.70 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3613.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.31 ms /    33 runs   (    0.55 ms per token,  1802.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4037.85 ms /    33 runs   (  122.36 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  4133.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.61 ms /    29 runs   (    0.54 ms per token,  1858.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3628.25 ms /    29 runs   (  125.11 ms per token,     7.99 tokens per second)\n",
      "llama_print_timings:       total time =  3711.75 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.80 ms /    27 runs   (    0.55 ms per token,  1824.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3321.00 ms /    27 runs   (  123.00 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3398.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n",
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.55 ms /    32 runs   (    0.55 ms per token,  1823.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3891.39 ms /    32 runs   (  121.61 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3983.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.03 ms /    24 runs   (    0.54 ms per token,  1842.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2946.69 ms /    24 runs   (  122.78 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  3015.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.20 ms /    30 runs   (    0.54 ms per token,  1851.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3702.65 ms /    30 runs   (  123.42 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  3788.51 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.42 ms /    12 runs   (    0.53 ms per token,  1869.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1468.02 ms /    12 runs   (  122.33 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  1501.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.28 ms /    12 runs   (    0.52 ms per token,  1910.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1469.46 ms /    12 runs   (  122.46 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  1502.60 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Um ponto objetivo situado a dois metros de uma lente convexa de 2 DE forma um ponto imagem a que distância da lente?\n",
      "a)2,0 m.\n",
      "b)1,5 m.\n",
      "c)0,67 m.\n",
      "d)0,4 m.\n",
      "Test #0: \n",
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.97 ms /    33 runs   (    0.57 ms per token,  1739.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1347.45 ms /    74 tokens (   18.21 ms per token,    54.92 tokens per second)\n",
      "llama_print_timings:        eval time =  3912.33 ms /    32 runs   (  122.26 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  5356.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.56 ms /    20 runs   (    0.58 ms per token,  1729.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2496.71 ms /    20 runs   (  124.84 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time =  2555.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.18 ms /    32 runs   (    0.57 ms per token,  1760.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3922.46 ms /    32 runs   (  122.58 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  4015.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.54 ms /    22 runs   (    0.57 ms per token,  1754.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2684.88 ms /    22 runs   (  122.04 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  2748.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.52 ms /    20 runs   (    0.58 ms per token,  1736.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2399.11 ms /    20 runs   (  119.96 ms per token,     8.34 tokens per second)\n",
      "llama_print_timings:       total time =  2456.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1738.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   841.71 ms /     7 runs   (  120.24 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:       total time =   862.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.48 ms /    20 runs   (    0.57 ms per token,  1742.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2433.59 ms /    20 runs   (  121.68 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  2491.21 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.93 ms /    23 runs   (    0.56 ms per token,  1778.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2834.21 ms /    23 runs   (  123.23 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  2900.38 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     7.50 ms /    13 runs   (    0.58 ms per token,  1733.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1639.67 ms /    13 runs   (  126.13 ms per token,     7.93 tokens per second)\n",
      "llama_print_timings:       total time =  1677.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.59 ms /    24 runs   (    0.57 ms per token,  1766.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2924.98 ms /    24 runs   (  121.87 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  2993.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 21: \n",
      "Language: english\n",
      "Question: \n",
      "Which of the following values ​​in decimal logarithm (logMAR) corresponds to normal visual acuity (1.0)?\n",
      "to 1.\n",
      "b)0.\n",
      "c)1.\n",
      "d)2.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.98 ms /    24 runs   (    0.58 ms per token,  1716.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1300.50 ms /    56 tokens (   23.22 ms per token,    43.06 tokens per second)\n",
      "llama_print_timings:        eval time =  2778.92 ms /    23 runs   (  120.82 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  4150.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.72 ms /    28 runs   (    0.56 ms per token,  1780.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3446.54 ms /    28 runs   (  123.09 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  3527.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.23 ms /    30 runs   (    0.57 ms per token,  1741.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3650.62 ms /    30 runs   (  121.69 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3737.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n",
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.48 ms /    27 runs   (    0.57 ms per token,  1744.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3296.76 ms /    27 runs   (  122.10 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3375.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.44 ms /    27 runs   (    0.57 ms per token,  1748.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3287.53 ms /    27 runs   (  121.76 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3366.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.98 ms /     7 runs   (    0.57 ms per token,  1758.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   836.24 ms /     7 runs   (  119.46 ms per token,     8.37 tokens per second)\n",
      "llama_print_timings:       total time =   856.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.47 ms /    31 runs   (    0.56 ms per token,  1774.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3748.65 ms /    31 runs   (  120.92 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  3837.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.33 ms /    27 runs   (    0.57 ms per token,  1760.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3316.84 ms /    27 runs   (  122.85 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  3394.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.74 ms /    28 runs   (    0.56 ms per token,  1779.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3418.46 ms /    28 runs   (  122.09 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3499.88 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.62 ms /    28 runs   (    0.56 ms per token,  1792.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3462.21 ms /    28 runs   (  123.65 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  3542.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Qual dos valores abaixo em logaritmo decimal (logMAR) corresponde a acuidade visual normal (1,0)?\n",
      "a)-1.\n",
      "b)0.\n",
      "c)1.\n",
      "d)2.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.29 ms /    30 runs   (    0.58 ms per token,  1735.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1414.52 ms /    60 tokens (   23.58 ms per token,    42.42 tokens per second)\n",
      "llama_print_timings:        eval time =  3542.32 ms /    29 runs   (  122.15 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  5045.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.00 ms /    28 runs   (    0.57 ms per token,  1749.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3376.73 ms /    28 runs   (  120.60 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =  3458.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.28 ms /    23 runs   (    0.58 ms per token,  1731.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2838.37 ms /    23 runs   (  123.41 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  2905.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.98 ms /     7 runs   (    0.57 ms per token,  1757.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   854.41 ms /     7 runs   (  122.06 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =   874.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.46 ms /    27 runs   (    0.57 ms per token,  1746.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3293.51 ms /    27 runs   (  121.98 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3370.99 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.99 ms /    28 runs   (    0.57 ms per token,  1751.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3393.35 ms /    28 runs   (  121.19 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3474.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.69 ms /    24 runs   (    0.57 ms per token,  1753.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2907.91 ms /    24 runs   (  121.16 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  2978.60 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.25 ms /    27 runs   (    0.56 ms per token,  1770.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3320.72 ms /    27 runs   (  122.99 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3398.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.96 ms /     7 runs   (    0.57 ms per token,  1767.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   861.53 ms /     7 runs   (  123.08 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =   880.93 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.74 ms /    28 runs   (    0.56 ms per token,  1778.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3412.62 ms /    28 runs   (  121.88 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3493.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 22: \n",
      "Language: english\n",
      "Question: \n",
      "What is the optical effect resulting from the interpolation of two polaroid filters perpendicular to each other?\n",
      "a) Transmission is reduced by 50%.\n",
      "b) Transmission is increased by 50%.\n",
      "c) Absence of light transmission.\n",
      "d)No loss of light energy.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.65 ms /    53 runs   (    0.58 ms per token,  1729.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1679.52 ms /    69 tokens (   24.34 ms per token,    41.08 tokens per second)\n",
      "llama_print_timings:        eval time =  6321.90 ms /    52 runs   (  121.57 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  8158.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.40 ms /    27 runs   (    0.57 ms per token,  1753.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3269.12 ms /    27 runs   (  121.08 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  3347.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.48 ms /    34 runs   (    0.57 ms per token,  1745.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4124.45 ms /    34 runs   (  121.31 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  4224.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n",
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.86 ms /    34 runs   (    0.58 ms per token,  1712.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4130.85 ms /    34 runs   (  121.50 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  4230.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.03 ms /    28 runs   (    0.57 ms per token,  1746.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3460.33 ms /    28 runs   (  123.58 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  3542.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n",
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.04 ms /    31 runs   (    0.58 ms per token,  1718.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3759.76 ms /    31 runs   (  121.28 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3850.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.87 ms /    31 runs   (    0.58 ms per token,  1734.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3809.56 ms /    31 runs   (  122.89 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  3899.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c} Absence of light transmission.'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.64 ms /    42 runs   (    0.56 ms per token,  1776.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5104.50 ms /    42 runs   (  121.54 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  5227.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.88 ms /    28 runs   (    0.57 ms per token,  1762.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3490.23 ms /    28 runs   (  124.65 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time =  3571.75 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.61 ms /    31 runs   (    0.57 ms per token,  1759.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3830.26 ms /    31 runs   (  123.56 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  3919.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c} Absence of light transmission.'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Qual o efeito óptico decorrente da interpolação de dois filtros polaroides perpendiculares entre si?\n",
      "a)Transmissão reduzida em 50%.\n",
      "b)Transmissão ampliada em 50%.\n",
      "c)Ausência de transmissão luminosa.\n",
      "d)Nenhuma perda de energia luminosa.\n",
      "Test #0: \n",
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.29 ms /    45 runs   (    0.58 ms per token,  1711.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2020.58 ms /    94 tokens (   21.50 ms per token,    46.52 tokens per second)\n",
      "llama_print_timings:        eval time =  5347.26 ms /    44 runs   (  121.53 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  7500.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.17 ms /    46 runs   (    0.57 ms per token,  1757.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5652.79 ms /    46 runs   (  122.89 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  5786.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.15 ms /    32 runs   (    0.57 ms per token,  1762.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3864.37 ms /    32 runs   (  120.76 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  3957.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.73 ms /    19 runs   (    0.56 ms per token,  1770.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2320.34 ms /    19 runs   (  122.12 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  2374.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.34 ms /    20 runs   (    0.57 ms per token,  1764.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2449.59 ms /    20 runs   (  122.48 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  2507.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #5: \n",
      "{'response': 'a'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.71 ms /    19 runs   (    0.56 ms per token,  1773.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2296.67 ms /    19 runs   (  120.88 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  2351.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.94 ms /     7 runs   (    0.56 ms per token,  1776.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   847.27 ms /     7 runs   (  121.04 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =   866.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n",
      "{'response': 'c} Ausência de transmissão luminosa.'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.05 ms /    18 runs   (    0.56 ms per token,  1790.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2243.47 ms /    18 runs   (  124.64 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time =  2295.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    47.55 ms /    83 runs   (    0.57 ms per token,  1745.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10142.74 ms /    83 runs   (  122.20 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time = 10388.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.12 ms /    18 runs   (    0.56 ms per token,  1778.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2244.83 ms /    18 runs   (  124.71 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time =  2296.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 23: \n",
      "Language: english\n",
      "Question: \n",
      "In which of the materials is the speed of light the lowest?\n",
      "a) Air.\n",
      "b) Diamond.\n",
      "c) Glass.\n",
      "d) The speed of light is constant regardless of the medium.\n",
      "Test #0: \n",
      "{'response': 'd'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.37 ms /    24 runs   (    0.60 ms per token,  1670.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1173.62 ms /    50 tokens (   23.47 ms per token,    42.60 tokens per second)\n",
      "llama_print_timings:        eval time =  2815.44 ms /    23 runs   (  122.41 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  4060.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.50 ms /    27 runs   (    0.57 ms per token,  1741.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3276.11 ms /    27 runs   (  121.34 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3354.75 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.47 ms /    27 runs   (    0.57 ms per token,  1745.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3300.25 ms /    27 runs   (  122.23 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3379.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.46 ms /    27 runs   (    0.57 ms per token,  1746.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3321.24 ms /    27 runs   (  123.01 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3399.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #4: \n",
      "{'response': 'd'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.74 ms /    17 runs   (    0.57 ms per token,  1745.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2044.72 ms /    17 runs   (  120.28 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:       total time =  2093.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.55 ms /    24 runs   (    0.56 ms per token,  1770.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2889.61 ms /    24 runs   (  120.40 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:       total time =  2959.02 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.39 ms /    27 runs   (    0.57 ms per token,  1754.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3293.19 ms /    27 runs   (  121.97 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3371.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.20 ms /    24 runs   (    0.59 ms per token,  1689.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2911.85 ms /    24 runs   (  121.33 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  2981.82 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n",
      "{'response': 'C'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.29 ms /    28 runs   (    0.62 ms per token,  1619.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3438.92 ms /    28 runs   (  122.82 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  3524.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.44 ms /    27 runs   (    0.76 ms per token,  1321.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3264.05 ms /    27 runs   (  120.89 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  3363.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Em qual dos materiais abaixo a velocidade da luz é a menor?\n",
      "a)Ar.\n",
      "b)Diamante.\n",
      "c)Vidro.\n",
      "d)A velocidade da luz é constante independente do meio.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.56 ms /    27 runs   (    0.69 ms per token,  1454.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1481.29 ms /    63 tokens (   23.51 ms per token,    42.53 tokens per second)\n",
      "llama_print_timings:        eval time =  3158.42 ms /    26 runs   (  121.48 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  4730.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.33 ms /    28 runs   (    0.73 ms per token,  1377.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3384.37 ms /    28 runs   (  120.87 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  3475.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.17 ms /    28 runs   (    0.76 ms per token,  1322.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3393.56 ms /    28 runs   (  121.20 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3490.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.27 ms /    27 runs   (    0.57 ms per token,  1768.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3291.63 ms /    27 runs   (  121.91 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3370.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.34 ms /    27 runs   (    0.57 ms per token,  1760.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3301.24 ms /    27 runs   (  122.27 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3379.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.28 ms /    27 runs   (    0.57 ms per token,  1767.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3277.98 ms /    27 runs   (  121.41 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3356.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.92 ms /    42 runs   (    0.57 ms per token,  1755.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5152.80 ms /    42 runs   (  122.69 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5276.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.55 ms /    41 runs   (    0.57 ms per token,  1741.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5033.16 ms /    41 runs   (  122.76 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5154.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.26 ms /    27 runs   (    0.57 ms per token,  1768.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3350.07 ms /    27 runs   (  124.08 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  3427.54 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #9: \n",
      "{'response': 'C'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 24: \n",
      "Language: english\n",
      "Question: \n",
      "Mark the alternative that corresponds to a reflection from a convex mirror.\n",
      "a) Keratoscopy with Placido disc.\n",
      "b) Shaving mirror.\n",
      "c) Dentist's mirror.\n",
      "d) Reflector headlamp.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.95 ms /    42 runs   (    0.57 ms per token,  1753.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5094.01 ms /    42 runs   (  121.29 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  5214.93 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    41.62 ms /    72 runs   (    0.58 ms per token,  1729.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1396.44 ms /    58 tokens (   24.08 ms per token,    41.53 tokens per second)\n",
      "llama_print_timings:        eval time =  8654.61 ms /    71 runs   (  121.90 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time = 10262.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    24.11 ms /    42 runs   (    0.57 ms per token,  1742.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5116.44 ms /    42 runs   (  121.82 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  5236.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.96 ms /     7 runs   (    0.57 ms per token,  1768.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   840.85 ms /     7 runs   (  120.12 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:       total time =   861.00 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.47 ms /    20 runs   (    0.57 ms per token,  1744.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2412.73 ms /    20 runs   (  120.64 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =  2469.83 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.23 ms /    32 runs   (    0.57 ms per token,  1755.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3911.88 ms /    32 runs   (  122.25 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  4003.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.91 ms /    33 runs   (    0.57 ms per token,  1744.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3954.08 ms /    33 runs   (  119.82 ms per token,     8.35 tokens per second)\n",
      "llama_print_timings:       total time =  4049.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.54 ms /    29 runs   (    0.57 ms per token,  1753.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3593.09 ms /    29 runs   (  123.90 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  3675.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n",
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.65 ms /    22 runs   (    0.57 ms per token,  1739.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2694.92 ms /    22 runs   (  122.50 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  2758.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.15 ms /    33 runs   (    0.58 ms per token,  1722.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3991.33 ms /    33 runs   (  120.95 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  4086.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Assinale a alternativa que corresponde a uma reflexão de um espelho convexo.\n",
      "a)Ceratoscopia com disco de Plácido.\n",
      "b)Espelho de barbear.\n",
      "c)Espelho do dentista.\n",
      "d)Farol refletor.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.23 ms /    30 runs   (    0.57 ms per token,  1741.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3607.61 ms /    30 runs   (  120.25 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:       total time =  3693.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    34.43 ms /    60 runs   (    0.57 ms per token,  1742.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1824.20 ms /    75 tokens (   24.32 ms per token,    41.11 tokens per second)\n",
      "llama_print_timings:        eval time =  7259.25 ms /    59 runs   (  123.04 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  9261.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    24.53 ms /    43 runs   (    0.57 ms per token,  1752.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5231.91 ms /    43 runs   (  121.67 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  5356.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n",
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    24.38 ms /    43 runs   (    0.57 ms per token,  1763.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5225.49 ms /    43 runs   (  121.52 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  5348.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.82 ms /     7 runs   (    0.55 ms per token,  1831.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   842.57 ms /     7 runs   (  120.37 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:       total time =   862.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.90 ms /     7 runs   (    0.56 ms per token,  1793.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   864.75 ms /     7 runs   (  123.54 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =   884.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n",
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.83 ms /    42 runs   (    0.57 ms per token,  1762.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5159.92 ms /    42 runs   (  122.86 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  5282.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.84 ms /    42 runs   (    0.57 ms per token,  1761.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5089.88 ms /    42 runs   (  121.19 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  5211.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.84 ms /    42 runs   (    0.57 ms per token,  1761.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5168.46 ms /    42 runs   (  123.06 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  5291.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n",
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.01 ms /    25 runs   (    0.56 ms per token,  1784.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3067.21 ms /    25 runs   (  122.69 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  3139.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.90 ms /     7 runs   (    0.56 ms per token,  1793.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   845.21 ms /     7 runs   (  120.74 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =   864.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 25: \n",
      "Language: english\n",
      "Question: \n",
      "\n",
      "A converging lens focuses light from a source 1 m away at a distance of 50 cm. How powerful is this lens?\n",
      "a) + 1 diopter.\n",
      "b) + 2 diopter.\n",
      "c) + 3 diopter.\n",
      "d) + 4 diopter.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.70 ms /    32 runs   (    0.55 ms per token,  1807.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2022.58 ms /    75 tokens (   26.97 ms per token,    37.08 tokens per second)\n",
      "llama_print_timings:        eval time =  3784.85 ms /    31 runs   (  122.09 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  5900.21 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   125.64 ms /   221 runs   (    0.57 ms per token,  1758.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 27247.50 ms /   221 runs   (  123.29 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 27926.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.18 ms /    31 runs   (    0.55 ms per token,  1803.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3832.02 ms /    31 runs   (  123.61 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  3920.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.48 ms /    33 runs   (    0.56 ms per token,  1785.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4087.98 ms /    33 runs   (  123.88 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  4183.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b) + 2 diopter'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    66.01 ms /   116 runs   (    0.57 ms per token,  1757.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14252.84 ms /   116 runs   (  122.87 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 14596.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.05 ms /    34 runs   (    0.62 ms per token,  1615.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4159.20 ms /    34 runs   (  122.33 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  4263.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    75.86 ms /   131 runs   (    0.58 ms per token,  1726.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16114.37 ms /   131 runs   (  123.01 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 16513.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    75.11 ms /   131 runs   (    0.57 ms per token,  1744.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16054.66 ms /   131 runs   (  122.55 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time = 16447.51 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   106.56 ms /   186 runs   (    0.57 ms per token,  1745.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 22856.15 ms /   186 runs   (  122.88 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 23423.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.03 ms /    34 runs   (    0.56 ms per token,  1786.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4120.27 ms /    34 runs   (  121.18 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  4218.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Uma lente convergente foca a luz proveniente de uma fonte situada a 1 m a uma distância de 50 cm. Qual o poder desta lente?\n",
      "a)+ 1 dioptria.\n",
      "b)+ 2 dioptria.\n",
      "c)+ 3 dioptria.\n",
      "d)+ 4 dioptria.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     8.53 ms /    15 runs   (    0.57 ms per token,  1757.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2035.81 ms /    86 tokens (   23.67 ms per token,    42.24 tokens per second)\n",
      "llama_print_timings:        eval time =  1697.83 ms /    14 runs   (  121.27 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3777.75 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.27 ms /    32 runs   (    0.57 ms per token,  1751.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3932.69 ms /    32 runs   (  122.90 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  4026.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c)'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.78 ms /    27 runs   (    0.55 ms per token,  1827.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3306.22 ms /    27 runs   (  122.45 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3385.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.00 ms /    27 runs   (    0.56 ms per token,  1799.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3303.13 ms /    27 runs   (  122.34 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3382.86 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b)+ 2 dioptria'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.03 ms /    36 runs   (    0.56 ms per token,  1797.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4408.79 ms /    36 runs   (  122.47 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  4513.82 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a) + 1 dioptria.'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.80 ms /    34 runs   (    0.55 ms per token,  1808.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4174.06 ms /    34 runs   (  122.77 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  4273.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a)'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.38 ms /    35 runs   (    0.55 ms per token,  1806.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4288.30 ms /    35 runs   (  122.52 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  4389.68 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a)'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.30 ms /    30 runs   (    0.54 ms per token,  1840.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3732.37 ms /    30 runs   (  124.41 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =  3818.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.38 ms /    33 runs   (    0.56 ms per token,  1795.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4050.62 ms /    33 runs   (  122.75 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  4145.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a) 1 dioptria'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.63 ms /    31 runs   (    0.54 ms per token,  1864.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3785.51 ms /    31 runs   (  122.11 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3873.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c)'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 26: \n",
      "Language: english\n",
      "Question: \n",
      "If we were to measure the focal power of the image of the posterior surface of the cornea after its removal from the eyeball, we would have a value around (consider the anterior curvature of the cornea 7.7 mm; the posterior curvature of the cornea 6.8 mm; the index of corneal stroma refraction 1.376 and the refractive index of air 1.00):\n",
      "a) + 5.00 Dioptres.\n",
      "b) 0.00 Dioptres.\n",
      "c)- 5.00 Dioptres.\n",
      "d)- 55.00 Dioptres.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.28 ms /    40 runs   (    0.56 ms per token,  1795.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3619.23 ms /   158 tokens (   22.91 ms per token,    43.66 tokens per second)\n",
      "llama_print_timings:        eval time =  4815.68 ms /    39 runs   (  123.48 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  8551.83 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.26 ms /    40 runs   (    0.56 ms per token,  1797.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4905.15 ms /    40 runs   (  122.63 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5021.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.55 ms /    39 runs   (    0.55 ms per token,  1810.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4768.41 ms /    39 runs   (  122.27 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  4882.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.29 ms /    39 runs   (    0.57 ms per token,  1749.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4824.87 ms /    39 runs   (  123.71 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  4941.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.66 ms /    34 runs   (    0.55 ms per token,  1822.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4176.18 ms /    34 runs   (  122.83 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  4276.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   119.42 ms /   209 runs   (    0.57 ms per token,  1750.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 26017.82 ms /   209 runs   (  124.49 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time = 26670.63 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   118.94 ms /   209 runs   (    0.57 ms per token,  1757.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 25874.75 ms /   209 runs   (  123.80 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time = 26523.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.67 ms /    43 runs   (    0.55 ms per token,  1816.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5315.45 ms /    43 runs   (  123.62 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  5441.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.96 ms /    40 runs   (    0.55 ms per token,  1821.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5020.14 ms /    40 runs   (  125.50 ms per token,     7.97 tokens per second)\n",
      "llama_print_timings:       total time =  5137.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.76 ms /    36 runs   (    0.55 ms per token,  1821.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4400.24 ms /    36 runs   (  122.23 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  4504.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Se fossemos medir o poder focal imagem da face posterior da córnea após sua retirada do globo ocular, teríamos um valor ao redor de (considere a curvatura anterior da córnea 7,7 mm; a curvatura posterior da córnea 6,8 mm; o índice de refração do estroma corneano 1,376 e o índice de refração do ar 1,00):\n",
      "a)+ 5,00 Dioptrias.\n",
      "b)0,00 Dioptrias.\n",
      "c)- 5,00 Dioptrias.\n",
      "d)- 55,00 Dioptrias.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.45 ms /    34 runs   (    0.54 ms per token,  1842.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3035.76 ms /   165 tokens (   18.40 ms per token,    54.35 tokens per second)\n",
      "llama_print_timings:        eval time =  4063.92 ms /    33 runs   (  123.15 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  7199.68 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.86 ms /    36 runs   (    0.52 ms per token,  1908.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4411.43 ms /    36 runs   (  122.54 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  4516.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.44 ms /    41 runs   (    0.55 ms per token,  1826.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5045.32 ms /    41 runs   (  123.06 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  5165.57 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.04 ms /    18 runs   (    0.50 ms per token,  1991.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2200.27 ms /    18 runs   (  122.24 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  2252.93 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.20 ms /    18 runs   (    0.51 ms per token,  1956.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2187.25 ms /    18 runs   (  121.51 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  2239.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.32 ms /    38 runs   (    0.53 ms per token,  1870.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4697.02 ms /    38 runs   (  123.61 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  4809.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a) 5,00 Dioptrias'}\n",
      "Test #6: \n",
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.26 ms /    18 runs   (    0.51 ms per token,  1943.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2204.49 ms /    18 runs   (  122.47 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  2256.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.15 ms /    39 runs   (    0.54 ms per token,  1844.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4795.32 ms /    39 runs   (  122.96 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  4909.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.88 ms /    37 runs   (    0.54 ms per token,  1861.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4550.02 ms /    37 runs   (  122.97 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  4658.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.37 ms /    38 runs   (    0.54 ms per token,  1865.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4645.96 ms /    38 runs   (  122.26 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  4757.07 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 27: \n",
      "Language: english\n",
      "Question: \n",
      "When reducing the cylinder prescription in a patient who did not adapt to his glasses with the following prescription -1.50 spherical diopter < > -3.50 cylindrical diopter X 180°, a possible prescription maintaining the spherical equivalent is:\n",
      "a)- 2.50 spherical diopter < > -2.50 cylindrical diopter X 180°.\n",
      "b) +1.50 spherical diopter < > -2.50 cylindrical diopter X 180°.\n",
      "c) -4.50 spherical diopter < > +2.50 cylindrical diopter X 90°.\n",
      "d) -2.50 spherical diopter < > +3.00 cylindrical diopter X 90°.a)- 2.50 spherical diopter < > -2.50 cylindrical diopter X 180°.\n",
      "b) +1.50 spherical diopter < > -2.50 cylindrical diopter X 180°.\n",
      "c) -4.50 spherical diopter < > +2.50 cylindrical diopter X 90°.\n",
      "d) -2.50 spherical diopter < > +3.00 cylindrical diopter X 90°.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.51 ms /    55 runs   (    0.57 ms per token,  1745.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5475.88 ms /   312 tokens (   17.55 ms per token,    56.98 tokens per second)\n",
      "llama_print_timings:        eval time =  6673.86 ms /    54 runs   (  123.59 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 12314.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b)'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.10 ms /    55 runs   (    0.57 ms per token,  1768.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6862.75 ms /    55 runs   (  124.78 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time =  7024.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.27 ms /    55 runs   (    0.57 ms per token,  1758.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6841.79 ms /    55 runs   (  124.40 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =  7005.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a)- 2.50 spherical diopter < > -2.50 cylindrical diopter X 180°'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.94 ms /    55 runs   (    0.56 ms per token,  1777.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6851.32 ms /    55 runs   (  124.57 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =  7014.57 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a)- 2.50 spherical diopter < > -2.50 cylindrical diopter X 180°'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.83 ms /    55 runs   (    0.56 ms per token,  1784.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6803.94 ms /    55 runs   (  123.71 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  6966.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'Answer option'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.88 ms /    32 runs   (    0.56 ms per token,  1789.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3975.47 ms /    32 runs   (  124.23 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =  4068.68 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n",
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.70 ms /    49 runs   (    0.57 ms per token,  1769.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6024.73 ms /    49 runs   (  122.95 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  6168.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.07 ms /    55 runs   (    0.56 ms per token,  1769.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6849.20 ms /    55 runs   (  124.53 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =  7012.02 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n",
      "Error converting respose to json:  Sure, I'm ready to assist you! Here's my response in JSON format:\n",
      "\n",
      "{\"response\": \"b\") +1.50 spherical diopter < > -2.50 cylindrical diopter X 180°.\"\n",
      "Generating new response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.22 ms /    55 runs   (    0.57 ms per token,  1761.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6788.20 ms /    55 runs   (  123.42 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  6950.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c)'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.34 ms /    54 runs   (    0.56 ms per token,  1779.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6646.68 ms /    54 runs   (  123.09 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  6805.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.44 ms /    55 runs   (    0.57 ms per token,  1749.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6817.41 ms /    55 runs   (  123.95 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  6979.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting respose to json:  Sure, I'm ready to assist you! Please provide the medical query in English or Portuguese with the appropriate delimiters (i.e., ####). I will respond with the answer in JSON format as requested.\n",
      "\n",
      "Here is the correct answer to the sample question\n",
      "Generating new response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.15 ms /    55 runs   (    0.57 ms per token,  1765.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6808.78 ms /    55 runs   (  123.80 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  6970.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting respose to json:  Sure, I'm ready to assist you! Here's the answer to your medical query in JSON format:\n",
      "\n",
      "{\"response\": \"a\") -2.50 spherical diopter < > -2.50 cylindrical diopter X 1\n",
      "Generating new response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.99 ms /    55 runs   (    0.56 ms per token,  1774.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6811.73 ms /    55 runs   (  123.85 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  6971.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting respose to json:  Sure, I'm ready to assist you! Please provide the medical query in English or Portuguese, delimited with #### characters, and I will provide the answer in JSON format with the key \"response\".\n",
      "\n",
      "Here is the correct answer for your first question:\n",
      "\n",
      "Generating new response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.07 ms /    29 runs   (    0.59 ms per token,  1698.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3574.83 ms /    29 runs   (  123.27 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  3662.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting respose to json:  Sure, I'm ready to assist you! Please provide the medical query with the four possible answer options delimited with #### characters.\n",
      "Generating new response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    34.91 ms /    54 runs   (    0.65 ms per token,  1546.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6690.05 ms /    54 runs   (  123.89 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  6856.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Ao reduzirmos a prescrição do cilindro em um paciente que não se adaptou a seus óculos com a seguinte prescrição -1,50 Diotria esferica < > -3,50 dioptria cilindrica X 180°, uma possível prescrição mantendo o equivalente esférico é:\n",
      "a)- 2,50 dioptria esferica < > -2,50 dioptria cilindrica X 180°.\n",
      "b)+1,50 dioptria esferica < > -2,50 dioptria cilindrica X 180°.\n",
      "c)-4,50 dioptria esferica < > +2,50 dioptria cilindrica X 90°.\n",
      "d)-2,50 dioptria esferica < > +3,00 dioptria cilindrica X 90°.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.07 ms /     7 runs   (    0.58 ms per token,  1721.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3762.66 ms /   225 tokens (   16.72 ms per token,    59.80 tokens per second)\n",
      "llama_print_timings:        eval time =   725.83 ms /     6 runs   (  120.97 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  4508.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.51 ms /    27 runs   (    0.57 ms per token,  1740.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3301.31 ms /    27 runs   (  122.27 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3379.85 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    81.54 ms /   142 runs   (    0.57 ms per token,  1741.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 17624.71 ms /   142 runs   (  124.12 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time = 18049.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     5.04 ms /     9 runs   (    0.56 ms per token,  1785.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1106.18 ms /     9 runs   (  122.91 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  1132.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1745.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   862.41 ms /     7 runs   (  123.20 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =   882.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n",
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.41 ms /    20 runs   (    0.57 ms per token,  1752.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2479.41 ms /    20 runs   (  123.97 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  2536.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.94 ms /     7 runs   (    0.56 ms per token,  1778.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   868.48 ms /     7 runs   (  124.07 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =   888.98 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n",
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.00 ms /     7 runs   (    0.57 ms per token,  1751.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   846.43 ms /     7 runs   (  120.92 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =   866.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.00 ms /     7 runs   (    0.57 ms per token,  1747.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   866.49 ms /     7 runs   (  123.78 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =   886.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.97 ms /     7 runs   (    0.57 ms per token,  1761.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   850.15 ms /     7 runs   (  121.45 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =   869.88 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 28: \n",
      "Language: english\n",
      "Question: \n",
      "A light ray is deviated by 20 mm from the visual axis at a distance of 50 cm from a prism. What is the power of the prism in diopters-prismatics?\n",
      "a) 0.4.\n",
      "b)2.5.\n",
      "c)4.\n",
      "d) 10.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.96 ms /    29 runs   (    0.58 ms per token,  1709.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1922.31 ms /    78 tokens (   24.64 ms per token,    40.58 tokens per second)\n",
      "llama_print_timings:        eval time =  3410.60 ms /    28 runs   (  121.81 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  5418.93 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.84 ms /    24 runs   (    0.58 ms per token,  1733.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2932.00 ms /    24 runs   (  122.17 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3001.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.82 ms /    24 runs   (    0.58 ms per token,  1736.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2906.08 ms /    24 runs   (  121.09 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  2976.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.95 ms /    26 runs   (    0.58 ms per token,  1738.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3150.94 ms /    26 runs   (  121.19 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3226.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.02 ms /    28 runs   (    0.57 ms per token,  1748.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3487.86 ms /    28 runs   (  124.57 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =  3571.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n",
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.77 ms /    24 runs   (    0.57 ms per token,  1742.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2883.59 ms /    24 runs   (  120.15 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:       total time =  2952.54 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.94 ms /    28 runs   (    0.57 ms per token,  1757.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3408.85 ms /    28 runs   (  121.74 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3489.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.34 ms /    29 runs   (    0.56 ms per token,  1774.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3558.81 ms /    29 runs   (  122.72 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  3642.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.72 ms /    24 runs   (    0.57 ms per token,  1748.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2915.45 ms /    24 runs   (  121.48 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  2985.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.81 ms /    24 runs   (    0.58 ms per token,  1738.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2925.80 ms /    24 runs   (  121.91 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  2996.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Um raio de luz sofre desvio de 20 mm do eixo visual a uma distância de 50 cm de um prima. Qual é o poder do prisma em dioptrias-prismáticas?\n",
      "a)0,4.\n",
      "b)2,5.\n",
      "c)4.\n",
      "d)10.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     7.33 ms /    12 runs   (    0.61 ms per token,  1636.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2062.66 ms /    83 tokens (   24.85 ms per token,    40.24 tokens per second)\n",
      "llama_print_timings:        eval time =  1357.47 ms /    11 runs   (  123.41 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  3456.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.66 ms /    29 runs   (    0.57 ms per token,  1740.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3539.55 ms /    29 runs   (  122.05 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3624.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n",
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.04 ms /    27 runs   (    0.56 ms per token,  1795.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3304.68 ms /    27 runs   (  122.40 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3383.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.58 ms /    31 runs   (    0.57 ms per token,  1763.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3769.31 ms /    31 runs   (  121.59 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3859.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.97 ms /     7 runs   (    0.57 ms per token,  1762.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   834.28 ms /     7 runs   (  119.18 ms per token,     8.39 tokens per second)\n",
      "llama_print_timings:       total time =   853.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n",
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    95.79 ms /   168 runs   (    0.57 ms per token,  1753.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 20593.88 ms /   168 runs   (  122.58 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time = 21112.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.26 ms /    25 runs   (    0.57 ms per token,  1753.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3024.44 ms /    25 runs   (  120.98 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  3097.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n",
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.58 ms /    31 runs   (    0.57 ms per token,  1763.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3782.19 ms /    31 runs   (  122.01 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3873.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.55 ms /    31 runs   (    0.57 ms per token,  1765.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3786.29 ms /    31 runs   (  122.14 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3876.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.68 ms /    26 runs   (    0.56 ms per token,  1771.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3197.62 ms /    26 runs   (  122.99 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3272.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 29: \n",
      "Language: english\n",
      "Question: \n",
      "\n",
      "Considering Gullstrand's schematic eye, what is the characteristic of the image formed on the retina?\n",
      "a) Real, inverted.\n",
      "b) Virtual, inverted.\n",
      "c) Real, direct.\n",
      "d) Virtual, direct.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.75 ms /    22 runs   (    0.58 ms per token,  1725.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1501.64 ms /    64 tokens (   23.46 ms per token,    42.62 tokens per second)\n",
      "llama_print_timings:        eval time =  2562.80 ms /    21 runs   (  122.04 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  4129.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.05 ms /    23 runs   (    0.57 ms per token,  1762.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2796.12 ms /    23 runs   (  121.57 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  2864.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.41 ms /    22 runs   (    0.56 ms per token,  1772.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2670.70 ms /    22 runs   (  121.40 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  2735.26 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.92 ms /    23 runs   (    0.56 ms per token,  1780.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2849.94 ms /    23 runs   (  123.91 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  2916.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.86 ms /    23 runs   (    0.56 ms per token,  1787.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2820.32 ms /    23 runs   (  122.62 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  2886.67 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #5: \n",
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.87 ms /    23 runs   (    0.56 ms per token,  1786.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2792.51 ms /    23 runs   (  121.41 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  2860.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.81 ms /    23 runs   (    0.56 ms per token,  1795.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2809.04 ms /    23 runs   (  122.13 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  2876.88 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.84 ms /    51 runs   (    0.57 ms per token,  1768.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6237.91 ms /    51 runs   (  122.31 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  6389.93 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.68 ms /    32 runs   (    0.58 ms per token,  1713.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3895.21 ms /    32 runs   (  121.73 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3991.82 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n",
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Considerando o olho esquemático de Gullstrand, qual a característica da imagem formada na retina?\n",
      "a)Real, invertida.\n",
      "b)Virtual, invertida.\n",
      "c)Real, direta.\n",
      "d)Virtual, direta.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.09 ms /    30 runs   (    0.57 ms per token,  1755.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3648.80 ms /    30 runs   (  121.63 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3737.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.56 ms /    32 runs   (    0.58 ms per token,  1723.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1242.33 ms /    67 tokens (   18.54 ms per token,    53.93 tokens per second)\n",
      "llama_print_timings:        eval time =  3832.49 ms /    31 runs   (  123.63 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  5171.85 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.00 ms /    23 runs   (    0.57 ms per token,  1769.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2775.37 ms /    23 runs   (  120.67 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =  2842.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c} Real, direta.'}\n",
      "Test #2: \n",
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.07 ms /    32 runs   (    0.56 ms per token,  1770.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3880.81 ms /    32 runs   (  121.28 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3975.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.75 ms /    33 runs   (    0.57 ms per token,  1759.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4033.75 ms /    33 runs   (  122.23 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  4130.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.12 ms /    30 runs   (    0.57 ms per token,  1752.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3654.56 ms /    30 runs   (  121.82 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3743.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.47 ms /    22 runs   (    0.57 ms per token,  1764.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2669.61 ms /    22 runs   (  121.35 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  2733.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.17 ms /    32 runs   (    0.57 ms per token,  1761.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3876.05 ms /    32 runs   (  121.13 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  3970.24 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.72 ms /    26 runs   (    0.57 ms per token,  1766.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3172.12 ms /    26 runs   (  122.00 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3247.68 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.16 ms /    25 runs   (    0.57 ms per token,  1766.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3059.15 ms /    25 runs   (  122.37 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3132.85 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.74 ms /    12 runs   (    0.56 ms per token,  1781.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1458.79 ms /    12 runs   (  121.57 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  1493.32 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 30: \n",
      "Language: english\n",
      "Question: \n",
      "Mark the alternative that correctly correlates the classes of antibacterials to their respective main sites of action.\n",
      "\n",
      "I- Cephalosporins\n",
      "II- Tetracyclines\n",
      "III- Quinolones\n",
      "IV- Macrolides\n",
      "\n",
      "A- Protein synthesis (30s inhibitor)\n",
      "B- Protein synthesis (50s inhibitor)\n",
      "C DNA gyrase\n",
      "D- Cell wall synthesis\n",
      "\n",
      "a) I: A; II: D; III: D; IV: b.\n",
      "b)I:D; II: A; III: C; IV: b.\n",
      "c) I: A; II: D; III: B; IV: c.\n",
      "d)I:D; II: B; III: A; IV: c.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    76.95 ms /   133 runs   (    0.58 ms per token,  1728.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3974.68 ms /   180 tokens (   22.08 ms per token,    45.29 tokens per second)\n",
      "llama_print_timings:        eval time = 16294.69 ms /   132 runs   (  123.44 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 20675.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.91 ms /    27 runs   (    0.55 ms per token,  1810.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3293.78 ms /    27 runs   (  121.99 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3372.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    77.92 ms /   136 runs   (    0.57 ms per token,  1745.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16913.89 ms /   136 runs   (  124.37 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time = 17329.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n",
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    97.09 ms /   170 runs   (    0.57 ms per token,  1750.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 21063.23 ms /   170 runs   (  123.90 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time = 21587.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   106.55 ms /   187 runs   (    0.57 ms per token,  1755.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 22906.42 ms /   187 runs   (  122.49 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time = 23480.21 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    82.80 ms /   145 runs   (    0.57 ms per token,  1751.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 17789.22 ms /   145 runs   (  122.68 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 18229.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n",
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    34.80 ms /    61 runs   (    0.57 ms per token,  1752.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7650.55 ms /    61 runs   (  125.42 ms per token,     7.97 tokens per second)\n",
      "llama_print_timings:       total time =  7832.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    88.39 ms /   155 runs   (    0.57 ms per token,  1753.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 18972.51 ms /   155 runs   (  122.40 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time = 19446.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   106.35 ms /   187 runs   (    0.57 ms per token,  1758.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 22964.04 ms /   187 runs   (  122.80 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 23537.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    93.75 ms /   165 runs   (    0.57 ms per token,  1760.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 20431.35 ms /   165 runs   (  123.83 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time = 20936.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Assinale a alternativa que correlaciona corretamente as classes de antibacterianos aos seus respectivos principais sítios de ação.\n",
      "\n",
      "I- Cefalosporinas\n",
      "II- Tetraciclínicas\n",
      "III- Quinolonas\n",
      "IV- Macrolídeos\n",
      "\n",
      "A- Síntese proteica (inibidor 30s)\n",
      "B- Síntese proteica (inibidor 50s)\n",
      "C- DNA girase\n",
      "D- Síntese de parede celular\n",
      "\n",
      "a)I: A; II: D; III: D; IV: B.\n",
      "b)I: D; II: A; III: C; IV: B.\n",
      "c)I: A; II: D; III: B; IV: C.\n",
      "d)I: D; II: B; III: A; IV: C.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.44 ms /    44 runs   (    0.58 ms per token,  1729.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3809.11 ms /   206 tokens (   18.49 ms per token,    54.08 tokens per second)\n",
      "llama_print_timings:        eval time =  5328.22 ms /    43 runs   (  123.91 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  9270.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.40 ms /     7 runs   (    0.49 ms per token,  2056.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   849.88 ms /     7 runs   (  121.41 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =   869.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    24.01 ms /    43 runs   (    0.56 ms per token,  1791.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5275.78 ms /    43 runs   (  122.69 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5402.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    24.21 ms /    44 runs   (    0.55 ms per token,  1817.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5387.96 ms /    44 runs   (  122.45 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  5516.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.41 ms /     7 runs   (    0.49 ms per token,  2051.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   858.26 ms /     7 runs   (  122.61 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =   877.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    24.31 ms /    44 runs   (    0.55 ms per token,  1809.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5437.81 ms /    44 runs   (  123.59 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  5568.58 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    24.26 ms /    44 runs   (    0.55 ms per token,  1813.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5401.53 ms /    44 runs   (  122.76 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5531.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.43 ms /     7 runs   (    0.49 ms per token,  2038.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   856.11 ms /     7 runs   (  122.30 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =   875.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    91.91 ms /   161 runs   (    0.57 ms per token,  1751.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 20014.24 ms /   161 runs   (  124.31 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time = 20508.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    95.67 ms /   161 runs   (    0.59 ms per token,  1682.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 19908.96 ms /   161 runs   (  123.66 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 20412.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 31: \n",
      "Language: english\n",
      "Question: \n",
      "A patient with Sjogren's syndrome started to have improvements in his oral symptoms (increased salivation) after the non-ophthalmologist prescribed:\n",
      "a) Brimonidine, however, evolved with punctiform miosis and low vision.\n",
      "b) Tetracaine, however, evolved with traditional retinal detachment.\n",
      "c) Atropine, however, evolved with medium medriasis and accommodation difficulties.\n",
      "d) Pilocarpine, however, evolved with accommodative spasm.\n",
      "Test #0: \n",
      "{'response': 'd) Pilocarpine'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.45 ms /    32 runs   (    0.58 ms per token,  1734.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2666.97 ms /   127 tokens (   21.00 ms per token,    47.62 tokens per second)\n",
      "llama_print_timings:        eval time =  3769.51 ms /    31 runs   (  121.60 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  6531.68 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.30 ms /    44 runs   (    0.58 ms per token,  1738.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5407.67 ms /    44 runs   (  122.90 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  5538.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Pilocarpine, however, evolved with accommodative spasm.'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     8.55 ms /    14 runs   (    0.61 ms per token,  1637.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1713.76 ms /    14 runs   (  122.41 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  1757.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Pilocarpine'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.87 ms /    30 runs   (    0.73 ms per token,  1371.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3620.84 ms /    30 runs   (  120.69 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =  3723.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Pilocarpine'}\n",
      "Test #4: \n",
      "{'response': 'd) Pilocarpine, however, evolved with accommodative spasm.'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.70 ms /    40 runs   (    0.57 ms per token,  1762.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4877.26 ms /    40 runs   (  121.93 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  4996.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Pilocarpine'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.67 ms /    33 runs   (    0.57 ms per token,  1767.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4040.41 ms /    33 runs   (  122.44 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  4137.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.07 ms /    41 runs   (    0.56 ms per token,  1777.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5052.03 ms /    41 runs   (  123.22 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  5173.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Pilocarpine, however, evolved with accommodative spasm.'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.02 ms /    41 runs   (    0.56 ms per token,  1781.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5082.93 ms /    41 runs   (  123.97 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  5204.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Pilocarpine, however, evolved with accommodative spasm.'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.85 ms /    30 runs   (    0.56 ms per token,  1780.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3719.93 ms /    30 runs   (  124.00 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  3809.12 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Pilocarpine'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.91 ms /    37 runs   (    0.57 ms per token,  1769.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4623.64 ms /    37 runs   (  124.96 ms per token,     8.00 tokens per second)\n",
      "llama_print_timings:       total time =  4733.51 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Pilocarpine, however, evolved with accommodative spasm.'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Paciente com síndrome Sjogren passou a ter melhoras dos seus sintomas orais (aumento da salivação) após o médico não-oftalmologista prescrever:\n",
      "a)Brimonidina, todavia, evoluiu com miose puntiforme e baixa de visão.\n",
      "b)Tetracaína, todavia, evoluiu com descolamento de retina tradicional.\n",
      "c)Atropina, todavia, evoluiu com média medríase e dificuldades de acomodação.\n",
      "d)Pilocarpina, todavia, evoluiu com espasmo acomodativo.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.93 ms /    48 runs   (    0.58 ms per token,  1718.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3761.16 ms /   159 tokens (   23.66 ms per token,    42.27 tokens per second)\n",
      "llama_print_timings:        eval time =  5861.08 ms /    47 runs   (  124.70 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time =  9766.21 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Pilocarpina, todavia, evoluiu com espasmo acomodativo.'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.53 ms /    48 runs   (    0.59 ms per token,  1682.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5941.88 ms /    48 runs   (  123.79 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  6087.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Pilocarpina, todavia, evoluiu com espasmo acomodativo.'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.62 ms /    48 runs   (    0.58 ms per token,  1737.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5892.21 ms /    48 runs   (  122.75 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  6035.07 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Pilocarpina, todavia, evoluiu com espasmo acomodativo.'}\n",
      "Test #3: \n",
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.05 ms /     7 runs   (    0.58 ms per token,  1727.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   880.23 ms /     7 runs   (  125.75 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:       total time =   901.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.45 ms /    11 runs   (    0.59 ms per token,  1704.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1344.19 ms /    11 runs   (  122.20 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  1376.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.09 ms /     7 runs   (    0.58 ms per token,  1712.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   869.44 ms /     7 runs   (  124.21 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =   889.75 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.09 ms /    28 runs   (    0.57 ms per token,  1739.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3471.38 ms /    28 runs   (  123.98 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  3554.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Pilocarpina, todavia, evoluiu com espasmo acomodativo.'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.61 ms /    48 runs   (    0.58 ms per token,  1738.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5979.25 ms /    48 runs   (  124.57 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =  6123.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Pilocarpina, todavia, evoluiu com espasmo acomodativo.'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    66.68 ms /   115 runs   (    0.58 ms per token,  1724.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14101.88 ms /   115 runs   (  122.62 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 14450.93 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.55 ms /    49 runs   (    0.58 ms per token,  1716.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6068.83 ms /    49 runs   (  123.85 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  6215.54 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Pilocarpina, todavia, evoluiu com espasmo acomodativo.'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 32: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding the use of tissue plasminogen activating factor, it is correct to state:\n",
      "a) This medication in the intracameral space is proscribed.\n",
      "b) Activation of plasmin into plaminogen helps in the degradation of blood.\n",
      "c) Activation of plasminogen to plasmin helps in the degradation of blood.\n",
      "d) Its main indication is in bleeding in the vitreous space.\n",
      "Test #0: \n",
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.61 ms /    25 runs   (    0.58 ms per token,  1711.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2512.99 ms /   107 tokens (   23.49 ms per token,    42.58 tokens per second)\n",
      "llama_print_timings:        eval time =  2915.81 ms /    24 runs   (  121.49 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  5503.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    58.95 ms /   101 runs   (    0.58 ms per token,  1713.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12391.15 ms /   101 runs   (  122.68 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 12702.03 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.97 ms /    25 runs   (    0.60 ms per token,  1669.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3054.23 ms /    25 runs   (  122.17 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3129.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.79 ms /    24 runs   (    0.57 ms per token,  1740.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2926.67 ms /    24 runs   (  121.94 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  2996.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.29 ms /    46 runs   (    0.57 ms per token,  1749.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5623.35 ms /    46 runs   (  122.25 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  5760.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.51 ms /    25 runs   (    0.58 ms per token,  1723.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3055.07 ms /    25 runs   (  122.20 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3128.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.37 ms /    25 runs   (    0.57 ms per token,  1739.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3054.06 ms /    25 runs   (  122.16 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3127.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.88 ms /    24 runs   (    0.58 ms per token,  1729.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2951.93 ms /    24 runs   (  123.00 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3022.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.50 ms /    25 runs   (    0.58 ms per token,  1724.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3041.58 ms /    25 runs   (  121.66 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3114.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.12 ms /    44 runs   (    0.57 ms per token,  1751.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5339.76 ms /    44 runs   (  121.36 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  5469.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Com relação ao uso de fator ativador do plasminogênio tecidual, é correto afirmar:\n",
      "a)Essa medicação no espaço intracameral é proscrita.\n",
      "b)A ativação da plasmina em plaminogênio ajuda na degradação do sangue.\n",
      "c)A ativação do plasminogênio em plasmina ajuda na degradação do sangue.\n",
      "d)Sua maior indicação é em sangramentos no espaço vítreo.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.23 ms /     7 runs   (    0.60 ms per token,  1654.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3205.29 ms /   135 tokens (   23.74 ms per token,    42.12 tokens per second)\n",
      "llama_print_timings:        eval time =   725.56 ms /     6 runs   (  120.93 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  3952.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n",
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.73 ms /    27 runs   (    0.58 ms per token,  1716.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3287.59 ms /    27 runs   (  121.76 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3367.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    36.30 ms /    62 runs   (    0.59 ms per token,  1707.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7666.27 ms /    62 runs   (  123.65 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  7855.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.78 ms /    24 runs   (    0.57 ms per token,  1741.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2930.63 ms /    24 runs   (  122.11 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3001.01 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.17 ms /    35 runs   (    0.58 ms per token,  1735.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4352.24 ms /    35 runs   (  124.35 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =  4455.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting respose to json:  ####\n",
      "\n",
      "Resposta: c) A ativação do plasminogênio em plasmina ajuda na degradação do sangue.\n",
      "Generating new response...\n",
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1733.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   877.50 ms /     7 runs   (  125.36 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:       total time =   897.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.35 ms /    25 runs   (    0.57 ms per token,  1742.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3033.87 ms /    25 runs   (  121.35 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3107.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n",
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.78 ms /    24 runs   (    0.57 ms per token,  1741.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2939.06 ms /    24 runs   (  122.46 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3009.60 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.15 ms /    28 runs   (    0.58 ms per token,  1733.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3426.76 ms /    28 runs   (  122.38 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3509.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.16 ms /    28 runs   (    0.58 ms per token,  1732.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3526.96 ms /    28 runs   (  125.96 ms per token,     7.94 tokens per second)\n",
      "llama_print_timings:       total time =  3609.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1731.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   885.46 ms /     7 runs   (  126.49 ms per token,     7.91 tokens per second)\n",
      "llama_print_timings:       total time =   906.12 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 33: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding the use of mitomycin-C in trabeculectomy, it is correct to state:\n",
      "a) Because it is a pyrimidine analogue, its action stems from the blockage of thymine synthesis, preventing the production of DNA/RNA.\n",
      "b) Scleral thinning, avascular bleb, postoperative pain and corneal edema are complications related to its use.\n",
      "c) In pregnant women, the concentration should be increased due to a greater tendency to fibrosis.\n",
      "d) It should not be used in melanodermic patients.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    57.72 ms /   100 runs   (    0.58 ms per token,  1732.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3172.82 ms /   143 tokens (   22.19 ms per token,    45.07 tokens per second)\n",
      "llama_print_timings:        eval time = 12159.51 ms /    99 runs   (  122.82 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 15634.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    75.73 ms /   131 runs   (    0.58 ms per token,  1729.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16184.64 ms /   131 runs   (  123.55 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 16585.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n",
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   129.34 ms /   224 runs   (    0.58 ms per token,  1731.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 27673.44 ms /   224 runs   (  123.54 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 28377.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    35.90 ms /    62 runs   (    0.58 ms per token,  1727.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7617.52 ms /    62 runs   (  122.86 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  7802.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    36.14 ms /    63 runs   (    0.57 ms per token,  1743.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7773.22 ms /    63 runs   (  123.38 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  7961.82 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.70 ms /    55 runs   (    0.58 ms per token,  1734.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6747.28 ms /    55 runs   (  122.68 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  6909.98 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     5.20 ms /     9 runs   (    0.58 ms per token,  1731.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1101.11 ms /     9 runs   (  122.35 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  1127.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n",
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    63.17 ms /   109 runs   (    0.58 ms per token,  1725.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13441.26 ms /   109 runs   (  123.31 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 13770.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    56.04 ms /    97 runs   (    0.58 ms per token,  1730.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11900.74 ms /    97 runs   (  122.69 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 12192.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    58.29 ms /   101 runs   (    0.58 ms per token,  1732.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12323.41 ms /   101 runs   (  122.01 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time = 12630.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Com relação ao uso de mitomicina-C na trabeculectomia, é correto afirmar:\n",
      "a)Por ser um análogo da pirimidina, sua ação decorre do bloqueio da síntese de timina, evitando a produção de DNA/RNA.\n",
      "b)Afinamento escleral, bolha avascular, dor pós-operatória e edema de córnea são complicações relacionadas ao seu uso.\n",
      "c)Nas gestantes, a concentração deve ser aumentada pela maior tendência à fibrose.\n",
      "d)Não deve ser usada em pacientes melanodérmicos.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   120.14 ms /   206 runs   (    0.58 ms per token,  1714.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3271.46 ms /   161 tokens (   20.32 ms per token,    49.21 tokens per second)\n",
      "llama_print_timings:        eval time = 25475.78 ms /   205 runs   (  124.27 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time = 29395.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    46.57 ms /    81 runs   (    0.57 ms per token,  1739.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9956.42 ms /    81 runs   (  122.92 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 10198.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    47.24 ms /    82 runs   (    0.58 ms per token,  1735.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10132.37 ms /    82 runs   (  123.57 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 10378.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    47.46 ms /    82 runs   (    0.58 ms per token,  1727.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10048.75 ms /    82 runs   (  122.55 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time = 10293.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.02 ms /    31 runs   (    0.58 ms per token,  1720.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3780.82 ms /    31 runs   (  121.96 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3874.49 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.02 ms /     7 runs   (    0.57 ms per token,  1740.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   857.42 ms /     7 runs   (  122.49 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =   878.38 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    57.86 ms /   100 runs   (    0.58 ms per token,  1728.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12353.92 ms /   100 runs   (  123.54 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 12659.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    59.46 ms /   103 runs   (    0.58 ms per token,  1732.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12720.52 ms /   103 runs   (  123.50 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 13037.02 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1734.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   847.35 ms /     7 runs   (  121.05 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =   868.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.64 ms /    17 runs   (    0.57 ms per token,  1764.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2091.38 ms /    17 runs   (  123.02 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  2144.88 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 34: \n",
      "Language: english\n",
      "Question: \n",
      "Patients with proliferative diabetic retinopathy will complement their treatment with intravitreal anti-VEGF. We can state that:\n",
      "a) Priority should be given to the inhibition of the B isoforms of VEGF, since they are the main new vessel formers.\n",
      "b)Ranibizumab inhibits all VEGF-B isoforms, but not VEGF-A.\n",
      "c) Bevacizumab does not allow VEGF-A isoforms to bind to their receptors.\n",
      "d) The use of pegaptanib is no longer used, as the drug does not penetrate the retinal layers.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   106.23 ms /   179 runs   (    0.59 ms per token,  1685.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2586.40 ms /   162 tokens (   15.97 ms per token,    62.64 tokens per second)\n",
      "llama_print_timings:        eval time = 21993.05 ms /   178 runs   (  123.56 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 25147.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.15 ms /    28 runs   (    0.58 ms per token,  1734.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3403.79 ms /    28 runs   (  121.56 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  3486.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   119.07 ms /   205 runs   (    0.58 ms per token,  1721.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 25261.60 ms /   205 runs   (  123.23 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 25903.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    98.13 ms /   170 runs   (    0.58 ms per token,  1732.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 21050.83 ms /   170 runs   (  123.83 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time = 21572.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    32.31 ms /    56 runs   (    0.58 ms per token,  1732.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6790.17 ms /    56 runs   (  121.25 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  6955.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    37.42 ms /    65 runs   (    0.58 ms per token,  1736.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7989.05 ms /    65 runs   (  122.91 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  8184.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.53 ms /    20 runs   (    0.58 ms per token,  1734.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2491.76 ms /    20 runs   (  124.59 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =  2552.86 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.06 ms /    17 runs   (    0.59 ms per token,  1689.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2207.55 ms /    17 runs   (  129.86 ms per token,     7.70 tokens per second)\n",
      "llama_print_timings:       total time =  2261.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.17 ms /    28 runs   (    0.58 ms per token,  1731.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3423.94 ms /    28 runs   (  122.28 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3507.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   118.69 ms /   205 runs   (    0.58 ms per token,  1727.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 25307.35 ms /   205 runs   (  123.45 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 25964.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Paciente com retinopatia diabética proliferativa complementará seu tratamento com anti-VEGF intravítreo. Podemos afirmar que:\n",
      "a)Deve-se priorizar a inibição das isoformas B do VEGF, uma vez que são as principais formadoras de neovasos.\n",
      "b)O ranibizumabe inibe todas as isoformas do VEGF-B, mas não as de VEGF-A.\n",
      "c)O bevacizumabe não permite que as isoformas de VEFG-A se liguem aos seus receptores.\n",
      "d)O uso de pegaptanibe não é mais utilizado, pois a droga não penetra nas camadas da retina.\n",
      "Test #0: \n",
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    51.02 ms /    86 runs   (    0.59 ms per token,  1685.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3719.99 ms /   186 tokens (   20.00 ms per token,    50.00 tokens per second)\n",
      "llama_print_timings:        eval time = 10417.25 ms /    85 runs   (  122.56 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time = 14411.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    68.93 ms /   119 runs   (    0.58 ms per token,  1726.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14707.55 ms /   119 runs   (  123.59 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 15076.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    77.40 ms /   133 runs   (    0.58 ms per token,  1718.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16435.44 ms /   133 runs   (  123.57 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 16841.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   104.22 ms /   181 runs   (    0.58 ms per token,  1736.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 22403.46 ms /   181 runs   (  123.78 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time = 22965.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1743.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   849.12 ms /     7 runs   (  121.30 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =   869.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.61 ms /    20 runs   (    0.58 ms per token,  1722.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2519.74 ms /    20 runs   (  125.99 ms per token,     7.94 tokens per second)\n",
      "llama_print_timings:       total time =  2578.85 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   105.73 ms /   181 runs   (    0.58 ms per token,  1711.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 22313.09 ms /   181 runs   (  123.28 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 22884.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   104.56 ms /   181 runs   (    0.58 ms per token,  1731.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 22414.40 ms /   181 runs   (  123.84 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time = 22979.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.00 ms /     7 runs   (    0.57 ms per token,  1747.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   841.37 ms /     7 runs   (  120.20 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:       total time =   862.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.22 ms /    28 runs   (    0.58 ms per token,  1726.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3497.59 ms /    28 runs   (  124.91 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time =  3580.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 35: \n",
      "Language: english\n",
      "Question: \n",
      "Considering conjunctival tumors, the presence of which parameter below is used to differentiate between conjunctival dysplasia and invasive carcinoma?\n",
      "a) Cellular atypia.\n",
      "b) Involvement of the epithelial full thickness.\n",
      "c)Increase in the number of atypical nucleoli.\n",
      "d) Invasion of the basement membrane.\n",
      " \n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.00 ms /    36 runs   (    0.58 ms per token,  1714.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2265.33 ms /    95 tokens (   23.85 ms per token,    41.94 tokens per second)\n",
      "llama_print_timings:        eval time =  4263.10 ms /    35 runs   (  121.80 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  6637.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.20 ms /    35 runs   (    0.58 ms per token,  1732.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4300.58 ms /    35 runs   (  122.87 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  4405.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Invasion of the basement membrane.'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    41.14 ms /    71 runs   (    0.58 ms per token,  1725.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8663.00 ms /    71 runs   (  122.01 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  8880.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    45.46 ms /    79 runs   (    0.58 ms per token,  1737.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9686.67 ms /    79 runs   (  122.62 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  9927.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    36.80 ms /    64 runs   (    0.57 ms per token,  1739.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7793.61 ms /    64 runs   (  121.78 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  7985.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    44.43 ms /    77 runs   (    0.58 ms per token,  1733.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9424.11 ms /    77 runs   (  122.39 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  9655.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #6: \n",
      "{'response': 'd'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.85 ms /    36 runs   (    0.61 ms per token,  1647.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4396.46 ms /    36 runs   (  122.12 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  4551.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.15 ms /    25 runs   (    0.65 ms per token,  1547.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3056.07 ms /    25 runs   (  122.24 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3228.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.67 ms /    36 runs   (    0.66 ms per token,  1521.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4486.33 ms /    36 runs   (  124.62 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time =  4733.82 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    85.55 ms /   143 runs   (    0.60 ms per token,  1671.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 17593.09 ms /   143 runs   (  123.03 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 18203.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Considerando os tumores conjuntivais, a presença de qual parâmetro abaixo é utilizado para a diferenciação entre displasia da conjuntiva e carcinoma invasivo?\n",
      "a)Atipia celular.\n",
      "b)Envolvimento da espessura total epitelial.\n",
      "c)Aumento do número de nucleolos atípicos.\n",
      "d)Invasão da membrana basal.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    53.35 ms /    91 runs   (    0.59 ms per token,  1705.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2552.97 ms /   104 tokens (   24.55 ms per token,    40.74 tokens per second)\n",
      "llama_print_timings:        eval time = 11074.84 ms /    90 runs   (  123.05 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 13908.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.67 ms /    41 runs   (    0.58 ms per token,  1732.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5003.86 ms /    41 runs   (  122.05 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  5124.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.58 ms /    27 runs   (    0.58 ms per token,  1732.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3326.14 ms /    27 runs   (  123.19 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  3405.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.58 ms /    27 runs   (    0.58 ms per token,  1732.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3311.26 ms /    27 runs   (  122.64 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  3391.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.87 ms /    50 runs   (    0.58 ms per token,  1731.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6221.58 ms /    50 runs   (  124.43 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =  6371.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1733.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   882.52 ms /     7 runs   (  126.07 ms per token,     7.93 tokens per second)\n",
      "llama_print_timings:       total time =   903.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.16 ms /    40 runs   (    0.58 ms per token,  1727.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4885.04 ms /    40 runs   (  122.13 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  5005.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n",
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.20 ms /    28 runs   (    0.58 ms per token,  1728.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3472.39 ms /    28 runs   (  124.01 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  3556.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    57.67 ms /    99 runs   (    0.58 ms per token,  1716.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12167.58 ms /    99 runs   (  122.90 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 12469.86 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.93 ms /    27 runs   (    0.59 ms per token,  1694.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3319.61 ms /    27 runs   (  122.95 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3400.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 36: \n",
      "Language: english\n",
      "Question: \n",
      "Fluorescein-labelled monoclonal antibodies (FITC) directed against the outer membrane cell wall protein are a better diagnostic method for which of the agents listed below?\n",
      "a) C. trachomatis.\n",
      "b) Acanthamoeba sp.\n",
      "c)Cryptococcus sp.\n",
      "d) Neisseria meningitidis.\n",
      "Test #0: \n",
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.06 ms /    24 runs   (    0.59 ms per token,  1706.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2270.78 ms /    93 tokens (   24.42 ms per token,    40.96 tokens per second)\n",
      "llama_print_timings:        eval time =  2794.46 ms /    23 runs   (  121.50 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  5137.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.81 ms /    24 runs   (    0.58 ms per token,  1737.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2954.90 ms /    24 runs   (  123.12 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  3026.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.01 ms /    24 runs   (    0.58 ms per token,  1712.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2971.03 ms /    24 runs   (  123.79 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  3042.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.92 ms /    24 runs   (    0.58 ms per token,  1724.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2984.90 ms /    24 runs   (  124.37 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =  3056.32 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n",
      "{'response': 'A'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.85 ms /    24 runs   (    0.58 ms per token,  1732.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2922.22 ms /    24 runs   (  121.76 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  2993.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.89 ms /    24 runs   (    0.58 ms per token,  1727.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2952.30 ms /    24 runs   (  123.01 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3024.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.64 ms /    27 runs   (    0.58 ms per token,  1726.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3329.75 ms /    27 runs   (  123.32 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  3410.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n",
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.73 ms /    24 runs   (    0.57 ms per token,  1747.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2940.57 ms /    24 runs   (  122.52 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3011.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.48 ms /    25 runs   (    0.58 ms per token,  1726.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3025.42 ms /    25 runs   (  121.02 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  3099.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.59 ms /    27 runs   (    0.58 ms per token,  1732.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3292.40 ms /    27 runs   (  121.94 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3372.93 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Anticorpos monoclonais marcados com fluoresceína (FITC) e dirigidos contra a membrana externa proteica da parede celular são um método diagnóstico mais bem indicado para qual dos agentes listados abaixo?\n",
      "a)C. tracomatis.\n",
      "b)Acanthamoeba sp.\n",
      "c)Cryptococcus sp.\n",
      "d)Neisseria meningitidis.\n",
      "Test #0: \n",
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.28 ms /    28 runs   (    0.58 ms per token,  1720.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2231.53 ms /   109 tokens (   20.47 ms per token,    48.85 tokens per second)\n",
      "llama_print_timings:        eval time =  3294.90 ms /    27 runs   (  122.03 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  5610.24 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.87 ms /    27 runs   (    0.59 ms per token,  1701.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3330.14 ms /    27 runs   (  123.34 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  3412.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.58 ms /    27 runs   (    0.58 ms per token,  1732.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3286.73 ms /    27 runs   (  121.73 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3367.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.45 ms /    25 runs   (    0.58 ms per token,  1730.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3058.92 ms /    25 runs   (  122.36 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3132.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n",
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.53 ms /    27 runs   (    0.58 ms per token,  1738.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3282.49 ms /    27 runs   (  121.57 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  3362.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.80 ms /    24 runs   (    0.57 ms per token,  1739.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2910.43 ms /    24 runs   (  121.27 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  2983.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.51 ms /    27 runs   (    0.57 ms per token,  1740.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3312.14 ms /    27 runs   (  122.67 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  3392.01 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.17 ms /    28 runs   (    0.58 ms per token,  1731.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3409.37 ms /    28 runs   (  121.76 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3493.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.07 ms /    28 runs   (    0.57 ms per token,  1741.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3473.51 ms /    28 runs   (  124.05 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  3557.49 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.65 ms /    36 runs   (    0.57 ms per token,  1743.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4378.60 ms /    36 runs   (  121.63 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  4486.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 37: \n",
      "Language: english\n",
      "Question: \n",
      "The innervation of the cornea is mainly done by which ciliary nerves?\n",
      "a) Short anterior.\n",
      "b) Short posterior.\n",
      "c) Long anterior.\n",
      "d) Long posterior.\n",
      "\n",
      "Test #0: \n",
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.29 ms /    39 runs   (    0.57 ms per token,  1749.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =   762.98 ms /    50 tokens (   15.26 ms per token,    65.53 tokens per second)\n",
      "llama_print_timings:        eval time =  4702.99 ms /    38 runs   (  123.76 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  5584.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.00 ms /    28 runs   (    0.57 ms per token,  1750.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3397.02 ms /    28 runs   (  121.32 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3480.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.85 ms /    28 runs   (    0.57 ms per token,  1766.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3386.84 ms /    28 runs   (  120.96 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  3469.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.75 ms /    31 runs   (    0.57 ms per token,  1746.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3739.85 ms /    31 runs   (  120.64 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =  3832.21 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.90 ms /    21 runs   (    0.57 ms per token,  1764.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2563.43 ms /    21 runs   (  122.07 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  2624.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.85 ms /    12 runs   (    0.57 ms per token,  1751.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1454.11 ms /    12 runs   (  121.18 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  1488.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b) Short posterior'}\n",
      "Test #6: \n",
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.03 ms /    28 runs   (    0.57 ms per token,  1746.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3436.17 ms /    28 runs   (  122.72 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  3520.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.41 ms /    27 runs   (    0.57 ms per token,  1752.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3308.63 ms /    27 runs   (  122.54 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3389.51 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.84 ms /    28 runs   (    0.57 ms per token,  1768.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3380.22 ms /    28 runs   (  120.72 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  3463.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c} Long anterior.'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.24 ms /    11 runs   (    0.57 ms per token,  1762.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1399.09 ms /    11 runs   (  127.19 ms per token,     7.86 tokens per second)\n",
      "llama_print_timings:       total time =  1431.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "A inervação da córnea é feita, principalmente, por quais nervos ciliares?\n",
      "a)Anteriores curtos.\n",
      "b)Posteriores curtos.\n",
      "c)Anteriores longos.\n",
      "d)Posteriores longos.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.13 ms /    28 runs   (    0.58 ms per token,  1736.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1709.00 ms /    69 tokens (   24.77 ms per token,    40.37 tokens per second)\n",
      "llama_print_timings:        eval time =  3293.05 ms /    27 runs   (  121.96 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  5086.54 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.71 ms /    28 runs   (    0.56 ms per token,  1782.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3418.98 ms /    28 runs   (  122.11 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3502.49 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n",
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.29 ms /    24 runs   (    0.55 ms per token,  1806.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2905.13 ms /    24 runs   (  121.05 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  2976.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.90 ms /    25 runs   (    0.56 ms per token,  1798.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3033.26 ms /    25 runs   (  121.33 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3107.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.14 ms /    27 runs   (    0.56 ms per token,  1783.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3275.28 ms /    27 runs   (  121.31 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3354.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.70 ms /    28 runs   (    0.56 ms per token,  1783.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3432.64 ms /    28 runs   (  122.59 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3515.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.77 ms /    28 runs   (    0.56 ms per token,  1775.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3409.12 ms /    28 runs   (  121.75 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3493.00 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n",
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.43 ms /    24 runs   (    0.56 ms per token,  1786.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2905.73 ms /    24 runs   (  121.07 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  2976.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.96 ms /     7 runs   (    0.57 ms per token,  1766.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   844.37 ms /     7 runs   (  120.62 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =   864.58 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.08 ms /    28 runs   (    0.57 ms per token,  1741.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3411.12 ms /    28 runs   (  121.83 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3494.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 38: \n",
      "Language: english\n",
      "Question: \n",
      "Which of the following extrinsic ocular muscles is innervated by the superior division of the oculomotor nerve?\n",
      "a) Superior oblique.\n",
      "b) Inferior rectum.\n",
      "c) medial rectus\n",
      "d) Superior rectum.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.64 ms /    25 runs   (    0.59 ms per token,  1707.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1639.82 ms /    66 tokens (   24.85 ms per token,    40.25 tokens per second)\n",
      "llama_print_timings:        eval time =  2883.88 ms /    24 runs   (  120.16 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:       total time =  4598.58 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.18 ms /    28 runs   (    0.58 ms per token,  1730.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3391.32 ms /    28 runs   (  121.12 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  3474.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.65 ms /    29 runs   (    0.57 ms per token,  1741.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3508.76 ms /    29 runs   (  120.99 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  3595.48 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.79 ms /    24 runs   (    0.57 ms per token,  1740.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2910.49 ms /    24 runs   (  121.27 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  2981.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.54 ms /    27 runs   (    0.58 ms per token,  1737.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3264.57 ms /    27 runs   (  120.91 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  3344.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.87 ms /    33 runs   (    0.57 ms per token,  1749.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4030.33 ms /    33 runs   (  122.13 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  4129.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.22 ms /    30 runs   (    0.57 ms per token,  1742.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3705.87 ms /    30 runs   (  123.53 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  3795.32 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.75 ms /    24 runs   (    0.57 ms per token,  1745.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2996.76 ms /    24 runs   (  124.87 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time =  3068.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    34.14 ms /    60 runs   (    0.57 ms per token,  1757.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7300.32 ms /    60 runs   (  121.67 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  7480.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    34.42 ms /    60 runs   (    0.57 ms per token,  1743.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7308.32 ms /    60 runs   (  121.81 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  7489.48 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Qual dos músculos oculares extrínsecos abaixo é inervado pela divisão superior do nervo oculomotor?\n",
      "a)Oblíquo superior.\n",
      "b)Reto inferior.\n",
      "c)Reto medial\n",
      "d)Reto superior.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.23 ms /    43 runs   (    0.59 ms per token,  1703.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1698.07 ms /    71 tokens (   23.92 ms per token,    41.81 tokens per second)\n",
      "llama_print_timings:        eval time =  5095.03 ms /    42 runs   (  121.31 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  6921.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    24.41 ms /    43 runs   (    0.57 ms per token,  1761.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5236.72 ms /    43 runs   (  121.78 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  5362.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1735.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   866.38 ms /     7 runs   (  123.77 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =   887.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.05 ms /    28 runs   (    0.57 ms per token,  1744.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3380.67 ms /    28 runs   (  120.74 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  3463.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n",
      "{'response': 'A'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.07 ms /     7 runs   (    0.58 ms per token,  1720.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   844.45 ms /     7 runs   (  120.64 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =   865.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.99 ms /     7 runs   (    0.57 ms per token,  1754.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   865.51 ms /     7 runs   (  123.64 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =   885.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n",
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    24.08 ms /    42 runs   (    0.57 ms per token,  1744.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5095.21 ms /    42 runs   (  121.31 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  5219.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1735.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   870.96 ms /     7 runs   (  124.42 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =   891.12 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    24.11 ms /    42 runs   (    0.57 ms per token,  1741.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5100.80 ms /    42 runs   (  121.45 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  5226.01 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n",
      "{'response': 'A'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 39: \n",
      "Language: english\n",
      "Question: \n",
      "Based on anatomical knowledge, choose the most likely alternative.\n",
      "a) Fracture of the ethmoid bone can cause orbital emphysema.\n",
      "b) Fracture of the lateral part of the orbit can cause intraorbital hemorrhage, due to rupture of the anterior and posterior ethmoidal arteries.\n",
      "c) Fracture of the orbital floor can cause impairment of masticatory movements, due to damage to the maxillary nerve, a branch of the trigeminal nerve.\n",
      "d) Fracture of the lacrimal bone may decrease tear excretion by the main lacrimal gland, by compromising the lacrimal nerve.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.01 ms /    42 runs   (    0.64 ms per token,  1554.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5114.43 ms /    42 runs   (  121.77 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  5246.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.00 ms /    24 runs   (    0.58 ms per token,  1714.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3612.94 ms /   165 tokens (   21.90 ms per token,    45.67 tokens per second)\n",
      "llama_print_timings:        eval time =  2899.34 ms /    23 runs   (  126.06 ms per token,     7.93 tokens per second)\n",
      "llama_print_timings:       total time =  6584.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.51 ms /    24 runs   (    0.69 ms per token,  1453.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2984.98 ms /    24 runs   (  124.37 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =  3062.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #2: \n",
      "{'response': 'B'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.41 ms /    25 runs   (    0.66 ms per token,  1523.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3102.28 ms /    25 runs   (  124.09 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  3181.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.91 ms /    28 runs   (    0.60 ms per token,  1655.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3475.59 ms /    28 runs   (  124.13 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  3560.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.37 ms /    25 runs   (    0.57 ms per token,  1740.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3077.14 ms /    25 runs   (  123.09 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  3150.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.20 ms /    28 runs   (    0.58 ms per token,  1728.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3443.44 ms /    28 runs   (  122.98 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3527.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.74 ms /    24 runs   (    0.57 ms per token,  1746.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2952.87 ms /    24 runs   (  123.04 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3024.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.02 ms /     7 runs   (    0.57 ms per token,  1740.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   855.27 ms /     7 runs   (  122.18 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =   875.49 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.49 ms /    25 runs   (    0.58 ms per token,  1725.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3088.47 ms /    25 runs   (  123.54 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  3162.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.15 ms /    28 runs   (    0.58 ms per token,  1734.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3428.34 ms /    28 runs   (  122.44 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3510.67 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Com base em conhecimentos anatômicos, escolha a alternativa mais provável.\n",
      "a)Fratura do osso etmoidal pode causar enfisema orbitário.\n",
      "b)Fratura da parte lateral da órbita pode causar hemorragia intraorbital, por ruptura das artérias etmoidais anteriores e posteriores.\n",
      "c)Fratura do assoalho da órbita pode causar comprometimento dos movimentos mastigatórios, por lesão no nervo maxilar, ramo do nervo trigêmeo.\n",
      "d)Fratura do osso lacrimal pode diminuir a excreção de lágrima pela glândula lacrimal principal, por comprometimento do nervo lacrimal.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.31 ms /    28 runs   (    0.58 ms per token,  1716.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3234.90 ms /   183 tokens (   17.68 ms per token,    56.57 tokens per second)\n",
      "llama_print_timings:        eval time =  3412.01 ms /    27 runs   (  126.37 ms per token,     7.91 tokens per second)\n",
      "llama_print_timings:       total time =  6730.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n",
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    77.41 ms /   134 runs   (    0.58 ms per token,  1731.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16400.88 ms /   134 runs   (  122.39 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time = 16808.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.51 ms /    25 runs   (    0.58 ms per token,  1723.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3125.24 ms /    25 runs   (  125.01 ms per token,     8.00 tokens per second)\n",
      "llama_print_timings:       total time =  3199.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    37.41 ms /    65 runs   (    0.58 ms per token,  1737.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8101.13 ms /    65 runs   (  124.63 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time =  8296.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    77.48 ms /   134 runs   (    0.58 ms per token,  1729.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16435.12 ms /   134 runs   (  122.65 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 16843.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.02 ms /     7 runs   (    0.57 ms per token,  1742.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   862.64 ms /     7 runs   (  123.23 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =   883.48 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.74 ms /    17 runs   (    0.57 ms per token,  1745.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2087.73 ms /    17 runs   (  122.81 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  2137.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.49 ms /    27 runs   (    0.57 ms per token,  1743.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3356.97 ms /    27 runs   (  124.33 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =  3436.75 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    43.18 ms /    75 runs   (    0.58 ms per token,  1737.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9236.45 ms /    75 runs   (  123.15 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  9461.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.83 ms /    24 runs   (    0.58 ms per token,  1735.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2934.86 ms /    24 runs   (  122.29 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3005.60 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 40: \n",
      "Language: english\n",
      "Question: \n",
      "There are many factors that contribute to the transparency of the cornea, but the main one is:\n",
      "a) Absence of adhesion proteoglycans, increasing the loose interfibrillar space and allowing free passage of light.\n",
      "b) Low metabolic demand, which allows nutrition by a reticular mesh of extremely fine stromal capillaries.\n",
      "c) Hyperosmotic characteristic of Descemet's membrane, which reduces stromal hydration.\n",
      "d) Organization of collagen fibrils in a regular, uniform and parallel way to each other.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.17 ms /    19 runs   (    0.59 ms per token,  1700.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2887.99 ms /   137 tokens (   21.08 ms per token,    47.44 tokens per second)\n",
      "llama_print_timings:        eval time =  2219.12 ms /    18 runs   (  123.28 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  5164.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n",
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.56 ms /    25 runs   (    0.58 ms per token,  1716.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3067.06 ms /    25 runs   (  122.68 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  3141.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.14 ms /    28 runs   (    0.58 ms per token,  1734.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3469.85 ms /    28 runs   (  123.92 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  3552.75 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n",
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.80 ms /    17 runs   (    0.58 ms per token,  1735.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2088.86 ms /    17 runs   (  122.87 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  2138.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.02 ms /    24 runs   (    0.58 ms per token,  1712.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2916.01 ms /    24 runs   (  121.50 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  2987.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.54 ms /    20 runs   (    0.58 ms per token,  1733.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2449.04 ms /    20 runs   (  122.45 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  2507.95 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.39 ms /    25 runs   (    0.58 ms per token,  1737.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3055.10 ms /    25 runs   (  122.20 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3129.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.55 ms /    20 runs   (    0.58 ms per token,  1731.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2448.86 ms /    20 runs   (  122.44 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  2508.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.78 ms /    29 runs   (    0.58 ms per token,  1728.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3516.47 ms /    29 runs   (  121.26 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3602.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.15 ms /    21 runs   (    0.58 ms per token,  1728.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2575.55 ms /    21 runs   (  122.65 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  2637.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Muitos são os fatores que contribuem para a transparência da córnea, mas o principal deles é:\n",
      "a)Ausência de proteoglicanos de adesão, aumentando o espaço interfibrilar frouxo e permitindo a passagem livre da luz.\n",
      "b)Baixa demanda metabólica, que permite a nutrição por uma malha reticular de capilares estromais extremamente finos.\n",
      "c)Característica hiperosmótica da membrana de Descemet, que reduz a hidratação estromal.\n",
      "d)Organização das fibrilas de colágeno de forma regular, uniforme e paralela umas às outras.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.13 ms /     7 runs   (    0.59 ms per token,  1694.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3339.48 ms /   173 tokens (   19.30 ms per token,    51.80 tokens per second)\n",
      "llama_print_timings:        eval time =   728.51 ms /     6 runs   (  121.42 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  4088.88 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.72 ms /    20 runs   (    0.59 ms per token,  1706.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2495.45 ms /    20 runs   (  124.77 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time =  2555.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.05 ms /    27 runs   (    0.59 ms per token,  1682.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3317.72 ms /    27 runs   (  122.88 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  3400.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.11 ms /    28 runs   (    0.58 ms per token,  1738.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3444.36 ms /    28 runs   (  123.01 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3527.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.52 ms /    27 runs   (    0.57 ms per token,  1739.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3335.28 ms /    27 runs   (  123.53 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  3415.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.04 ms /    28 runs   (    0.57 ms per token,  1745.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3395.29 ms /    28 runs   (  121.26 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3478.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1731.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   873.42 ms /     7 runs   (  124.77 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time =   893.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n",
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.50 ms /    20 runs   (    0.57 ms per token,  1739.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2501.96 ms /    20 runs   (  125.10 ms per token,     7.99 tokens per second)\n",
      "llama_print_timings:       total time =  2561.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.00 ms /     7 runs   (    0.57 ms per token,  1749.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   853.72 ms /     7 runs   (  121.96 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =   874.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.09 ms /    28 runs   (    0.57 ms per token,  1740.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3458.85 ms /    28 runs   (  123.53 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  3543.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 41: \n",
      "Language: english\n",
      "Question: \n",
      "Mark the correct alternative regarding the anatomy of the posterior segment of the eye.\n",
      "a) The emergence of the two long ciliary nerves at the equator of the eye, approximately at twelve and six o'clock, explains greater sensitivity to pain in these quadrants during panphotocoagulation.\n",
      "b) The subretinal vorticose arteries give the lacy \"tiger\" appearance in the ophthalmoscopic examination of people with diffuse atrophy of the retinal pigment epithelium.\n",
      "c) In the ora serrata, there are small radial meridional folds next to the dentate processes, with, eventually, a small atrophic retinal hole at its base, which does not need to be blocked.\n",
      "d) The foveola, area of ​​greater metabolic activity due to the high concentration of photoreceptors, presents an intense vascular network formed by anastomoses of the superior and inferior temporal arcades.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    45.50 ms /    78 runs   (    0.58 ms per token,  1714.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3700.39 ms /   230 tokens (   16.09 ms per token,    62.16 tokens per second)\n",
      "llama_print_timings:        eval time =  9507.78 ms /    77 runs   (  123.48 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 13447.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    43.86 ms /    76 runs   (    0.58 ms per token,  1732.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9359.43 ms /    76 runs   (  123.15 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  9592.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    45.47 ms /    79 runs   (    0.58 ms per token,  1737.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9745.00 ms /    79 runs   (  123.35 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  9984.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    48.33 ms /    84 runs   (    0.58 ms per token,  1738.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10367.48 ms /    84 runs   (  123.42 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 10621.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.11 ms /     7 runs   (    0.59 ms per token,  1702.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   872.09 ms /     7 runs   (  124.58 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =   893.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    44.95 ms /    77 runs   (    0.58 ms per token,  1713.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9495.13 ms /    77 runs   (  123.31 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  9728.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    53.59 ms /    93 runs   (    0.58 ms per token,  1735.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11472.06 ms /    93 runs   (  123.36 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 11751.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    43.79 ms /    76 runs   (    0.58 ms per token,  1735.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9386.38 ms /    76 runs   (  123.50 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  9615.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    51.67 ms /    90 runs   (    0.57 ms per token,  1741.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11088.00 ms /    90 runs   (  123.20 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 11361.07 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    47.59 ms /    83 runs   (    0.57 ms per token,  1744.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10221.70 ms /    83 runs   (  123.15 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 10473.26 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Marque a alternativa correta quanto à anatomia do segmento posterior do olho.\n",
      "a)A emergência dos dois nervos ciliares longos no equador do olho, aproximadamente às doze e seis horas, explica maior sensibilidade à dor nesses quadrantes durante a panfotocoagulação.\n",
      "b)As artérias vorticosas sub-retinianas conferem o aspecto rendilhado \"em tigre\" no exame oftalmoscópico de pessoas com atrofia difusa do epitélio pigmentado da retina.\n",
      "c)Na ora serrata encontram-se pequenas pregas meridionais radiais junto aos processos denteados com, eventualmente pequeno buraco retiniano atrófico na sua base, que não necessita ser bloqueado.\n",
      "d)A fovéola, área de maior atividade metabólica devido a alta concentração de fotorreceptores, apresenta intensa rede vascular formada por anastomoses das arcadas temporais superiores e inferiores.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.97 ms /    20 runs   (    0.60 ms per token,  1670.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4591.80 ms /   263 tokens (   17.46 ms per token,    57.28 tokens per second)\n",
      "llama_print_timings:        eval time =  2340.18 ms /    19 runs   (  123.17 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  6992.98 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.08 ms /     7 runs   (    0.58 ms per token,  1715.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   854.92 ms /     7 runs   (  122.13 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =   876.07 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    58.69 ms /    97 runs   (    0.60 ms per token,  1652.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12022.96 ms /    97 runs   (  123.95 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time = 12322.99 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    63.17 ms /   104 runs   (    0.61 ms per token,  1646.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12792.16 ms /   104 runs   (  123.00 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 13116.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.05 ms /     7 runs   (    0.58 ms per token,  1728.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   887.23 ms /     7 runs   (  126.75 ms per token,     7.89 tokens per second)\n",
      "llama_print_timings:       total time =   909.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.21 ms /    20 runs   (    0.66 ms per token,  1514.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2505.18 ms /    20 runs   (  125.26 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:       total time =  2567.03 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.14 ms /    28 runs   (    0.58 ms per token,  1735.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3527.54 ms /    28 runs   (  125.98 ms per token,     7.94 tokens per second)\n",
      "llama_print_timings:       total time =  3612.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.06 ms /     7 runs   (    0.58 ms per token,  1724.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   863.83 ms /     7 runs   (  123.40 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =   885.24 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    37.21 ms /    64 runs   (    0.58 ms per token,  1720.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7998.94 ms /    64 runs   (  124.98 ms per token,     8.00 tokens per second)\n",
      "llama_print_timings:       total time =  8193.21 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    52.44 ms /    91 runs   (    0.58 ms per token,  1735.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11201.50 ms /    91 runs   (  123.09 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 11476.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 42: \n",
      "Language: english\n",
      "Question: \n",
      "Mark the correct alternative regarding the anatomy of the camerular sinus.\n",
      "a) Schwalbe's line corresponds to the gonioscopic projection of the scleral spur.\n",
      "b) The ciliary body band usually cannot be seen by gonioscopy.\n",
      "c) pectineal iris processes reaching Schwalbe's line are not present in normal eyes; and when found, they are indicative of previous iridocyclitis.\n",
      "d) The drainage flow of aqueous humor occurs mainly in the posterior portion of the trabecular meshwork, which is more pigmented.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    97.56 ms /   167 runs   (    0.58 ms per token,  1711.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3193.28 ms /   143 tokens (   22.33 ms per token,    44.78 tokens per second)\n",
      "llama_print_timings:        eval time = 20528.94 ms /   166 runs   (  123.67 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 24241.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    89.01 ms /   154 runs   (    0.58 ms per token,  1730.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 18969.33 ms /   154 runs   (  123.18 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 19444.00 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   128.07 ms /   221 runs   (    0.58 ms per token,  1725.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 27272.72 ms /   221 runs   (  123.41 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 27967.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    38.86 ms /    67 runs   (    0.58 ms per token,  1724.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8284.37 ms /    67 runs   (  123.65 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  8486.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    42.25 ms /    73 runs   (    0.58 ms per token,  1727.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8989.52 ms /    73 runs   (  123.14 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  9216.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.04 ms /    24 runs   (    0.58 ms per token,  1709.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2935.05 ms /    24 runs   (  122.29 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3009.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   104.12 ms /   177 runs   (    0.59 ms per token,  1700.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 21779.16 ms /   177 runs   (  123.05 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 22343.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    38.59 ms /    67 runs   (    0.58 ms per token,  1736.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8194.16 ms /    67 runs   (  122.30 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  8392.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n",
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    38.75 ms /    67 runs   (    0.58 ms per token,  1729.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8256.02 ms /    67 runs   (  123.22 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  8454.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   115.15 ms /   199 runs   (    0.58 ms per token,  1728.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 24528.27 ms /   199 runs   (  123.26 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 25151.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Assinale a alternativa correta quanto à anatomia do seio camerular.\n",
      "a)A linha de Schwalbe corresponde à projeção gonioscópica do esporão escleral.\n",
      "b)A banda do corpo ciliar geralmente não pode ser observada pela gonioscopia.\n",
      "c)Processos irianos pectíneos que alcançam a linha de Schwalbe não estão presentes em olhos normais; e quando encontrados são indicativos de iridociclite prévia.\n",
      "d)O fluxo de drenagem do humor aquoso ocorre principalmente na porção posterior da malha trabecular, que é mais pigmentada.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.50 ms /    17 runs   (    0.62 ms per token,  1619.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2944.86 ms /   172 tokens (   17.12 ms per token,    58.41 tokens per second)\n",
      "llama_print_timings:        eval time =  1976.75 ms /    16 runs   (  123.55 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  4973.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #1: \n",
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    40.59 ms /    69 runs   (    0.59 ms per token,  1700.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8527.50 ms /    69 runs   (  123.59 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  8737.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.09 ms /     7 runs   (    0.58 ms per token,  1711.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   851.85 ms /     7 runs   (  121.69 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =   872.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.07 ms /     7 runs   (    0.58 ms per token,  1719.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   856.52 ms /     7 runs   (  122.36 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =   877.03 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.59 ms /    27 runs   (    0.58 ms per token,  1732.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3294.31 ms /    27 runs   (  122.01 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3374.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1746.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   846.77 ms /     7 runs   (  120.97 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =   866.98 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.52 ms /    20 runs   (    0.58 ms per token,  1735.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2446.98 ms /    20 runs   (  122.35 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  2506.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.13 ms /     7 runs   (    0.59 ms per token,  1695.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   853.31 ms /     7 runs   (  121.90 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =   873.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    52.87 ms /    90 runs   (    0.59 ms per token,  1702.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11135.12 ms /    90 runs   (  123.72 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time = 11407.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    52.65 ms /    91 runs   (    0.58 ms per token,  1728.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11189.17 ms /    91 runs   (  122.96 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 11464.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 43: \n",
      "Language: english\n",
      "Question: \n",
      "The sentences below refer to which structure of the human eye?\n",
      "\n",
      "1- Most anterior extension of the uveal tract.\n",
      "2- Formed by blood vessels, connective tissue and malanocytes.\n",
      "3- Presents pigmented epithelium whose basal end of the cells is facing the posterior chamber.\n",
      "4- It has smooth muscle fibers of sympathetic innervation.\n",
      "\n",
      "a) ciliary body.\n",
      "b) iris.\n",
      "c) Camerular sinus.\n",
      "d) Choroidal tract of ora serrata.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.74 ms /    34 runs   (    0.58 ms per token,  1722.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2448.32 ms /   130 tokens (   18.83 ms per token,    53.10 tokens per second)\n",
      "llama_print_timings:        eval time =  4038.33 ms /    33 runs   (  122.37 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  6589.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     7.44 ms /    13 runs   (    0.57 ms per token,  1748.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1602.59 ms /    13 runs   (  123.28 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  1642.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.12 ms /    30 runs   (    0.57 ms per token,  1752.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3705.25 ms /    30 runs   (  123.51 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  3793.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.22 ms /    32 runs   (    0.57 ms per token,  1756.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3943.29 ms /    32 runs   (  123.23 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  4037.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #4: \n",
      "{'response': 'a'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.19 ms /    32 runs   (    0.57 ms per token,  1759.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3873.03 ms /    32 runs   (  121.03 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  3967.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.41 ms /    48 runs   (    0.57 ms per token,  1751.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5905.31 ms /    48 runs   (  123.03 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  6048.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.43 ms /    25 runs   (    0.58 ms per token,  1732.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3049.65 ms /    25 runs   (  121.99 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3124.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.80 ms /    33 runs   (    0.57 ms per token,  1755.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4116.59 ms /    33 runs   (  124.75 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time =  4214.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.40 ms /    29 runs   (    0.57 ms per token,  1768.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3566.72 ms /    29 runs   (  122.99 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3652.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.57 ms /    31 runs   (    0.57 ms per token,  1764.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3765.96 ms /    31 runs   (  121.48 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  3857.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "As sentenças abaixo dizem respeito a qual estrutura do olho humano?\n",
      "\n",
      "1- Extensão mais anterior do trato uveal.\n",
      "2- Formada por vasos sanguíneos, tecido conjuntivo e malanócitos.\n",
      "3- Apresenta epitélio pigmentado cuja extremidade basal das células está voltada para a câmara posterior.\n",
      "4- Possui fibras musculares lisas de inervação simpática.\n",
      "\n",
      "a)Corpo ciliar.\n",
      "b)Íris.\n",
      "c)Seio camerular.\n",
      "d)Trato coroidal da ora serrata.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     7.67 ms /    13 runs   (    0.59 ms per token,  1695.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3754.38 ms /   165 tokens (   22.75 ms per token,    43.95 tokens per second)\n",
      "llama_print_timings:        eval time =  1467.54 ms /    12 runs   (  122.30 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  5260.49 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     8.45 ms /    14 runs   (    0.60 ms per token,  1656.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1735.64 ms /    14 runs   (  123.97 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  1778.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     8.28 ms /    14 runs   (    0.59 ms per token,  1691.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1748.95 ms /    14 runs   (  124.92 ms per token,     8.00 tokens per second)\n",
      "llama_print_timings:       total time =  1791.48 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.41 ms /    49 runs   (    0.58 ms per token,  1724.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6056.88 ms /    49 runs   (  123.61 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  6204.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     8.05 ms /    14 runs   (    0.57 ms per token,  1739.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1693.84 ms /    14 runs   (  120.99 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  1735.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     8.11 ms /    14 runs   (    0.58 ms per token,  1726.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1720.68 ms /    14 runs   (  122.91 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  1761.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     8.02 ms /    14 runs   (    0.57 ms per token,  1744.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1722.55 ms /    14 runs   (  123.04 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  1763.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     8.07 ms /    14 runs   (    0.58 ms per token,  1734.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1702.23 ms /    14 runs   (  121.59 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  1743.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     8.09 ms /    14 runs   (    0.58 ms per token,  1731.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1708.22 ms /    14 runs   (  122.02 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  1748.92 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     8.11 ms /    14 runs   (    0.58 ms per token,  1726.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1730.93 ms /    14 runs   (  123.64 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  1773.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 44: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding the anatomy of the retina, choose the alternative that contains the correct correlation between the two columns below:\n",
      "\n",
      "I - outer plexiform layer\n",
      "II - External limit\n",
      "III - inner nuclear layer\n",
      "IV - Internal limit\n",
      "V - Nerve fiber layer\n",
      "\n",
      "A- It is in contact with the vitreous\n",
      "B- It is between the photoreceptors and the bipolar cells\n",
      "C- Contains amacrine and horizontal cells\n",
      "D - Muller cell processes\n",
      "E- Contains axons of ganglion cells\n",
      "\n",
      "a) A: II; B: III; C: I; D: V; I: IV.\n",
      "b) A: IV; B: I; C: III; D: II; E: V.\n",
      "c) A: IV; B: V; C: III; D: I; I: II.\n",
      "d) A: V; B:II; C: III; D: IV; E:I.\n",
      "Test #0: \n",
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    84.97 ms /   148 runs   (    0.57 ms per token,  1741.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3862.27 ms /   219 tokens (   17.64 ms per token,    56.70 tokens per second)\n",
      "llama_print_timings:        eval time = 18133.86 ms /   147 runs   (  123.36 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 22453.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    84.70 ms /   148 runs   (    0.57 ms per token,  1747.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 18174.22 ms /   148 runs   (  122.80 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 18629.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.01 ms /    41 runs   (    0.56 ms per token,  1781.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5139.14 ms /    41 runs   (  125.34 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:       total time =  5260.98 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A: II; B: III; C: I; D: V'}\n",
      "Test #3: \n",
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    84.38 ms /   148 runs   (    0.57 ms per token,  1754.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 18141.16 ms /   148 runs   (  122.58 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time = 18593.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.08 ms /    48 runs   (    0.56 ms per token,  1772.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5956.69 ms /    48 runs   (  124.10 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  6100.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n",
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    52.71 ms /    93 runs   (    0.57 ms per token,  1764.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11521.12 ms /    93 runs   (  123.88 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time = 11803.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    84.49 ms /   148 runs   (    0.57 ms per token,  1751.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 18175.09 ms /   148 runs   (  122.80 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 18631.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.22 ms /    48 runs   (    0.59 ms per token,  1700.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5983.69 ms /    48 runs   (  124.66 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time =  6131.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    74.95 ms /   132 runs   (    0.57 ms per token,  1761.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16276.40 ms /   132 runs   (  123.31 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 16683.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n",
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "A respeito da anatomia da retina, escolha a alternativa que contenha a correlação correta entre as duas colunas abaixo:\n",
      "\n",
      "I- Camada plexiforme externa\n",
      "II- Limitante externa\n",
      "III- Camada nuclear interna\n",
      "IV- Limitante interna\n",
      "V- Camada de fibras nervosas\n",
      "\n",
      "A- Está em contato com o vítreo\n",
      "B- Está entre os fotorreceptores e as células bipolares\n",
      "C- Contém células amácrinas e horizontais\n",
      "D- São processos das células de Muller\n",
      "E- Contém axônios de células ganglionares\n",
      "\n",
      "a)A: II; B: III; C: I; D: V; E: IV.\n",
      "b)A: IV; B: I; C: III; D: II; E: V.\n",
      "c)A: IV; B: V; C: III; D: I; E: II.\n",
      "d)A: V; B: II; C: III; D: IV; E: I.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.70 ms /    39 runs   (    0.56 ms per token,  1797.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4792.61 ms /    39 runs   (  122.89 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  4909.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.75 ms /    49 runs   (    0.57 ms per token,  1765.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4158.56 ms /   260 tokens (   15.99 ms per token,    62.52 tokens per second)\n",
      "llama_print_timings:        eval time =  6017.80 ms /    48 runs   (  125.37 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:       total time = 10325.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.19 ms /    49 runs   (    0.58 ms per token,  1738.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6051.39 ms /    49 runs   (  123.50 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  6199.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.63 ms /    28 runs   (    0.52 ms per token,  1914.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3449.77 ms /    28 runs   (  123.21 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  3532.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    60.86 ms /   107 runs   (    0.57 ms per token,  1758.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13201.89 ms /   107 runs   (  123.38 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 13528.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.77 ms /     7 runs   (    0.54 ms per token,  1855.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   913.46 ms /     7 runs   (  130.49 ms per token,     7.66 tokens per second)\n",
      "llama_print_timings:       total time =   933.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    60.47 ms /   107 runs   (    0.57 ms per token,  1769.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13338.57 ms /   107 runs   (  124.66 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time = 13664.82 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    60.88 ms /   107 runs   (    0.57 ms per token,  1757.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13220.70 ms /   107 runs   (  123.56 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 13547.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.96 ms /    28 runs   (    0.53 ms per token,  1872.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3447.47 ms /    28 runs   (  123.12 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  3529.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.84 ms /     7 runs   (    0.55 ms per token,  1821.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   853.89 ms /     7 runs   (  121.98 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =   874.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.09 ms /    28 runs   (    0.54 ms per token,  1855.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3488.52 ms /    28 runs   (  124.59 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =  3572.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 45: \n",
      "Language: english\n",
      "Question: \n",
      "Phakic patient evolved with a significant increase in intraocular pressure and athalamia 15 hours after a trabeculectomy. It is correct to say that:\n",
      "a) This severe ocular condition is more prevalent in aphakic patients.\n",
      "b) The use of topical cycloplegic during the immediate postoperative period is a risk factor for this condition.\n",
      "c) If there is a pervious iridotomy, this would not occur.\n",
      "d) Typically, this condition occurs in eyes with an anteroposterior diameter smaller than those of the general population.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    47.67 ms /    82 runs   (    0.58 ms per token,  1720.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2498.43 ms /   140 tokens (   17.85 ms per token,    56.04 tokens per second)\n",
      "llama_print_timings:        eval time = 10024.71 ms /    81 runs   (  123.76 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time = 12774.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.96 ms /    50 runs   (    0.58 ms per token,  1726.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6153.74 ms /    50 runs   (  123.07 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  6304.75 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    61.22 ms /   106 runs   (    0.58 ms per token,  1731.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13075.50 ms /   106 runs   (  123.35 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 13401.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   127.11 ms /   220 runs   (    0.58 ms per token,  1730.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 27088.85 ms /   220 runs   (  123.13 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 27786.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n",
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    46.92 ms /    79 runs   (    0.59 ms per token,  1683.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9717.58 ms /    79 runs   (  123.01 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  9962.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.34 ms /    25 runs   (    0.57 ms per token,  1743.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3029.71 ms /    25 runs   (  121.19 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3102.80 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    48.56 ms /    84 runs   (    0.58 ms per token,  1729.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10211.05 ms /    84 runs   (  121.56 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time = 10463.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.39 ms /    25 runs   (    0.58 ms per token,  1737.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3115.18 ms /    25 runs   (  124.61 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =  3190.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    44.88 ms /    78 runs   (    0.58 ms per token,  1738.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9690.74 ms /    78 runs   (  124.24 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =  9925.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    47.18 ms /    82 runs   (    0.58 ms per token,  1738.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10067.52 ms /    82 runs   (  122.77 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 10317.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Paciente fácico evoluiu com quadro de aumento importante da pressão intraocular e atalamia 15 horas após uma trabeculectomia. É correto afirmar que:\n",
      "a)Esse quadro ocular grave é mais prevalente em pacientes afácicos.\n",
      "b)O uso de cicloplégico tópico durante o pós-operatório imediato é fator de risco para esse quadro.\n",
      "c)Se há iridotomia pérvia, esse quadro não ocorre.\n",
      "d)Tipicamente, esse quadro ocorre em olhos com diâmetro anteroposterior menor que os da população em geral.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.38 ms /    28 runs   (    0.58 ms per token,  1709.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3335.18 ms /   174 tokens (   19.17 ms per token,    52.17 tokens per second)\n",
      "llama_print_timings:        eval time =  3322.26 ms /    27 runs   (  123.05 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  6740.68 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    66.67 ms /   115 runs   (    0.58 ms per token,  1724.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14182.52 ms /   115 runs   (  123.33 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 14537.60 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    87.03 ms /   149 runs   (    0.58 ms per token,  1712.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 18422.14 ms /   149 runs   (  123.64 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 18889.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    36.02 ms /    59 runs   (    0.61 ms per token,  1638.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7178.29 ms /    59 runs   (  121.67 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  7358.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n",
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1739.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   856.70 ms /     7 runs   (  122.39 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =   877.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.67 ms /     7 runs   (    0.67 ms per token,  1498.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   858.31 ms /     7 runs   (  122.62 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =   880.57 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n",
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   118.03 ms /   193 runs   (    0.61 ms per token,  1635.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 23791.89 ms /   193 runs   (  123.27 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 24414.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.02 ms /     7 runs   (    0.57 ms per token,  1739.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   859.45 ms /     7 runs   (  122.78 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =   880.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1737.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   856.75 ms /     7 runs   (  122.39 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =   877.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1738.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   858.82 ms /     7 runs   (  122.69 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =   880.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 46: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding cortisonic glaucoma, it is correct to state that:\n",
      "a) Occurs at all ages.\n",
      "b) Corticosteroids in low concentrations do not increase intraocular pressure.\n",
      "c) Glaucoma is typically angle-closure.\n",
      "d) It is more frequent due to the use of oral corticosteroids than in the form of eye drops.\n",
      "Test #0: \n",
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.57 ms /    28 runs   (    0.59 ms per token,  1690.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2259.23 ms /    93 tokens (   24.29 ms per token,    41.16 tokens per second)\n",
      "llama_print_timings:        eval time =  3271.29 ms /    27 runs   (  121.16 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  5615.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.35 ms /    44 runs   (    0.58 ms per token,  1735.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5426.48 ms /    44 runs   (  123.33 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  5557.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.45 ms /    25 runs   (    0.58 ms per token,  1730.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3063.52 ms /    25 runs   (  122.54 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3137.32 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.93 ms /    24 runs   (    0.58 ms per token,  1723.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2931.44 ms /    24 runs   (  122.14 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3003.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.43 ms /    25 runs   (    0.58 ms per token,  1732.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3032.51 ms /    25 runs   (  121.30 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3106.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.74 ms /    24 runs   (    0.57 ms per token,  1746.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2928.27 ms /    24 runs   (  122.01 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  2998.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.01 ms /    24 runs   (    0.58 ms per token,  1712.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2972.37 ms /    24 runs   (  123.85 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  3045.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.20 ms /    54 runs   (    0.58 ms per token,  1730.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6569.09 ms /    54 runs   (  121.65 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  6735.95 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.49 ms /    25 runs   (    0.58 ms per token,  1725.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3126.13 ms /    25 runs   (  125.05 ms per token,     8.00 tokens per second)\n",
      "llama_print_timings:       total time =  3203.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    61.70 ms /   107 runs   (    0.58 ms per token,  1734.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13197.50 ms /   107 runs   (  123.34 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 13528.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Sobre o glaucoma cortisônico, é correto afirmar que:\n",
      "a)Ocorre em todas as idades.\n",
      "b)Os corticoides em concentrações baixas não elevam a pressão intraocular.\n",
      "c)O glaucoma é tipicamente de ângulo fechado.\n",
      "d)É mais frequente por uso de corticoide via oral do que na forma de colírio.\n",
      "Test #0: \n",
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.17 ms /     7 runs   (    0.60 ms per token,  1679.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1641.52 ms /   107 tokens (   15.34 ms per token,    65.18 tokens per second)\n",
      "llama_print_timings:        eval time =   719.98 ms /     6 runs   (  120.00 ms per token,     8.33 tokens per second)\n",
      "llama_print_timings:       total time =  2382.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    67.09 ms /   115 runs   (    0.58 ms per token,  1714.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14247.34 ms /   115 runs   (  123.89 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time = 14601.67 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   147.40 ms /   254 runs   (    0.58 ms per token,  1723.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 31222.37 ms /   254 runs   (  122.92 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 32036.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   145.93 ms /   253 runs   (    0.58 ms per token,  1733.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 31125.86 ms /   253 runs   (  123.03 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 31935.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.77 ms /    41 runs   (    0.58 ms per token,  1725.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5055.45 ms /    41 runs   (  123.30 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  5177.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   150.33 ms /   260 runs   (    0.58 ms per token,  1729.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 32016.10 ms /   260 runs   (  123.14 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 32841.07 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n",
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1747.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   843.48 ms /     7 runs   (  120.50 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:       total time =   864.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.09 ms /     7 runs   (    0.58 ms per token,  1709.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   843.23 ms /     7 runs   (  120.46 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:       total time =   863.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.14 ms /     7 runs   (    0.59 ms per token,  1692.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   845.09 ms /     7 runs   (  120.73 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =   866.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    24.92 ms /    42 runs   (    0.59 ms per token,  1685.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5163.26 ms /    42 runs   (  122.93 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  5289.58 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 47: \n",
      "Language: english\n",
      "Question: \n",
      "Which of the visual field defects below best correlates with localized loss of the peripapillary nerve fiber layer (Hoyt's sign) in the inferior temporal sector?\n",
      "a) Inferior temporal cecal center.\n",
      "b) Upper nasal.\n",
      "c) Wedge-shaped upper temporal.\n",
      "d) Lower vertical.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1737.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1842.03 ms /    83 tokens (   22.19 ms per token,    45.06 tokens per second)\n",
      "llama_print_timings:        eval time =   727.62 ms /     6 runs   (  121.27 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  2590.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.47 ms /    25 runs   (    0.58 ms per token,  1727.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3058.50 ms /    25 runs   (  122.34 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3132.38 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.85 ms /    24 runs   (    0.58 ms per token,  1733.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2911.91 ms /    24 runs   (  121.33 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  2982.92 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.53 ms /    33 runs   (    0.59 ms per token,  1689.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4060.28 ms /    33 runs   (  123.04 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  4159.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n",
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.79 ms /    24 runs   (    0.57 ms per token,  1740.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2928.08 ms /    24 runs   (  122.00 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  2999.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.73 ms /    16 runs   (    0.61 ms per token,  1644.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1975.16 ms /    16 runs   (  123.45 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  2024.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.43 ms /    34 runs   (    0.57 ms per token,  1750.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4127.82 ms /    34 runs   (  121.41 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  4229.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n",
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.87 ms /    33 runs   (    0.57 ms per token,  1748.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4059.24 ms /    33 runs   (  123.01 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  4156.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.97 ms /     7 runs   (    0.57 ms per token,  1762.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   877.49 ms /     7 runs   (  125.36 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:       total time =   898.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1746.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   859.25 ms /     7 runs   (  122.75 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =   880.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Qual dos defeitos de campo visual abaixo melhor se correlaciona com perda localizada da camada de fibras nervosas peripapilar (sinal de Hoyt) no setor temporal inferior?\n",
      "a)Centro-cecal temporal inferior.\n",
      "b)Nasal superior.\n",
      "c)Temporal superior em cunha.\n",
      "d)Vertical inferior.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.17 ms /    36 runs   (    0.59 ms per token,  1700.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1971.90 ms /    89 tokens (   22.16 ms per token,    45.13 tokens per second)\n",
      "llama_print_timings:        eval time =  4298.86 ms /    35 runs   (  122.82 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  6379.15 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1736.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   850.63 ms /     7 runs   (  121.52 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =   871.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n",
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.30 ms /    37 runs   (    0.58 ms per token,  1737.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4519.79 ms /    37 runs   (  122.16 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  4630.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.05 ms /    28 runs   (    0.57 ms per token,  1744.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3369.85 ms /    28 runs   (  120.35 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:       total time =  3452.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n",
      "{'response': 'a'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.08 ms /    35 runs   (    0.57 ms per token,  1743.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4268.55 ms /    35 runs   (  121.96 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  4373.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.09 ms /     7 runs   (    0.58 ms per token,  1711.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   842.38 ms /     7 runs   (  120.34 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:       total time =   862.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.05 ms /    28 runs   (    0.57 ms per token,  1744.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3428.87 ms /    28 runs   (  122.46 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3513.03 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.73 ms /    24 runs   (    0.57 ms per token,  1748.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2927.26 ms /    24 runs   (  121.97 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  2997.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.01 ms /    35 runs   (    0.57 ms per token,  1748.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4353.54 ms /    35 runs   (  124.39 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =  4459.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.58 ms /    27 runs   (    0.58 ms per token,  1732.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3302.08 ms /    27 runs   (  122.30 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3383.07 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 48: \n",
      "Language: english\n",
      "Question: \n",
      "Hemosiderotic glaucoma is caused by: a) Accumulation of macrophages in the trabecular meshwork. b) Accumulation of blood cells incapable of diapedesis in the trabecular meshwork. c) Accumulation of iron present in hemoglobin in the trabecular meshwork. d) Clogging of the trabecular meshwork by fresh red blood cells, fibrin and plasma.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.33 ms /    45 runs   (    0.59 ms per token,  1708.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2193.08 ms /   103 tokens (   21.29 ms per token,    46.97 tokens per second)\n",
      "llama_print_timings:        eval time =  5367.95 ms /    44 runs   (  122.00 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  7699.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n",
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.62 ms /    44 runs   (    0.58 ms per token,  1717.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5361.11 ms /    44 runs   (  121.84 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  5493.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.13 ms /    37 runs   (    0.60 ms per token,  1672.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4562.53 ms /    37 runs   (  123.31 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  4673.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.93 ms /    45 runs   (    0.58 ms per token,  1735.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5520.67 ms /    45 runs   (  122.68 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5655.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.14 ms /    45 runs   (    0.58 ms per token,  1721.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5551.87 ms /    45 runs   (  123.37 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  5686.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n",
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.48 ms /    44 runs   (    0.58 ms per token,  1726.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5341.14 ms /    44 runs   (  121.39 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  5472.12 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.48 ms /    44 runs   (    0.58 ms per token,  1727.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5400.19 ms /    44 runs   (  122.73 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5531.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.97 ms /    45 runs   (    0.58 ms per token,  1732.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5490.09 ms /    45 runs   (  122.00 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  5624.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    24.19 ms /    37 runs   (    0.65 ms per token,  1529.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4548.95 ms /    37 runs   (  122.94 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  4666.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n",
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "O glaucoma hemossiderótico é causado por: a) Acúmulo de macrófagos na malha trabecular. b) Acúmulo de células sanguíneas incapazes de diapedese na malha trabecular.  c) Acúmulo de ferro presente na hemoglobina na malha trabecular. d) Entupimento da malha trabecular por hemácias frescas, fibrina e plasma.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.68 ms /    25 runs   (    0.79 ms per token,  1270.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3073.53 ms /    25 runs   (  122.94 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3161.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.97 ms /    24 runs   (    0.58 ms per token,  1718.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2498.17 ms /   116 tokens (   21.54 ms per token,    46.43 tokens per second)\n",
      "llama_print_timings:        eval time =  2804.43 ms /    23 runs   (  121.93 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  5375.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.11 ms /    47 runs   (    0.58 ms per token,  1733.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5724.69 ms /    47 runs   (  121.80 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  5865.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.44 ms /    20 runs   (    0.57 ms per token,  1747.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2475.38 ms /    20 runs   (  123.77 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  2533.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.56 ms /    27 runs   (    0.58 ms per token,  1734.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3305.66 ms /    27 runs   (  122.43 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3385.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.00 ms /     7 runs   (    0.57 ms per token,  1750.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   860.62 ms /     7 runs   (  122.95 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =   880.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.30 ms /    44 runs   (    0.58 ms per token,  1739.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5401.17 ms /    44 runs   (  122.75 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5533.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.53 ms /    27 runs   (    0.58 ms per token,  1738.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3350.08 ms /    27 runs   (  124.08 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  3432.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.12 ms /    52 runs   (    0.58 ms per token,  1726.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6420.82 ms /    52 runs   (  123.48 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  6583.60 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.78 ms /    17 runs   (    0.58 ms per token,  1738.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2060.95 ms /    17 runs   (  121.23 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  2112.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.78 ms /    24 runs   (    0.57 ms per token,  1741.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2958.99 ms /    24 runs   (  123.29 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  3033.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 49: \n",
      "Language: english\n",
      "Question: \n",
      "The classic risk factors for the development of primary open-angle glaucoma are: a) Asians, positive family history, young people. b) Elevated intraocular pressure, Caucasian race, age between 25 and 45 years. c) Elevated intraocular pressure, black race, positive family history. d) Female gender, increased corneal thickness, intraocular pressure with wide fluctuation.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.05 ms /    25 runs   (    0.60 ms per token,  1661.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2578.64 ms /   108 tokens (   23.88 ms per token,    41.88 tokens per second)\n",
      "llama_print_timings:        eval time =  2916.82 ms /    24 runs   (  121.53 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  5573.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.45 ms /    54 runs   (    0.58 ms per token,  1716.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6594.32 ms /    54 runs   (  122.12 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  6763.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.72 ms /    20 runs   (    0.59 ms per token,  1706.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2432.95 ms /    20 runs   (  121.65 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  2495.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n",
      "{'response': 'B'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.51 ms /    20 runs   (    0.58 ms per token,  1738.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2438.73 ms /    20 runs   (  121.94 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  2500.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.51 ms /    20 runs   (    0.58 ms per token,  1737.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2456.29 ms /    20 runs   (  122.81 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  2518.54 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.17 ms /    28 runs   (    0.58 ms per token,  1731.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3452.95 ms /    28 runs   (  123.32 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  3540.15 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.53 ms /    25 runs   (    0.58 ms per token,  1720.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3025.04 ms /    25 runs   (  121.00 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  3101.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.48 ms /    25 runs   (    0.58 ms per token,  1726.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3032.79 ms /    25 runs   (  121.31 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3108.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n",
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.48 ms /    25 runs   (    0.58 ms per token,  1726.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3036.50 ms /    25 runs   (  121.46 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  3113.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "São fatores de risco clássicos para o desenvolvimento de glaucoma primário de ângulo aberto: a) Asiáticos, história familiar positiva, jovens. b) Pressão intraocular elevada, raça caucasiana, idade entre 25 e 45 anos. c) Pressão intraocular elevada, raça negra, história familiar positiva. d) Sexo feminino, espessura corneal elevada, pressão intraocular com ampla flutuação.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.21 ms /    21 runs   (    0.58 ms per token,  1719.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2528.27 ms /    21 runs   (  120.39 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:       total time =  2593.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.17 ms /    28 runs   (    0.61 ms per token,  1630.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2183.89 ms /   134 tokens (   16.30 ms per token,    61.36 tokens per second)\n",
      "llama_print_timings:        eval time =  3300.05 ms /    27 runs   (  122.22 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  5574.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     5.21 ms /     9 runs   (    0.58 ms per token,  1728.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1091.73 ms /     9 runs   (  121.30 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  1119.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n",
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.05 ms /     7 runs   (    0.58 ms per token,  1728.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   844.88 ms /     7 runs   (  120.70 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =   867.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1738.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   860.50 ms /     7 runs   (  122.93 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =   881.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     5.20 ms /     9 runs   (    0.58 ms per token,  1730.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1103.88 ms /     9 runs   (  122.65 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  1131.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.05 ms /     7 runs   (    0.58 ms per token,  1727.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   872.42 ms /     7 runs   (  124.63 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time =   893.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.08 ms /    28 runs   (    0.57 ms per token,  1741.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3386.29 ms /    28 runs   (  120.94 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  3471.24 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1732.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   848.17 ms /     7 runs   (  121.17 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =   869.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.20 ms /    20 runs   (    0.61 ms per token,  1639.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2504.34 ms /    20 runs   (  125.22 ms per token,     7.99 tokens per second)\n",
      "llama_print_timings:       total time =  2568.63 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1730.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   846.34 ms /     7 runs   (  120.91 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =   868.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 50: \n",
      "Language: english\n",
      "Question: \n",
      "Male patient, 25 years old, with recurrent ocular hypertension (between 40 and 60 mmHg), painless, unilateral and of short duration, without loss of vision. Which are the most probable diagnostics? a) Fuchs heterochromic uveitis. b) Attack of acute glaucoma with bombé iris. c) Attack of acute glaucoma with plateau iris. d) Glaucomatocyclic crisis.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    38.86 ms /    64 runs   (    0.61 ms per token,  1646.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2538.25 ms /   118 tokens (   21.51 ms per token,    46.49 tokens per second)\n",
      "llama_print_timings:        eval time =  7689.64 ms /    63 runs   (  122.06 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time = 10430.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.56 ms /    27 runs   (    0.58 ms per token,  1734.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3306.30 ms /    27 runs   (  122.46 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3387.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.10 ms /    28 runs   (    0.57 ms per token,  1739.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3453.57 ms /    28 runs   (  123.34 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  3536.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.53 ms /    27 runs   (    0.58 ms per token,  1738.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3303.45 ms /    27 runs   (  122.35 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3382.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.06 ms /     7 runs   (    0.58 ms per token,  1724.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   846.35 ms /     7 runs   (  120.91 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =   866.88 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n",
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.05 ms /    28 runs   (    0.57 ms per token,  1744.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3407.45 ms /    28 runs   (  121.69 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3489.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    57.07 ms /    99 runs   (    0.58 ms per token,  1734.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12191.91 ms /    99 runs   (  123.15 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 12492.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n",
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.55 ms /    27 runs   (    0.58 ms per token,  1736.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3296.03 ms /    27 runs   (  122.08 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3376.49 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.11 ms /    28 runs   (    0.58 ms per token,  1738.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3398.39 ms /    28 runs   (  121.37 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3481.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.05 ms /     7 runs   (    0.58 ms per token,  1730.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   849.12 ms /     7 runs   (  121.30 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =   869.60 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Paciente masculino, 25 anos, com quadros reincidentes de hipertensão ocular (entre 40 e 60 mmHg), indolor, unilateral e de curta duração, sem baixa de visão. Qual o diagnóstico mais provável? a) Uveíte heterocrômica de Fuchs. b) Crise de glaucoma agudo com íris bombé. c) Crise de glaucoma agudo com íris em plateau. d) Crise glaucomatocíclica.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    39.78 ms /    69 runs   (    0.58 ms per token,  1734.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2349.34 ms /   138 tokens (   17.02 ms per token,    58.74 tokens per second)\n",
      "llama_print_timings:        eval time =  8400.17 ms /    68 runs   (  123.53 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 10959.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n",
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.58 ms /    28 runs   (    0.59 ms per token,  1688.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3456.30 ms /    28 runs   (  123.44 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  3541.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.33 ms /    27 runs   (    0.57 ms per token,  1761.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3302.69 ms /    27 runs   (  122.32 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3382.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.73 ms /    40 runs   (    0.57 ms per token,  1759.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4911.46 ms /    40 runs   (  122.79 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  5028.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.38 ms /    27 runs   (    0.57 ms per token,  1755.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3289.20 ms /    27 runs   (  121.82 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3368.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.82 ms /    28 runs   (    0.57 ms per token,  1769.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3395.28 ms /    28 runs   (  121.26 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3478.12 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    35.59 ms /    62 runs   (    0.57 ms per token,  1742.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7650.46 ms /    62 runs   (  123.39 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  7836.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.10 ms /    49 runs   (    0.57 ms per token,  1743.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5999.76 ms /    49 runs   (  122.44 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  6148.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.91 ms /     7 runs   (    0.56 ms per token,  1789.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   843.90 ms /     7 runs   (  120.56 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =   864.12 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.85 ms /     7 runs   (    0.55 ms per token,  1818.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   857.29 ms /     7 runs   (  122.47 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =   877.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 51: \n",
      "Language: english\n",
      "Question: \n",
      "A 60-year-old diabetic patient complains of pain and low visual acuity in the right eye. On ophthalmological examination, visual acuity was 0.1, intraocular pressure 45 mmHg, corneal edema, iris neovascularization, open angle in all quadrants on gonioscopy, cup/disc ratio 0.3, and proliferative diabetic retinopathy. After initiating hypotensive clinical treatment, the patient had a good response. Among the options below, which is the most appropriate course of action to follow? a) Cyclophotocoagulation. b) Drainage implant. c) Panretinal photocoagulation. d) Trabeculectomy with mitomycin C.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    94.06 ms /   162 runs   (    0.58 ms per token,  1722.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3049.17 ms /   189 tokens (   16.13 ms per token,    61.98 tokens per second)\n",
      "llama_print_timings:        eval time = 19870.23 ms /   161 runs   (  123.42 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 23425.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.18 ms /    34 runs   (    0.59 ms per token,  1684.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4161.57 ms /    34 runs   (  122.40 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  4263.95 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    97.82 ms /   169 runs   (    0.58 ms per token,  1727.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 20892.59 ms /   169 runs   (  123.62 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 21419.80 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.02 ms /     7 runs   (    0.57 ms per token,  1740.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   851.67 ms /     7 runs   (  121.67 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =   872.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.24 ms /    16 runs   (    0.58 ms per token,  1730.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1953.50 ms /    16 runs   (  122.09 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  2001.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1744.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   850.77 ms /     7 runs   (  121.54 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =   871.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.89 ms /    36 runs   (    0.58 ms per token,  1723.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4461.45 ms /    36 runs   (  123.93 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  4569.24 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   103.58 ms /   178 runs   (    0.58 ms per token,  1718.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 21945.66 ms /   178 runs   (  123.29 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 22503.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n",
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    72.76 ms /   125 runs   (    0.58 ms per token,  1718.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 15447.26 ms /   125 runs   (  123.58 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 15840.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.07 ms /    33 runs   (    0.58 ms per token,  1730.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4068.43 ms /    33 runs   (  123.29 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  4166.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Paciente diabético com 60 anos queixa-se de dor e baixa de acuidade visual no olho direito. Ao exame oftalmológico apresenta acuidade visual 0,1, pressão intraocular 45 mmHg, edema de cornea, neovascularização de íris, ângulo aberto em todos os quadrantes na gonioscopia, relação escavação/disco 0,3 e retinopatia diabética proliferativa. Após iniciar tratamento clínico hipotensor, apresentou boa resposta. Dentre as opções abaixo, qual é a conduta mais adequada a seguir? a) Ciclofotocoagulação. b) Implante de drenagem. c) Panfotocoagulação retiniana. d) Trabeculectomia com mitomicina C.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.54 ms /    19 runs   (    0.61 ms per token,  1647.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4200.43 ms /   216 tokens (   19.45 ms per token,    51.42 tokens per second)\n",
      "llama_print_timings:        eval time =  2234.62 ms /    18 runs   (  124.15 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  6493.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Trabecuectomia com mitomicina C'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.84 ms /    19 runs   (    0.62 ms per token,  1605.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2354.24 ms /    19 runs   (  123.91 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  2414.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Trabecuectomia com mitomicina C'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.25 ms /    19 runs   (    0.59 ms per token,  1689.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2352.81 ms /    19 runs   (  123.83 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  2411.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Trabecuectomia com mitomicina C'}\n",
      "Test #3: \n",
      "{'response': 'd) Trabecuectomia com mitomicina C'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.97 ms /    19 runs   (    0.58 ms per token,  1731.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2320.69 ms /    19 runs   (  122.14 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  2378.24 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.54 ms /    20 runs   (    0.58 ms per token,  1732.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2477.49 ms /    20 runs   (  123.87 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  2537.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.75 ms /    20 runs   (    0.59 ms per token,  1702.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2471.82 ms /    20 runs   (  123.59 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  2532.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.12 ms /    19 runs   (    0.59 ms per token,  1708.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2322.72 ms /    19 runs   (  122.25 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  2379.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Trabecuectomia com mitomicina C'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.20 ms /    19 runs   (    0.59 ms per token,  1696.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2336.35 ms /    19 runs   (  122.97 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  2393.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Trabecuectomia com mitomicina C'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.53 ms /    20 runs   (    0.58 ms per token,  1735.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2449.27 ms /    20 runs   (  122.46 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  2509.26 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.00 ms /    19 runs   (    0.58 ms per token,  1728.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2332.18 ms /    19 runs   (  122.75 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  2388.48 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Trabecuectomia com mitomicina C'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 52: \n",
      "Language: english\n",
      "Question: \n",
      "Among the options below, which would be the most suggestive finding of glaucoma? a) Cupping asymmetry of 0.2 between the eyes. b) Peripapillary atrophy with alpha zone in the temporal region of the nerve. c) Bilateral excavation/disk ratio of 0.5. d) Optic nerve cup/disk ratio of 0.4 with the presence of an inferior temporal notch.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.30 ms /    28 runs   (    0.58 ms per token,  1718.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2115.01 ms /   105 tokens (   20.14 ms per token,    49.65 tokens per second)\n",
      "llama_print_timings:        eval time =  3302.58 ms /    27 runs   (  122.32 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  5501.99 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.61 ms /    27 runs   (    0.58 ms per token,  1729.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3343.72 ms /    27 runs   (  123.84 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  3424.85 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.92 ms /     7 runs   (    0.56 ms per token,  1785.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   838.11 ms /     7 runs   (  119.73 ms per token,     8.35 tokens per second)\n",
      "llama_print_timings:       total time =   858.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.02 ms /    28 runs   (    0.57 ms per token,  1748.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3406.55 ms /    28 runs   (  121.66 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3489.75 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.56 ms /    27 runs   (    0.58 ms per token,  1735.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3277.91 ms /    27 runs   (  121.40 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3358.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.30 ms /    28 runs   (    0.58 ms per token,  1717.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3410.95 ms /    28 runs   (  121.82 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3494.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #6: \n",
      "{'response': 'd'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   103.70 ms /   181 runs   (    0.57 ms per token,  1745.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 22301.49 ms /   181 runs   (  123.21 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 22861.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.66 ms /    24 runs   (    0.57 ms per token,  1756.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2929.53 ms /    24 runs   (  122.06 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  2999.21 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    29.21 ms /    51 runs   (    0.57 ms per token,  1745.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6260.07 ms /    51 runs   (  122.75 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  6412.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    42.60 ms /    74 runs   (    0.58 ms per token,  1737.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9025.51 ms /    74 runs   (  121.97 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  9248.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Dentre as opções abaixo, qual seria o achado mais sugestivo de glaucoma? a) Assimetria de escavação de 0,2 entre os olhos. b) Atrofia peripapilar com zona alfa na região temporal do nervo. c) Relação escavação/disco bilateral de 0,5. d) Relação escavação/disco do nervo óptico de 0,4 com a presença de um notch temporal inferior.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    33.02 ms /    56 runs   (    0.59 ms per token,  1695.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2484.84 ms /   124 tokens (   20.04 ms per token,    49.90 tokens per second)\n",
      "llama_print_timings:        eval time =  6749.38 ms /    55 runs   (  122.72 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  9406.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.63 ms /     8 runs   (    0.58 ms per token,  1726.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   963.47 ms /     8 runs   (  120.43 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:       total time =   987.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd)'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.00 ms /    52 runs   (    0.58 ms per token,  1733.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6423.14 ms /    52 runs   (  123.52 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  6579.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.87 ms /    38 runs   (    0.58 ms per token,  1737.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4640.11 ms /    38 runs   (  122.11 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  4752.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd)'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.30 ms /    39 runs   (    0.57 ms per token,  1748.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4788.12 ms /    39 runs   (  122.77 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  4903.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.56 ms /    39 runs   (    0.58 ms per token,  1728.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4745.58 ms /    39 runs   (  121.68 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  4861.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #6: \n",
      "{'response': 'd'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    34.55 ms /    60 runs   (    0.58 ms per token,  1736.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7287.55 ms /    60 runs   (  121.46 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  7466.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.48 ms /    39 runs   (    0.58 ms per token,  1734.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4766.19 ms /    39 runs   (  122.21 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  4882.95 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    33.96 ms /    59 runs   (    0.58 ms per token,  1737.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7216.06 ms /    59 runs   (  122.31 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  7396.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    35.34 ms /    60 runs   (    0.59 ms per token,  1697.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7347.54 ms /    60 runs   (  122.46 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  7528.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 53: \n",
      "Language: english\n",
      "Question: \n",
      "\n",
      "Topical use of alpha-2-agonist eye drops, such as brimonidine tartrate, should be avoided in which of the following situations? a) Children under two years old. b) Renal lithiasis. c) Asthma. d) Sickle cell anemia.\n",
      "Test #0: \n",
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.19 ms /     7 runs   (    0.60 ms per token,  1671.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1354.81 ms /    73 tokens (   18.56 ms per token,    53.88 tokens per second)\n",
      "llama_print_timings:        eval time =   719.52 ms /     6 runs   (  119.92 ms per token,     8.34 tokens per second)\n",
      "llama_print_timings:       total time =  2095.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.57 ms /    32 runs   (    0.58 ms per token,  1722.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3845.97 ms /    32 runs   (  120.19 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:       total time =  3941.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n",
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.14 ms /    36 runs   (    0.59 ms per token,  1702.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4435.75 ms /    36 runs   (  123.22 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  4542.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.53 ms /    25 runs   (    0.58 ms per token,  1720.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3040.69 ms /    25 runs   (  121.63 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3113.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n",
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.84 ms /    17 runs   (    0.58 ms per token,  1727.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2037.94 ms /    17 runs   (  119.88 ms per token,     8.34 tokens per second)\n",
      "llama_print_timings:       total time =  2087.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    44.98 ms /    78 runs   (    0.58 ms per token,  1733.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9484.96 ms /    78 runs   (  121.60 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  9718.12 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    97.07 ms /   168 runs   (    0.58 ms per token,  1730.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 20477.54 ms /   168 runs   (  121.89 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time = 20994.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.71 ms /    25 runs   (    0.59 ms per token,  1699.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3071.53 ms /    25 runs   (  122.86 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  3146.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.82 ms /    17 runs   (    0.58 ms per token,  1730.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2045.84 ms /    17 runs   (  120.34 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:       total time =  2094.80 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.83 ms /    17 runs   (    0.58 ms per token,  1730.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2081.84 ms /    17 runs   (  122.46 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  2131.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "O uso tópico de colírio de alfa-2-agonista, como tartarato de brimonidina, deve ser evitado em qual situação, dentre as abaixo? a) Crianças abaixo de dois anos. b) Litíase renal. c) Asma. d) Anemia falciforme.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.08 ms /     7 runs   (    0.58 ms per token,  1716.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2190.22 ms /    88 tokens (   24.89 ms per token,    40.18 tokens per second)\n",
      "llama_print_timings:        eval time =   729.24 ms /     6 runs   (  121.54 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  2939.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    43.63 ms /    69 runs   (    0.63 ms per token,  1581.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8454.17 ms /    69 runs   (  122.52 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  8669.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    44.48 ms /    69 runs   (    0.64 ms per token,  1551.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8458.26 ms /    69 runs   (  122.58 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  8677.03 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     5.55 ms /     7 runs   (    0.79 ms per token,  1262.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   849.12 ms /     7 runs   (  121.30 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =   872.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    48.38 ms /    72 runs   (    0.67 ms per token,  1488.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8808.80 ms /    72 runs   (  122.34 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  9045.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n",
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.94 ms /    31 runs   (    0.71 ms per token,  1413.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3809.47 ms /    31 runs   (  122.89 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  3911.58 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.97 ms /     7 runs   (    0.57 ms per token,  1762.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   852.52 ms /     7 runs   (  121.79 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =   872.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1732.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   892.12 ms /     7 runs   (  127.45 ms per token,     7.85 tokens per second)\n",
      "llama_print_timings:       total time =   912.93 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n",
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    40.97 ms /    71 runs   (    0.58 ms per token,  1732.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8657.17 ms /    71 runs   (  121.93 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  8873.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 54: \n",
      "Language: english\n",
      "Question: \n",
      "Which finding is most commonly associated with primary congenital glaucoma? a) Optic nerve edema. b) Ruptures in Descemet's membrane. c) Dislocation of the lens. d) Tent-like peripheral anterior synechiae.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.99 ms /     7 runs   (    0.57 ms per token,  1755.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   842.94 ms /     7 runs   (  120.42 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:       total time =   863.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.87 ms /    39 runs   (    0.59 ms per token,  1705.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1668.71 ms /    69 tokens (   24.18 ms per token,    41.35 tokens per second)\n",
      "llama_print_timings:        eval time =  4592.09 ms /    38 runs   (  120.84 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  6376.95 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Tent-like peripheral anterior synechiae.'}\n",
      "Test #1: \n",
      "{'response': 'd) Tent-like peripheral anterior synechiae.'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.54 ms /    39 runs   (    0.58 ms per token,  1730.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4783.20 ms /    39 runs   (  122.65 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  4900.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Tent-like peripheral anterior synechiae.'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.41 ms /    38 runs   (    0.59 ms per token,  1695.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4662.92 ms /    38 runs   (  122.71 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  4778.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.95 ms /    38 runs   (    0.58 ms per token,  1731.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4629.27 ms /    38 runs   (  121.82 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  4742.57 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Tent-like peripheral anterior synechiae.'}\n",
      "Test #4: \n",
      "{'response': 'd) Tent-like peripheral anterior synechiae.'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.50 ms /    39 runs   (    0.58 ms per token,  1733.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4834.32 ms /    39 runs   (  123.96 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  4948.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.57 ms /    39 runs   (    0.58 ms per token,  1728.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4751.12 ms /    39 runs   (  121.82 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  4867.54 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Tent-like peripheral anterior synechiae.'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.15 ms /    21 runs   (    0.58 ms per token,  1728.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2574.15 ms /    21 runs   (  122.58 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  2636.54 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Tent-like peripheral anterior synechiae.'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.60 ms /    25 runs   (    0.58 ms per token,  1712.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3025.09 ms /    25 runs   (  121.00 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  3098.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.57 ms /    39 runs   (    0.58 ms per token,  1728.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4746.65 ms /    39 runs   (  121.71 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  4862.67 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Tent-like peripheral anterior synechiae.'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.97 ms /    38 runs   (    0.58 ms per token,  1729.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4626.62 ms /    38 runs   (  121.75 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  4738.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Tent-like peripheral anterior synechiae.'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Qual achado é mais comumente associado ao glaucoma congênito primário? a) Edema de nervo óptico. b) Rupturas na membrana de Descemet. c) Luxação do cristalino. d) Sinéquias anteriores periféricas em tenda.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.11 ms /    38 runs   (    0.58 ms per token,  1719.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2036.10 ms /    78 tokens (   26.10 ms per token,    38.31 tokens per second)\n",
      "llama_print_timings:        eval time =  4526.57 ms /    37 runs   (  122.34 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  6674.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n",
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.06 ms /     7 runs   (    0.58 ms per token,  1722.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   855.63 ms /     7 runs   (  122.23 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =   875.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.18 ms /    28 runs   (    0.58 ms per token,  1731.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3384.26 ms /    28 runs   (  120.87 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  3465.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.10 ms /    28 runs   (    0.65 ms per token,  1546.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3420.32 ms /    28 runs   (  122.15 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3508.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.19 ms /    28 runs   (    0.65 ms per token,  1539.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3388.78 ms /    28 runs   (  121.03 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  3475.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.03 ms /    44 runs   (    0.59 ms per token,  1690.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5403.71 ms /    44 runs   (  122.81 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  5537.92 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.61 ms /    20 runs   (    0.58 ms per token,  1722.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2430.48 ms /    20 runs   (  121.52 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  2491.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.05 ms /    28 runs   (    0.57 ms per token,  1744.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3382.95 ms /    28 runs   (  120.82 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  3468.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.58 ms /    27 runs   (    0.58 ms per token,  1732.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3319.84 ms /    27 runs   (  122.96 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3402.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.54 ms /    25 runs   (    0.58 ms per token,  1719.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3039.77 ms /    25 runs   (  121.59 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3115.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 55: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding the pinhole test, it is correct to state: a) If the patient's visual acuity does not improve, it is a case of amblyopia. b) Decrease the depth of focus, due to the reduction of the pupil. c) Reduces diffusion circles on the retina. d) The size of the pinhole hole should be between 4 and 5 mm.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.12 ms /    20 runs   (    0.61 ms per token,  1650.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2503.49 ms /    93 tokens (   26.92 ms per token,    37.15 tokens per second)\n",
      "llama_print_timings:        eval time =  2314.09 ms /    19 runs   (  121.79 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  4879.49 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.51 ms /    28 runs   (    0.59 ms per token,  1696.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3432.48 ms /    28 runs   (  122.59 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3520.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.53 ms /    20 runs   (    0.58 ms per token,  1734.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2449.26 ms /    20 runs   (  122.46 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  2510.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.23 ms /    28 runs   (    0.58 ms per token,  1724.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3436.18 ms /    28 runs   (  122.72 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  3522.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.58 ms /    20 runs   (    0.58 ms per token,  1727.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2467.03 ms /    20 runs   (  123.35 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  2527.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.03 ms /    28 runs   (    0.57 ms per token,  1746.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3407.33 ms /    28 runs   (  121.69 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3492.88 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.52 ms /    27 runs   (    0.57 ms per token,  1739.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3297.18 ms /    27 runs   (  122.12 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3378.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.09 ms /    28 runs   (    0.57 ms per token,  1740.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3526.55 ms /    28 runs   (  125.95 ms per token,     7.94 tokens per second)\n",
      "llama_print_timings:       total time =  3608.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.04 ms /    28 runs   (    0.57 ms per token,  1745.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3402.18 ms /    28 runs   (  121.51 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  3484.92 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.81 ms /    17 runs   (    0.58 ms per token,  1733.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2095.44 ms /    17 runs   (  123.26 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  2145.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Sobre o teste do buraco estenopeico, é correto afirmar: a) Se a acuidade visual do paciente não melhorar, trata-se de um caso de ambliopia. b) Diminui a profundidade de foco, por haver redução da pupila. c) Reduz os círculos de difusão na retina. d) O tamanho do buraco estenopeico deve ser entre 4 e 5 mm.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.21 ms /     7 runs   (    0.60 ms per token,  1663.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1896.90 ms /   116 tokens (   16.35 ms per token,    61.15 tokens per second)\n",
      "llama_print_timings:        eval time =   722.15 ms /     6 runs   (  120.36 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:       total time =  2640.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.41 ms /    23 runs   (    0.58 ms per token,  1714.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2811.68 ms /    23 runs   (  122.25 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  2880.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1735.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   885.78 ms /     7 runs   (  126.54 ms per token,     7.90 tokens per second)\n",
      "llama_print_timings:       total time =   906.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.02 ms /     7 runs   (    0.57 ms per token,  1740.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   842.50 ms /     7 runs   (  120.36 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:       total time =   862.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.06 ms /     7 runs   (    0.58 ms per token,  1724.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   850.78 ms /     7 runs   (  121.54 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =   871.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n",
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.80 ms /    17 runs   (    0.58 ms per token,  1734.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2069.37 ms /    17 runs   (  121.73 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  2119.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1738.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   841.10 ms /     7 runs   (  120.16 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:       total time =   862.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.07 ms /     7 runs   (    0.58 ms per token,  1721.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   861.48 ms /     7 runs   (  123.07 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =   882.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1733.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   871.24 ms /     7 runs   (  124.46 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =   891.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.13 ms /     7 runs   (    0.59 ms per token,  1695.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   890.75 ms /     7 runs   (  127.25 ms per token,     7.86 tokens per second)\n",
      "llama_print_timings:       total time =   911.58 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 56: \n",
      "Language: english\n",
      "Question: \n",
      "With regard to the Snellen chart, it is correct to state: a) A visual acuity of 0.05 in one eye allows for a professional D driver's license. b) Facectomy with implantation of an intraocular lens is necessary in phakic patients with visual acuity of 1.5 with the best fix. c) The patient with a visual acuity of 0.25 has a visual angle of 0.5 minutes. d) The patient with a visual acuity of 0.2 has subnormal vision.\n",
      "Test #0: \n",
      "{'response': 'd'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   136.62 ms /   235 runs   (    0.58 ms per token,  1720.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2911.62 ms /   132 tokens (   22.06 ms per token,    45.34 tokens per second)\n",
      "llama_print_timings:        eval time = 28757.73 ms /   234 runs   (  122.90 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 32414.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.50 ms /    46 runs   (    0.58 ms per token,  1735.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5598.81 ms /    46 runs   (  121.71 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  5738.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    77.88 ms /   136 runs   (    0.57 ms per token,  1746.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16727.90 ms /   136 runs   (  123.00 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 17144.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #3: \n",
      "{'response': 'd'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    46.92 ms /    82 runs   (    0.57 ms per token,  1747.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10083.81 ms /    82 runs   (  122.97 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 10329.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    68.85 ms /   120 runs   (    0.57 ms per token,  1742.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14751.22 ms /   120 runs   (  122.93 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 15116.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #5: \n",
      "{'response': 'd'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.98 ms /    28 runs   (    0.57 ms per token,  1752.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3421.45 ms /    28 runs   (  122.19 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3504.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.52 ms /    53 runs   (    0.58 ms per token,  1736.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6513.61 ms /    53 runs   (  122.90 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  6671.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'D'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.70 ms /    24 runs   (    0.57 ms per token,  1751.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2909.81 ms /    24 runs   (  121.24 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  2981.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    33.08 ms /    58 runs   (    0.57 ms per token,  1753.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7128.57 ms /    58 runs   (  122.91 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  7301.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.02 ms /    28 runs   (    0.57 ms per token,  1748.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3430.18 ms /    28 runs   (  122.51 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3514.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Com relação à tabela de Snellen, é correto afirmar: a) A acuidade visual de 0,05 em um olho permite a carteira profissional de direção letra D. b) É necessária a facectomia com implante de lente intraocular em pacientes fácicos com acuidades visuais de 1,5 com a melhor correção. c) O paciente com acuidade visual de 0,25 tem um ângulo visual de 0,5 minuto. d) O paciente com acuidade visual de 0,2 tem visão subnormal.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.10 ms /     7 runs   (    0.59 ms per token,  1708.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2980.35 ms /   152 tokens (   19.61 ms per token,    51.00 tokens per second)\n",
      "llama_print_timings:        eval time =   737.96 ms /     6 runs   (  122.99 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3738.92 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   124.33 ms /   215 runs   (    0.58 ms per token,  1729.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 26571.31 ms /   215 runs   (  123.59 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 27256.58 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) O paciente com acuidade visual de 0,2 tem visão subnormal.'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.96 ms /     7 runs   (    0.57 ms per token,  1769.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   868.87 ms /     7 runs   (  124.12 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =   890.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   102.71 ms /   180 runs   (    0.57 ms per token,  1752.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 22222.92 ms /   180 runs   (  123.46 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 22785.01 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   124.05 ms /   215 runs   (    0.58 ms per token,  1733.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 26565.50 ms /   215 runs   (  123.56 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 27244.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    49.22 ms /    86 runs   (    0.57 ms per token,  1747.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10600.17 ms /    86 runs   (  123.26 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 10857.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    48.11 ms /    84 runs   (    0.57 ms per token,  1746.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10312.81 ms /    84 runs   (  122.77 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 10564.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'D'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.86 ms /     7 runs   (    0.55 ms per token,  1811.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   893.83 ms /     7 runs   (  127.69 ms per token,     7.83 tokens per second)\n",
      "llama_print_timings:       total time =   913.67 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   123.89 ms /   215 runs   (    0.58 ms per token,  1735.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 26548.40 ms /   215 runs   (  123.48 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 27220.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.36 ms /    45 runs   (    0.56 ms per token,  1774.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5508.45 ms /    45 runs   (  122.41 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  5642.82 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 57: \n",
      "Language: english\n",
      "Question: \n",
      "Which of the materials below has the highest Abbe index? a) CR-39 resin b) Trivex. c) High index resin. d) Polycarbonate.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.81 ms /    24 runs   (    0.58 ms per token,  1738.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1097.01 ms /    46 tokens (   23.85 ms per token,    41.93 tokens per second)\n",
      "llama_print_timings:        eval time =  2801.28 ms /    23 runs   (  121.79 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3969.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.35 ms /    25 runs   (    0.57 ms per token,  1742.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3014.41 ms /    25 runs   (  120.58 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =  3089.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    35.09 ms /    61 runs   (    0.58 ms per token,  1738.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7429.48 ms /    61 runs   (  121.79 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  7612.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.75 ms /    28 runs   (    0.60 ms per token,  1672.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3382.08 ms /    28 runs   (  120.79 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  3466.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.00 ms /     7 runs   (    0.57 ms per token,  1751.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   846.56 ms /     7 runs   (  120.94 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =   867.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.22 ms /    25 runs   (    0.57 ms per token,  1758.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3001.14 ms /    25 runs   (  120.05 ms per token,     8.33 tokens per second)\n",
      "llama_print_timings:       total time =  3075.26 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.93 ms /     7 runs   (    0.56 ms per token,  1779.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   844.75 ms /     7 runs   (  120.68 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =   864.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.77 ms /    54 runs   (    0.59 ms per token,  1699.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6598.00 ms /    54 runs   (  122.19 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  6761.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #8: \n",
      "{'response': 'B'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.82 ms /    24 runs   (    0.58 ms per token,  1736.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2896.70 ms /    24 runs   (  120.70 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =  2968.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.90 ms /     7 runs   (    0.56 ms per token,  1793.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   844.06 ms /     7 runs   (  120.58 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =   864.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Qual dos materiais abaixo apresenta maior índice Abbe? a) Resina CR-39 b) Trivex. c) Resina de alto índice. d) Policarbonato.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.97 ms /    24 runs   (    0.58 ms per token,  1718.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1378.11 ms /    56 tokens (   24.61 ms per token,    40.64 tokens per second)\n",
      "llama_print_timings:        eval time =  2800.46 ms /    23 runs   (  121.76 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  4250.68 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.06 ms /     7 runs   (    0.58 ms per token,  1723.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   904.61 ms /     7 runs   (  129.23 ms per token,     7.74 tokens per second)\n",
      "llama_print_timings:       total time =   925.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.06 ms /     7 runs   (    0.58 ms per token,  1722.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   848.22 ms /     7 runs   (  121.17 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =   869.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.99 ms /     7 runs   (    0.57 ms per token,  1755.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   849.57 ms /     7 runs   (  121.37 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =   869.68 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n",
      "{'response': 'A'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1730.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   867.04 ms /     7 runs   (  123.86 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =   886.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.05 ms /     7 runs   (    0.58 ms per token,  1727.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   846.82 ms /     7 runs   (  120.97 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =   866.99 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.11 ms /    25 runs   (    0.56 ms per token,  1771.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3100.53 ms /    25 runs   (  124.02 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  3174.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1735.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   852.13 ms /     7 runs   (  121.73 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =   872.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.21 ms /    16 runs   (    0.58 ms per token,  1737.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1972.37 ms /    16 runs   (  123.27 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  2018.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1733.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   850.20 ms /     7 runs   (  121.46 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =   870.57 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 58: \n",
      "Language: english\n",
      "Question: \n",
      "A 65-year-old patient, myopic to -1.00 spherical diopters in both eyes, can read at 66 cm without lens correction. What is his accommodative range and what eyeglasses are sufficient for him to read at 33 cm, respectively? a) 0.50 spherical diopters; +1.00 spherical diopters. b) 0.50 spherical diopters; +1.50 spherical diopters. c) 1.00 spherical diopters; +1.50 spherical diopters. d) 1.00 spherical diopters; +2.00 spherical diopters.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   110.97 ms /   198 runs   (    0.56 ms per token,  1784.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4085.70 ms /   169 tokens (   24.18 ms per token,    41.36 tokens per second)\n",
      "llama_print_timings:        eval time = 24462.31 ms /   197 runs   (  124.17 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time = 29161.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   109.04 ms /   193 runs   (    0.56 ms per token,  1770.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 23816.58 ms /   193 runs   (  123.40 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 24421.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   112.17 ms /   198 runs   (    0.57 ms per token,  1765.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 24428.66 ms /   198 runs   (  123.38 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 25046.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   113.00 ms /   198 runs   (    0.57 ms per token,  1752.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 24538.80 ms /   198 runs   (  123.93 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time = 25163.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n",
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   113.06 ms /   198 runs   (    0.57 ms per token,  1751.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 24494.36 ms /   198 runs   (  123.71 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time = 25108.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   112.82 ms /   198 runs   (    0.57 ms per token,  1754.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 24457.27 ms /   198 runs   (  123.52 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 25069.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   112.12 ms /   198 runs   (    0.57 ms per token,  1765.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 24502.37 ms /   198 runs   (  123.75 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time = 25122.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.34 ms /    48 runs   (    0.53 ms per token,  1894.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5903.91 ms /    48 runs   (  123.00 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  6044.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n",
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.89 ms /    51 runs   (    0.55 ms per token,  1828.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6269.71 ms /    51 runs   (  122.94 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  6420.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.09 ms /    50 runs   (    0.54 ms per token,  1845.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6086.41 ms /    50 runs   (  121.73 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  6233.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Um paciente de 65 anos, míope de -1,00 dioptrias esfericas em ambos olhos, consegue ler a 66 cm, sem correção de lentes. Qual a sua amplitude acomodativa e qual grau de óculos é suficiente para ele ler a 33 cm, respectivamente? a) 0,50 dioptrias esfericas; +1,00 dioptrias esfericas. b) 0,50 dioptrias esfericas; +1,50 dioptrias esfericas. c) 1,00 dioptrias esfericas; +1,50 dioptrias esfericas. d) 1,00 dioptrias esfericas; +2,00 dioptrias esfericas.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.69 ms /     7 runs   (    0.53 ms per token,  1895.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3398.21 ms /   188 tokens (   18.08 ms per token,    55.32 tokens per second)\n",
      "llama_print_timings:        eval time =   727.08 ms /     6 runs   (  121.18 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  4145.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.51 ms /     7 runs   (    0.50 ms per token,  1992.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   854.76 ms /     7 runs   (  122.11 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =   875.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n",
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.49 ms /     7 runs   (    0.50 ms per token,  2007.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   848.22 ms /     7 runs   (  121.17 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =   869.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.47 ms /     7 runs   (    0.50 ms per token,  2015.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   858.77 ms /     7 runs   (  122.68 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =   878.67 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.48 ms /     7 runs   (    0.50 ms per token,  2014.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   846.38 ms /     7 runs   (  120.91 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =   866.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.49 ms /     7 runs   (    0.50 ms per token,  2008.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   850.20 ms /     7 runs   (  121.46 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =   869.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n",
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.47 ms /     7 runs   (    0.50 ms per token,  2017.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   885.36 ms /     7 runs   (  126.48 ms per token,     7.91 tokens per second)\n",
      "llama_print_timings:       total time =   905.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.54 ms /     7 runs   (    0.51 ms per token,  1980.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   868.50 ms /     7 runs   (  124.07 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =   888.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n",
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.52 ms /     7 runs   (    0.50 ms per token,  1990.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   870.01 ms /     7 runs   (  124.29 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =   890.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.48 ms /     7 runs   (    0.50 ms per token,  2011.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   850.89 ms /     7 runs   (  121.56 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =   870.83 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 59: \n",
      "Language: english\n",
      "Question: \n",
      "What range of accommodation is expected for a 30-year-old? a) 1.00 spherical diopters. b) 4.00 spherical diopters. c) 7.00 spherical diopters. d) 12.00 spherical diopters.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.52 ms /    36 runs   (    0.54 ms per token,  1843.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1863.36 ms /    76 tokens (   24.52 ms per token,    40.79 tokens per second)\n",
      "llama_print_timings:        eval time =  4237.63 ms /    35 runs   (  121.08 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  6207.12 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.37 ms /    39 runs   (    0.55 ms per token,  1825.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4791.48 ms /    39 runs   (  122.86 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  4906.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.20 ms /    28 runs   (    0.54 ms per token,  1841.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3413.21 ms /    28 runs   (  121.90 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3494.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n",
      "{'response': 'B'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.58 ms /    27 runs   (    0.54 ms per token,  1851.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3258.98 ms /    27 runs   (  120.70 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  3336.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    52.81 ms /    89 runs   (    0.59 ms per token,  1685.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10922.07 ms /    89 runs   (  122.72 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 11197.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.86 ms /    36 runs   (    0.55 ms per token,  1812.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4395.40 ms /    36 runs   (  122.09 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  4502.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.43 ms /    39 runs   (    0.55 ms per token,  1819.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4815.89 ms /    39 runs   (  123.48 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  4929.67 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.17 ms /    40 runs   (    0.55 ms per token,  1804.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4864.71 ms /    40 runs   (  121.62 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  4982.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.06 ms /    40 runs   (    0.55 ms per token,  1813.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4880.13 ms /    40 runs   (  122.00 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  4999.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.06 ms /    37 runs   (    0.54 ms per token,  1844.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4495.18 ms /    37 runs   (  121.49 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  4606.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Qual amplitude de acomodação esperada para uma pessoa de 30 anos de idade? a) 1,00 dioptria esferica. b) 4,00 dioptria esferica. c) 7,00 dioptria esferica. d) 12,00 dioptria esferica.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.62 ms /     7 runs   (    0.52 ms per token,  1933.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2065.73 ms /    85 tokens (   24.30 ms per token,    41.15 tokens per second)\n",
      "llama_print_timings:        eval time =   722.59 ms /     6 runs   (  120.43 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:       total time =  2808.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    36.91 ms /    65 runs   (    0.57 ms per token,  1761.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7962.61 ms /    65 runs   (  122.50 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  8163.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.59 ms /    30 runs   (    0.52 ms per token,  1924.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3646.61 ms /    30 runs   (  121.55 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  3736.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.21 ms /    20 runs   (    0.51 ms per token,  1957.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2479.39 ms /    20 runs   (  123.97 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  2536.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    48.09 ms /    85 runs   (    0.57 ms per token,  1767.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10436.65 ms /    85 runs   (  122.78 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 10694.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.32 ms /    33 runs   (    0.52 ms per token,  1905.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4034.09 ms /    33 runs   (  122.25 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  4132.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n",
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.41 ms /    37 runs   (    0.55 ms per token,  1812.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4500.59 ms /    37 runs   (  121.64 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  4610.68 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.98 ms /    20 runs   (    0.50 ms per token,  2004.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2424.37 ms /    20 runs   (  121.22 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  2483.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.46 ms /    41 runs   (    0.55 ms per token,  1825.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4968.57 ms /    41 runs   (  121.18 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  5091.68 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.44 ms /    20 runs   (    0.52 ms per token,  1916.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2445.92 ms /    20 runs   (  122.30 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  2505.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 60: \n",
      "Language: english\n",
      "Question: \n",
      "An electronics technician performs, without correction, an accommodative effort of 6.00 DE when working at 20 cm. Which alternative best represents this patient's refraction? a) +1.50 spherical diopters -2.00 cylindrical diopters x 180°. b) +1.50 spherical diopters -3.00 cylindrical diopters x 90°. c) +2.50 spherical diopters -3.00 cylindrical diopters x 180°. d) +3.50 spherical diopters -3.00 cylindrical diopters x 90°.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    71.71 ms /   126 runs   (    0.57 ms per token,  1757.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3106.58 ms /   164 tokens (   18.94 ms per token,    52.79 tokens per second)\n",
      "llama_print_timings:        eval time = 15355.10 ms /   125 runs   (  122.84 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 18854.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n",
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    55.89 ms /    99 runs   (    0.56 ms per token,  1771.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12245.07 ms /    99 runs   (  123.69 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time = 12548.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.15 ms /    56 runs   (    0.56 ms per token,  1797.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6839.00 ms /    56 runs   (  122.12 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  7006.75 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.55 ms /    56 runs   (    0.56 ms per token,  1774.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6877.99 ms /    56 runs   (  122.82 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  7045.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n",
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    28.32 ms /    51 runs   (    0.56 ms per token,  1800.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6214.16 ms /    51 runs   (  121.85 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  6365.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.66 ms /    55 runs   (    0.56 ms per token,  1793.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6806.16 ms /    55 runs   (  123.75 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  6969.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.25 ms /    35 runs   (    0.52 ms per token,  1917.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4325.61 ms /    35 runs   (  123.59 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  4426.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    32.20 ms /    55 runs   (    0.59 ms per token,  1707.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6728.06 ms /    55 runs   (  122.33 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  6892.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.48 ms /    55 runs   (    0.55 ms per token,  1804.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6734.31 ms /    55 runs   (  122.44 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  6896.15 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    31.20 ms /    55 runs   (    0.57 ms per token,  1762.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6802.53 ms /    55 runs   (  123.68 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  6966.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Um técnico em eletrônica realiza, sem correção, esforço acomodativo de 6,00 DE quando trabalha a 20 cm. Qual alternativa melhor representa a refração deste paciente? a) +1,50 dioptrias esfericas -2,00 dioptrias cilindricas x 180°. b) +1,50 dioptrias esfericas -3,00 dioptrias cilindricas x 90°. c) +2,50 dioptrias esfericas -3,00 dioptrias cilindricas x 180°. d) +3,50 dioptrias esfericas -3,00 dioptrias cilindricas x 90°.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.84 ms /     7 runs   (    0.55 ms per token,  1822.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3297.02 ms /   189 tokens (   17.44 ms per token,    57.32 tokens per second)\n",
      "llama_print_timings:        eval time =   741.81 ms /     6 runs   (  123.63 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  4058.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.44 ms /    38 runs   (    0.56 ms per token,  1772.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4704.93 ms /    38 runs   (  123.81 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  4817.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    33.80 ms /    59 runs   (    0.57 ms per token,  1745.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7309.28 ms /    59 runs   (  123.89 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  7486.57 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.83 ms /     7 runs   (    0.55 ms per token,  1827.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   862.42 ms /     7 runs   (  123.20 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =   883.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.84 ms /     7 runs   (    0.55 ms per token,  1821.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   856.68 ms /     7 runs   (  122.38 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =   877.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.15 ms /    38 runs   (    0.56 ms per token,  1796.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4682.11 ms /    38 runs   (  123.21 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  4794.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.31 ms /    38 runs   (    0.56 ms per token,  1783.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4646.74 ms /    38 runs   (  122.28 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  4759.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.28 ms /    38 runs   (    0.56 ms per token,  1785.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4640.50 ms /    38 runs   (  122.12 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  4753.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    51.45 ms /    88 runs   (    0.58 ms per token,  1710.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10817.72 ms /    88 runs   (  122.93 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 11087.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.19 ms /    38 runs   (    0.56 ms per token,  1793.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4687.99 ms /    38 runs   (  123.37 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  4801.02 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 61: \n",
      "Language: english\n",
      "Question: \n",
      "A three-year-old child has an esotropia of 70 prismatic diopters (uncorrected), an accommodative convergence/accommodation ratio of 6, and refraction under cycloplegia of +3.50 spherical diopters in both eyes. After prescription of refraction, it is more likely that the deviation with the use of glasses, in prismatic diopters, is approximately: a) Zero. b) 20. c) 50. d) 70.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    12.22 ms /    21 runs   (    0.58 ms per token,  1718.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2923.88 ms /   124 tokens (   23.58 ms per token,    42.41 tokens per second)\n",
      "llama_print_timings:        eval time =  2451.91 ms /    20 runs   (  122.60 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  5438.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   140.49 ms /   243 runs   (    0.58 ms per token,  1729.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 29830.00 ms /   243 runs   (  122.76 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 30600.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.19 ms /    28 runs   (    0.58 ms per token,  1729.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3412.23 ms /    28 runs   (  121.87 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3496.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) 70'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    65.41 ms /   113 runs   (    0.58 ms per token,  1727.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13816.41 ms /   113 runs   (  122.27 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time = 14170.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.23 ms /    28 runs   (    0.58 ms per token,  1725.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3425.41 ms /    28 runs   (  122.34 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3512.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) 70'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.18 ms /    28 runs   (    0.58 ms per token,  1730.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3428.02 ms /    28 runs   (  122.43 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3513.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    57.00 ms /    99 runs   (    0.58 ms per token,  1736.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12079.24 ms /    99 runs   (  122.01 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time = 12392.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.96 ms /    24 runs   (    0.58 ms per token,  1719.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3013.12 ms /    24 runs   (  125.55 ms per token,     7.97 tokens per second)\n",
      "llama_print_timings:       total time =  3089.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.13 ms /    28 runs   (    0.58 ms per token,  1735.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3445.33 ms /    28 runs   (  123.05 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3531.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n",
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Criança de três anos apresenta esotropia de 70 dioptrias prismáticas (sem correção), relação convergência acomodativa/acomodação de 6, e refração sob cicloplegia de +3.50 dioptrias esfericas em ambos os olhos. Após a prescrição da refração, é mais provável que o desvio com o uso dos óculos, em dioptrias prismáticas, seja de aproximadamente: a) Zero. b) 20. c) 50. d) 70.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    89.93 ms /   153 runs   (    0.59 ms per token,  1701.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 18862.47 ms /   153 runs   (  123.28 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 19359.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    40.84 ms /    70 runs   (    0.58 ms per token,  1714.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2220.64 ms /   143 tokens (   15.53 ms per token,    64.40 tokens per second)\n",
      "llama_print_timings:        eval time =  8560.33 ms /    69 runs   (  124.06 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time = 11004.32 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1733.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   867.78 ms /     7 runs   (  123.97 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =   888.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n",
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1737.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   862.73 ms /     7 runs   (  123.25 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =   884.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.06 ms /     7 runs   (    0.58 ms per token,  1724.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   848.58 ms /     7 runs   (  121.23 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =   869.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    77.59 ms /   134 runs   (    0.58 ms per token,  1727.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16522.56 ms /   134 runs   (  123.30 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 16948.60 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.43 ms /    32 runs   (    0.58 ms per token,  1736.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3902.79 ms /    32 runs   (  121.96 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  4001.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.10 ms /     7 runs   (    0.59 ms per token,  1706.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   855.31 ms /     7 runs   (  122.19 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =   876.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    86.17 ms /   149 runs   (    0.58 ms per token,  1729.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 18355.43 ms /   149 runs   (  123.19 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 18825.12 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.34 ms /    11 runs   (    0.58 ms per token,  1735.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1345.72 ms /    11 runs   (  122.34 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  1378.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   124.19 ms /   215 runs   (    0.58 ms per token,  1731.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 26607.64 ms /   215 runs   (  123.76 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time = 27280.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 62: \n",
      "Language: english\n",
      "Question: \n",
      "The measurement of the amplitude of fusional convergence is made by placing prisms in front of the eyes. Which of the alternatives below best represents the proper position of the prisms, considering a patient without strabismus? a) Temporal base in the eye with better visual acuity and nasal base in the other eye. b) Temporal base in the right eye. c) Nasal base in the eye with better visual acuity and temporal base in the other eye. d) Nasal base in the left eye.\n",
      "Test #0: \n",
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.00 ms /    45 runs   (    0.58 ms per token,  1730.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2334.09 ms /   117 tokens (   19.95 ms per token,    50.13 tokens per second)\n",
      "llama_print_timings:        eval time =  5382.59 ms /    44 runs   (  122.33 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  7850.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.50 ms /    45 runs   (    0.57 ms per token,  1764.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5523.95 ms /    45 runs   (  122.75 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5656.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.59 ms /    24 runs   (    0.57 ms per token,  1765.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2972.60 ms /    24 runs   (  123.86 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  3043.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.70 ms /    24 runs   (    0.57 ms per token,  1752.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2929.76 ms /    24 runs   (  122.07 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3000.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1737.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   847.50 ms /     7 runs   (  121.07 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =   868.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n",
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    33.85 ms /    59 runs   (    0.57 ms per token,  1743.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7203.41 ms /    59 runs   (  122.09 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  7383.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.69 ms /    24 runs   (    0.57 ms per token,  1752.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2933.51 ms /    24 runs   (  122.23 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3005.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.44 ms /    46 runs   (    0.57 ms per token,  1740.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5646.21 ms /    46 runs   (  122.74 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5784.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.03 ms /    46 runs   (    0.57 ms per token,  1767.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5644.01 ms /    46 runs   (  122.70 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5782.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n",
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "A medida da amplitude de convergência fusional é feita pela colocação de prismas diante dos olhos. Qual das alternativas abaixo melhor representa a posição adequada dos prismas, considerando um paciente sem estrabismo? a) Base temporal no olho de melhor acuidade visual e base nasal no outro olho. b) Base temporal no olho direito. c) Base nasal no olho de melhor acuidade visual e base temporal no outro olho. d) Base nasal no olho esquerdo.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.16 ms /    27 runs   (    0.56 ms per token,  1780.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3340.92 ms /    27 runs   (  123.74 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  3421.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.10 ms /     7 runs   (    0.59 ms per token,  1708.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2654.56 ms /   134 tokens (   19.81 ms per token,    50.48 tokens per second)\n",
      "llama_print_timings:        eval time =   751.80 ms /     6 runs   (  125.30 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:       total time =  3427.49 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.79 ms /    28 runs   (    0.60 ms per token,  1668.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3399.29 ms /    28 runs   (  121.40 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3486.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n",
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1744.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   873.59 ms /     7 runs   (  124.80 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time =   894.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.36 ms /    27 runs   (    0.57 ms per token,  1757.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3300.90 ms /    27 runs   (  122.26 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3381.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.38 ms /    27 runs   (    0.57 ms per token,  1754.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3318.07 ms /    27 runs   (  122.89 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  3399.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n",
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1737.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   841.67 ms /     7 runs   (  120.24 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:       total time =   862.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.13 ms /    28 runs   (    0.58 ms per token,  1735.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3518.71 ms /    28 runs   (  125.67 ms per token,     7.96 tokens per second)\n",
      "llama_print_timings:       total time =  3603.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.97 ms /    38 runs   (    0.58 ms per token,  1729.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4685.44 ms /    38 runs   (  123.30 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  4800.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1746.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   843.52 ms /     7 runs   (  120.50 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:       total time =   864.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.54 ms /    27 runs   (    0.58 ms per token,  1737.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3337.91 ms /    27 runs   (  123.63 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  3419.92 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 63: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding atropine 1% eye drops, mark the correct alternative. a) It is contraindicated for the treatment of amblyopia b) It causes good cycloplegia, but mild mydriasis. c) It causes prolonged cycloplegia, with an effect that lasts for up to 15 days after instillation. d) Its maximum action occurs four hours after instillation.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    86.73 ms /   150 runs   (    0.58 ms per token,  1729.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2444.61 ms /    98 tokens (   24.94 ms per token,    40.09 tokens per second)\n",
      "llama_print_timings:        eval time = 18315.78 ms /   149 runs   (  122.92 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 21226.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    34.37 ms /    60 runs   (    0.57 ms per token,  1745.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7290.01 ms /    60 runs   (  121.50 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  7468.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.56 ms /    20 runs   (    0.58 ms per token,  1729.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2461.70 ms /    20 runs   (  123.09 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  2520.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.54 ms /    20 runs   (    0.58 ms per token,  1733.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2404.92 ms /    20 runs   (  120.25 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:       total time =  2463.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    36.64 ms /    64 runs   (    0.57 ms per token,  1746.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7794.08 ms /    64 runs   (  121.78 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  7986.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.81 ms /    24 runs   (    0.58 ms per token,  1737.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2899.10 ms /    24 runs   (  120.80 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  2970.98 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    83.52 ms /   143 runs   (    0.58 ms per token,  1712.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 17559.19 ms /   143 runs   (  122.79 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 18020.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.58 ms /    25 runs   (    0.58 ms per token,  1714.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3056.89 ms /    25 runs   (  122.28 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3135.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.65 ms /    48 runs   (    0.58 ms per token,  1735.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5889.21 ms /    48 runs   (  122.69 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  6035.07 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   108.08 ms /   185 runs   (    0.58 ms per token,  1711.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 22824.05 ms /   185 runs   (  123.37 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 23408.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Com relação ao colírio de atropina 1%, assinale a alternativa correta. a) É contraindicado para tratamento de ambliopia b) Provoca boa cicloplegia, mas midríase discreta. c) Provoca cicloplegia prolongada, com efeito que se estende por, até, 15 dias após a instilação. d) Sua ação máxima ocorre quatro horas após a instilação.\n",
      "Test #0: \n",
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.59 ms /    28 runs   (    0.59 ms per token,  1687.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1867.99 ms /   118 tokens (   15.83 ms per token,    63.17 tokens per second)\n",
      "llama_print_timings:        eval time =  3306.03 ms /    27 runs   (  122.45 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  5259.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    42.69 ms /    74 runs   (    0.58 ms per token,  1733.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9033.42 ms /    74 runs   (  122.07 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  9260.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.80 ms /    20 runs   (    0.59 ms per token,  1694.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2437.44 ms /    20 runs   (  121.87 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  2496.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     5.20 ms /     9 runs   (    0.58 ms per token,  1729.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1087.01 ms /     9 runs   (  120.78 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  1113.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.56 ms /    53 runs   (    0.58 ms per token,  1734.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6556.74 ms /    53 runs   (  123.71 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  6716.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    75.15 ms /   130 runs   (    0.58 ms per token,  1729.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16068.35 ms /   130 runs   (  123.60 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 16472.93 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #6: \n",
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    41.10 ms /    71 runs   (    0.58 ms per token,  1727.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8715.95 ms /    71 runs   (  122.76 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  8932.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.59 ms /    20 runs   (    0.58 ms per token,  1725.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2447.54 ms /    20 runs   (  122.38 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  2506.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.45 ms /    17 runs   (    0.61 ms per token,  1627.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2132.38 ms /    17 runs   (  125.43 ms per token,     7.97 tokens per second)\n",
      "llama_print_timings:       total time =  2184.51 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n",
      "{'response': 'B'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 64: \n",
      "Language: english\n",
      "Question: \n",
      "A 6-year-old emmetropic child has an exophoria of 6 prismatic diopters (PD). After placement of minus lenses of -2.00 spherical diopters, he presented esophoria of 2 SD. Its accommodative convergence/accommodation ratio is: a) 2. b) 4. c) 6. d) 8.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.15 ms /    28 runs   (    0.58 ms per token,  1733.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3400.44 ms /    28 runs   (  121.44 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  3484.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.43 ms /    31 runs   (    0.59 ms per token,  1682.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2248.97 ms /    98 tokens (   22.95 ms per token,    43.58 tokens per second)\n",
      "llama_print_timings:        eval time =  3661.16 ms /    30 runs   (  122.04 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  6004.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.58 ms /    32 runs   (    0.58 ms per token,  1722.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3895.93 ms /    32 runs   (  121.75 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3992.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.96 ms /    31 runs   (    0.58 ms per token,  1726.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3819.51 ms /    31 runs   (  123.21 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  3912.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.22 ms /    23 runs   (    0.57 ms per token,  1740.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2789.87 ms /    23 runs   (  121.30 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  2858.32 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.86 ms /    31 runs   (    0.58 ms per token,  1736.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3787.91 ms /    31 runs   (  122.19 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3880.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.44 ms /    27 runs   (    0.57 ms per token,  1748.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3297.61 ms /    27 runs   (  122.13 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3378.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n",
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.55 ms /    20 runs   (    0.58 ms per token,  1732.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2514.61 ms /    20 runs   (  125.73 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:       total time =  2574.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.15 ms /    23 runs   (    0.57 ms per token,  1748.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2782.63 ms /    23 runs   (  120.98 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  2850.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    81.52 ms /   141 runs   (    0.58 ms per token,  1729.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 17295.48 ms /   141 runs   (  122.66 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 17733.24 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.87 ms /    31 runs   (    0.58 ms per token,  1734.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3770.02 ms /    31 runs   (  121.61 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3863.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Criança de 6 anos de idade, emétrope, apresenta exoforia de 6 dioptrias prismáticas (DP). Após colocação de lentes negativas de -2,00 dioptrias esféricas, apresentou esoforia de 2 DP. Sua relação convergência acomodativa/acomodação é: a) 2. b) 4. c) 6. d) 8.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    66.03 ms /   114 runs   (    0.58 ms per token,  1726.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2054.13 ms /   112 tokens (   18.34 ms per token,    54.52 tokens per second)\n",
      "llama_print_timings:        eval time = 13859.62 ms /   113 runs   (  122.65 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 16268.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n",
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   120.71 ms /   210 runs   (    0.57 ms per token,  1739.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 25749.31 ms /   210 runs   (  122.62 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time = 26412.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    64.78 ms /   112 runs   (    0.58 ms per token,  1728.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13715.77 ms /   112 runs   (  122.46 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time = 14059.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    70.70 ms /   123 runs   (    0.57 ms per token,  1739.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 15046.90 ms /   123 runs   (  122.33 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time = 15423.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n",
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.13 ms /    28 runs   (    0.58 ms per token,  1735.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3424.71 ms /    28 runs   (  122.31 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3509.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.19 ms /    23 runs   (    0.57 ms per token,  1743.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2826.78 ms /    23 runs   (  122.90 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  2895.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    73.55 ms /   126 runs   (    0.58 ms per token,  1713.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 15440.34 ms /   126 runs   (  122.54 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time = 15832.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.04 ms /    28 runs   (    0.57 ms per token,  1745.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3509.47 ms /    28 runs   (  125.34 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:       total time =  3592.80 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    67.91 ms /   118 runs   (    0.58 ms per token,  1737.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14522.98 ms /   118 runs   (  123.08 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 14885.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    93.43 ms /   162 runs   (    0.58 ms per token,  1733.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 19919.94 ms /   162 runs   (  122.96 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 20424.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 65: \n",
      "Language: english\n",
      "Question: \n",
      "Regarding Fresnel prisms, mark the correct alternative. a) Those with high values ​​are associated with reduced visual acuity. b) Are obtained from prisms of equal power joined by their bases. c) They are obtained from prisms of equal power joined by their apexes. d) They are little used, mainly due to the discomfort caused by their high weight.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.81 ms /    24 runs   (    0.58 ms per token,  1737.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2145.24 ms /    89 tokens (   24.10 ms per token,    41.49 tokens per second)\n",
      "llama_print_timings:        eval time =  2765.18 ms /    23 runs   (  120.23 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:       total time =  4982.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.78 ms /    17 runs   (    0.58 ms per token,  1737.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2083.57 ms /    17 runs   (  122.56 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  2135.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.86 ms /    17 runs   (    0.58 ms per token,  1723.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2081.33 ms /    17 runs   (  122.43 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  2132.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.72 ms /    17 runs   (    0.57 ms per token,  1748.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2077.47 ms /    17 runs   (  122.20 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  2128.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.77 ms /    24 runs   (    0.57 ms per token,  1743.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2928.93 ms /    24 runs   (  122.04 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3000.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.79 ms /    17 runs   (    0.58 ms per token,  1737.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2066.92 ms /    17 runs   (  121.58 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  2118.02 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n",
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.03 ms /    28 runs   (    0.57 ms per token,  1747.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3414.87 ms /    28 runs   (  121.96 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3499.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.42 ms /    27 runs   (    0.57 ms per token,  1750.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3372.02 ms /    27 runs   (  124.89 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time =  3452.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.72 ms /    17 runs   (    0.57 ms per token,  1748.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2048.10 ms /    17 runs   (  120.48 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:       total time =  2098.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.53 ms /     7 runs   (    0.65 ms per token,  1546.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   841.33 ms /     7 runs   (  120.19 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:       total time =   863.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Com relação aos prismas de Fresnel, assinale a alternativa correta. a) Os de altos valores são associados à redução da acuidade visual. b) São obtidos a partir de prismas de igual poder unidos por suas bases. c) São obtidos a partir de prismas de igual poder unidos por seus ápices.d) São pouco usados, principalmente pelo desconforto causado pelo seu alto peso.\n",
      "Test #0: \n",
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.14 ms /     7 runs   (    0.59 ms per token,  1690.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2271.70 ms /   110 tokens (   20.65 ms per token,    48.42 tokens per second)\n",
      "llama_print_timings:        eval time =   734.09 ms /     6 runs   (  122.35 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3026.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.17 ms /    28 runs   (    0.58 ms per token,  1731.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3415.12 ms /    28 runs   (  121.97 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3500.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.66 ms /    24 runs   (    0.57 ms per token,  1756.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2937.17 ms /    24 runs   (  122.38 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3008.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.61 ms /    27 runs   (    0.58 ms per token,  1729.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3301.54 ms /    27 runs   (  122.28 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3381.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.75 ms /    17 runs   (    0.57 ms per token,  1742.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2065.55 ms /    17 runs   (  121.50 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  2116.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.65 ms /    17 runs   (    0.57 ms per token,  1762.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2113.04 ms /    17 runs   (  124.30 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =  2163.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n",
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.24 ms /    25 runs   (    0.57 ms per token,  1755.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3097.51 ms /    25 runs   (  123.90 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  3172.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.17 ms /    28 runs   (    0.58 ms per token,  1731.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3430.33 ms /    28 runs   (  122.51 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3514.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    33.41 ms /    58 runs   (    0.58 ms per token,  1736.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7251.07 ms /    58 runs   (  125.02 ms per token,     8.00 tokens per second)\n",
      "llama_print_timings:       total time =  7425.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.46 ms /    27 runs   (    0.57 ms per token,  1746.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3292.23 ms /    27 runs   (  121.93 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3372.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 66: \n",
      "Language: english\n",
      "Question: \n",
      "A 45-year-old +1.00 spherical diopter hyperopic patient no longer has accommodative tolerance. Therefore, to carry out an activity at 25 cm, which correction below will you need? a) +1.00 spherical diopters. b) +2.00 spherical diopters. c) +4.00 spherical diopters. d) +5.00 spherical diopters.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    93.77 ms /   163 runs   (    0.58 ms per token,  1738.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2186.34 ms /   107 tokens (   20.43 ms per token,    48.94 tokens per second)\n",
      "llama_print_timings:        eval time = 19905.02 ms /   162 runs   (  122.87 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 22605.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.63 ms /    39 runs   (    0.55 ms per token,  1802.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4741.56 ms /    39 runs   (  121.58 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  4858.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.47 ms /    40 runs   (    0.56 ms per token,  1780.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4849.55 ms /    40 runs   (  121.24 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  4971.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.16 ms /    36 runs   (    0.56 ms per token,  1785.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4364.93 ms /    36 runs   (  121.25 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  4474.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.69 ms /    29 runs   (    0.54 ms per token,  1848.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3548.66 ms /    29 runs   (  122.37 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3635.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.02 ms /    39 runs   (    0.56 ms per token,  1771.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4780.32 ms /    39 runs   (  122.57 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  4897.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.39 ms /    38 runs   (    0.56 ms per token,  1776.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4634.18 ms /    38 runs   (  121.95 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  4747.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.91 ms /    31 runs   (    0.55 ms per token,  1833.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3847.61 ms /    31 runs   (  124.12 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  3938.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.52 ms /    39 runs   (    0.58 ms per token,  1731.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4835.22 ms /    39 runs   (  123.98 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  4951.38 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.91 ms /    39 runs   (    0.56 ms per token,  1779.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4775.47 ms /    39 runs   (  122.45 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  4890.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Um paciente hipermétrope de +1,00 dioptrias esfericas de 45 anos não possui mais tolerância acomodativa. Logo, para exercer uma atividade a 25 cm necessitará de qual correção abaixo? a) +1,00 Dioptrias esfericas. b) +2,00 dioptrias esfericas. c) +4,00 dioptrias esfericas. d) +5,00 dioptrias esfericas.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.85 ms /     7 runs   (    0.55 ms per token,  1817.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2552.01 ms /   127 tokens (   20.09 ms per token,    49.76 tokens per second)\n",
      "llama_print_timings:        eval time =   732.15 ms /     6 runs   (  122.03 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3304.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.60 ms /    41 runs   (    0.55 ms per token,  1814.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4985.06 ms /    41 runs   (  121.59 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  5107.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    92.48 ms /   162 runs   (    0.57 ms per token,  1751.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 19914.13 ms /   162 runs   (  122.93 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 20422.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.74 ms /     7 runs   (    0.53 ms per token,  1873.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   867.02 ms /     7 runs   (  123.86 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =   887.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.42 ms /    32 runs   (    0.54 ms per token,  1836.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3899.77 ms /    32 runs   (  121.87 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3995.63 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.67 ms /     7 runs   (    0.52 ms per token,  1908.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   850.36 ms /     7 runs   (  121.48 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =   870.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.30 ms /    35 runs   (    0.55 ms per token,  1813.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4299.96 ms /    35 runs   (  122.86 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  4404.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    73.68 ms /   128 runs   (    0.58 ms per token,  1737.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 15669.32 ms /   128 runs   (  122.42 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time = 16068.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.69 ms /     7 runs   (    0.53 ms per token,  1898.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   847.61 ms /     7 runs   (  121.09 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =   868.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.77 ms /     7 runs   (    0.54 ms per token,  1856.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   872.18 ms /     7 runs   (  124.60 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =   893.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 67: \n",
      "Language: english\n",
      "Question: \n",
      "A six-year-old child with a learning disability has a static refraction of +2.00 spherical diopters in both eyes, achieving a visual acuity of 1.0. No ocular deviation. What is the best course of action in this case? a) It is not necessary to prescribe glasses. b) Full prescription. c) Prescription of +1.00 pherical diopters in both eyes. d) Prescription of reading glasses.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.43 ms /    28 runs   (    0.59 ms per token,  1704.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2171.15 ms /   114 tokens (   19.05 ms per token,    52.51 tokens per second)\n",
      "llama_print_timings:        eval time =  3272.14 ms /    27 runs   (  121.19 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  5528.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n",
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.41 ms /    25 runs   (    0.58 ms per token,  1734.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3119.70 ms /    25 runs   (  124.79 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time =  3194.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    61.99 ms /   107 runs   (    0.58 ms per token,  1726.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13188.34 ms /   107 runs   (  123.26 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 13513.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.37 ms /    25 runs   (    0.57 ms per token,  1740.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3033.66 ms /    25 runs   (  121.35 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3107.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.82 ms /    24 runs   (    0.58 ms per token,  1736.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2911.96 ms /    24 runs   (  121.33 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  2983.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1747.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   868.53 ms /     7 runs   (  124.08 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =   888.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.40 ms /    25 runs   (    0.58 ms per token,  1736.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3085.20 ms /    25 runs   (  123.41 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  3159.92 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n",
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.66 ms /    34 runs   (    0.58 ms per token,  1728.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4154.30 ms /    34 runs   (  122.19 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  4257.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.15 ms /    28 runs   (    0.58 ms per token,  1733.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3478.09 ms /    28 runs   (  124.22 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =  3562.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.27 ms /     7 runs   (    0.61 ms per token,  1637.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   865.94 ms /     7 runs   (  123.71 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =   886.85 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Uma criança de seis anos de idade com dificuldade de aprendizado, apresenta refração estática de +2,00 DE em ambos os olhos, alcançando acuidade visual 1,0. Não apresenta desvio ocular. Qual a melhor conduta neste caso? a) Não é necessária a prescrição de óculos. b) Prescrição total. c) Prescrição de +1,00 dioptrias esfericas em ambos os olhos. d) Prescrição de óculos de leitura.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    69.92 ms /   120 runs   (    0.58 ms per token,  1716.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2216.96 ms /   138 tokens (   16.06 ms per token,    62.25 tokens per second)\n",
      "llama_print_timings:        eval time = 14641.05 ms /   119 runs   (  123.03 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 17236.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.49 ms /    46 runs   (    0.58 ms per token,  1736.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5724.43 ms /    46 runs   (  124.44 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =  5865.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    84.38 ms /   146 runs   (    0.58 ms per token,  1730.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 17983.30 ms /   146 runs   (  123.17 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 18444.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   114.39 ms /   198 runs   (    0.58 ms per token,  1730.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 24342.38 ms /   198 runs   (  122.94 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 24973.24 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   101.77 ms /   176 runs   (    0.58 ms per token,  1729.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 21590.30 ms /   176 runs   (  122.67 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 22151.48 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    29.04 ms /    50 runs   (    0.58 ms per token,  1721.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6120.72 ms /    50 runs   (  122.41 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  6275.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.99 ms /     7 runs   (    0.57 ms per token,  1753.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   852.45 ms /     7 runs   (  121.78 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =   874.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1734.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   867.36 ms /     7 runs   (  123.91 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =   888.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    88.74 ms /   153 runs   (    0.58 ms per token,  1724.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 18838.55 ms /   153 runs   (  123.13 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 19320.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n",
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 68: \n",
      "Language: english\n",
      "Question: \n",
      "What is the anisometropia of the patient who uses -3.00 spherical diopters in the right eye and +1.00 spherical diopters -2.00 cylindrical diopters x 90° in the left eye? a) 0 DE. b) 1 spherical diopters. c) 2 spherical diopters. d) 3 spherical diopters.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.00 ms /     7 runs   (    0.57 ms per token,  1748.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   927.96 ms /     7 runs   (  132.57 ms per token,     7.54 tokens per second)\n",
      "llama_print_timings:       total time =   948.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.27 ms /    32 runs   (    0.57 ms per token,  1751.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2457.70 ms /   101 tokens (   24.33 ms per token,    41.10 tokens per second)\n",
      "llama_print_timings:        eval time =  3749.64 ms /    31 runs   (  120.96 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  6305.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   131.70 ms /   230 runs   (    0.57 ms per token,  1746.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 28290.51 ms /   230 runs   (  123.00 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 29020.26 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   150.65 ms /   266 runs   (    0.57 ms per token,  1765.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 32715.79 ms /   266 runs   (  122.99 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 33560.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n",
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.31 ms /    32 runs   (    0.57 ms per token,  1747.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3959.61 ms /    32 runs   (  123.74 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  4055.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.11 ms /    34 runs   (    0.65 ms per token,  1537.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4121.36 ms /    34 runs   (  121.22 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  4228.63 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   139.01 ms /   242 runs   (    0.57 ms per token,  1740.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 29708.92 ms /   242 runs   (  122.76 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 30484.48 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n",
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.63 ms /    33 runs   (    0.56 ms per token,  1771.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4098.86 ms /    33 runs   (  124.21 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =  4198.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   141.78 ms /   252 runs   (    0.56 ms per token,  1777.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 30929.88 ms /   252 runs   (  122.74 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 31735.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   121.43 ms /   214 runs   (    0.57 ms per token,  1762.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 26425.04 ms /   214 runs   (  123.48 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 27097.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    99.99 ms /   175 runs   (    0.57 ms per token,  1750.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 21500.21 ms /   175 runs   (  122.86 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 22046.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Qual a anisometropia do paciente que usa -3,00 dioptrias esfericas no olho direito e +1,00 dioptrias esfericas -2,00 dioptrias cilindricas x 90° no olho esquerdo? a) 0 DE. b) 1 dioptrias esfericas. c) 2 dioptrias esfericas. d) 3 dioptrias esfericas.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.80 ms /    36 runs   (    0.58 ms per token,  1730.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2556.14 ms /   111 tokens (   23.03 ms per token,    43.42 tokens per second)\n",
      "llama_print_timings:        eval time =  4257.02 ms /    35 runs   (  121.63 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  6922.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.62 ms /    36 runs   (    0.57 ms per token,  1745.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4399.71 ms /    36 runs   (  122.21 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  4508.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     8.93 ms /    16 runs   (    0.56 ms per token,  1791.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1935.96 ms /    16 runs   (  121.00 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  1983.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.52 ms /    34 runs   (    0.57 ms per token,  1741.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4151.80 ms /    34 runs   (  122.11 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  4254.86 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n",
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     8.92 ms /    16 runs   (    0.56 ms per token,  1794.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1988.33 ms /    16 runs   (  124.27 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =  2036.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.29 ms /    34 runs   (    0.57 ms per token,  1762.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4190.29 ms /    34 runs   (  123.24 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  4292.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.16 ms /    35 runs   (    0.58 ms per token,  1735.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4239.55 ms /    35 runs   (  121.13 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  4343.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.59 ms /    17 runs   (    0.56 ms per token,  1772.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2111.94 ms /    17 runs   (  124.23 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =  2162.86 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.57 ms /    41 runs   (    0.57 ms per token,  1739.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5007.07 ms /    41 runs   (  122.12 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  5130.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    20.48 ms /    36 runs   (    0.57 ms per token,  1757.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4370.15 ms /    36 runs   (  121.39 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  4476.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 69: \n",
      "Language: english\n",
      "Question: \n",
      "An examiner located 0.67 m from the examined eye uses a retinoscope and scans the horizontal meridian (vertical axis), observing a movement in favor. After adding plus lenses, the flare is neutralized with +1.50 diopter. With this lens, when sweeping the vertical meridian (horizontal axis), a movement in favor is observed. Which of the following could be a prescription for the patient? a) +1.00 spherical diopter - 1.00 x 90°. b) +1.00 spherical diopter - 1.00 x 180°. c) -1.00 spherical diopter + 1.00 X 90°. d) -1.00 spherical diopter + 1.00 X 180°.\n",
      "Test #0: \n",
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.85 ms /    49 runs   (    0.57 ms per token,  1759.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3386.05 ms /   197 tokens (   17.19 ms per token,    58.18 tokens per second)\n",
      "llama_print_timings:        eval time =  5959.28 ms /    48 runs   (  124.15 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =  9494.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    73.96 ms /   130 runs   (    0.57 ms per token,  1757.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16137.36 ms /   130 runs   (  124.13 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time = 16537.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.44 ms /    49 runs   (    0.56 ms per token,  1786.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6071.70 ms /    49 runs   (  123.91 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  6218.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.31 ms /    49 runs   (    0.56 ms per token,  1794.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6053.10 ms /    49 runs   (  123.53 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  6200.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n",
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.24 ms /    49 runs   (    0.56 ms per token,  1798.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6155.98 ms /    49 runs   (  125.63 ms per token,     7.96 tokens per second)\n",
      "llama_print_timings:       total time =  6303.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.66 ms /    27 runs   (    0.54 ms per token,  1842.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3338.74 ms /    27 runs   (  123.66 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  3419.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    76.94 ms /   135 runs   (    0.57 ms per token,  1754.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16590.71 ms /   135 runs   (  122.89 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 17011.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.75 ms /    48 runs   (    0.56 ms per token,  1794.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5896.19 ms /    48 runs   (  122.84 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  6040.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.92 ms /    50 runs   (    0.56 ms per token,  1791.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6128.01 ms /    50 runs   (  122.56 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  6276.24 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    27.07 ms /    49 runs   (    0.55 ms per token,  1809.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6090.45 ms /    49 runs   (  124.29 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =  6235.67 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Um examinador localizado a 0,67 m do olho examinado utiliza um retinoscópio e varre o meridiano horizontal (eixo vertical), observando um movimento a favor. Após adicionar lentes positivas, o reflexo é neutralizado com +1,50 dioptria. Com esta lente, ao varrer o meridiano vertical (eixo horizontal), observa-se um movimento a favor. Qual das alternativas abaixo pode ser uma prescrição para o paciente? a) +1,00 dioptria esferica - 1,00 x 90°. b) +1,00 dioptria esferica - 1,00 x 180°. c) -1,00 dioptria esferica + 1,00 X 90°. d) -1,00 dioptria esferica + 1,00 X 180°.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.91 ms /     7 runs   (    0.56 ms per token,  1789.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4698.79 ms /   232 tokens (   20.25 ms per token,    49.37 tokens per second)\n",
      "llama_print_timings:        eval time =   736.61 ms /     6 runs   (  122.77 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5456.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.16 ms /    30 runs   (    0.54 ms per token,  1855.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3698.05 ms /    30 runs   (  123.27 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  3789.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #2: \n",
      "Error converting respose to json: {: response: 'a'}\n",
      "Generating new response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.36 ms /     8 runs   (    0.54 ms per token,  1835.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   999.02 ms /     8 runs   (  124.88 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time =  1022.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.73 ms /     7 runs   (    0.53 ms per token,  1878.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   861.24 ms /     7 runs   (  123.03 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =   881.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.67 ms /    31 runs   (    0.54 ms per token,  1860.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3850.70 ms /    31 runs   (  124.22 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =  3945.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n",
      "{'response': 'a'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.92 ms /    30 runs   (    0.53 ms per token,  1884.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3737.95 ms /    30 runs   (  124.60 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =  3829.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.77 ms /     7 runs   (    0.54 ms per token,  1857.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   874.49 ms /     7 runs   (  124.93 ms per token,     8.00 tokens per second)\n",
      "llama_print_timings:       total time =   895.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    77.64 ms /   135 runs   (    0.58 ms per token,  1738.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16694.14 ms /   135 runs   (  123.66 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 17118.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.80 ms /     7 runs   (    0.54 ms per token,  1841.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   866.55 ms /     7 runs   (  123.79 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =   886.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    29.34 ms /    52 runs   (    0.56 ms per token,  1772.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6474.23 ms /    52 runs   (  124.50 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =  6632.26 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.05 ms /    30 runs   (    0.53 ms per token,  1869.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3677.31 ms /    30 runs   (  122.58 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3768.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 70: \n",
      "Language: english\n",
      "Question: \n",
      "A previously emmetropic 40-year-old patient with a history of epilepsy reports sudden onset blurred vision in both eyes. He has myopia of -4.50 spherical diopters in both eyes, reaching normal visual acuity. What is the likely cause? a) Stroke. b) Use of topiramate. c) Ectopia lentis. d) Cataract.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.12 ms /     7 runs   (    0.59 ms per token,  1699.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2254.97 ms /   103 tokens (   21.89 ms per token,    45.68 tokens per second)\n",
      "llama_print_timings:        eval time =   726.21 ms /     6 runs   (  121.03 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  3002.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n",
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.23 ms /    25 runs   (    0.73 ms per token,  1371.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3016.20 ms /    25 runs   (  120.65 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =  3101.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    85.48 ms /   125 runs   (    0.68 ms per token,  1462.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 15276.55 ms /   125 runs   (  122.21 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time = 15705.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1733.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   846.48 ms /     7 runs   (  120.93 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =   866.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    52.68 ms /    84 runs   (    0.63 ms per token,  1594.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10307.37 ms /    84 runs   (  122.71 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 10573.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.47 ms /    24 runs   (    0.60 ms per token,  1658.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2910.03 ms /    24 runs   (  121.25 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  2981.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.97 ms /    24 runs   (    0.58 ms per token,  1717.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2956.92 ms /    24 runs   (  123.20 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  3027.98 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.77 ms /    17 runs   (    0.57 ms per token,  1739.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2042.92 ms /    17 runs   (  120.17 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:       total time =  2092.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.36 ms /    25 runs   (    0.57 ms per token,  1741.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3041.29 ms /    25 runs   (  121.65 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3114.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    40.36 ms /    70 runs   (    0.58 ms per token,  1734.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8546.52 ms /    70 runs   (  122.09 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  8755.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Um paciente previamente emétrope de 40 anos de idade com histórico de epilepsia refere visão embaçada para longe de aparecimento súbito em ambos os olhos. Apresenta miopia de -4,50 dioptrias esfericas em ambos os olhos atingindo acuidade visual normal. Qual a provável causa? a) Acidente vascular cerebral. b) Uso de topiramato. c) Ectopia lentis. d) Catarata.\n",
      "Test #0: \n",
      "{'response': 'd) Catarata.'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    42.48 ms /    73 runs   (    0.58 ms per token,  1718.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2747.83 ms /   133 tokens (   20.66 ms per token,    48.40 tokens per second)\n",
      "llama_print_timings:        eval time =  8771.43 ms /    72 runs   (  121.83 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time = 11740.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.06 ms /     7 runs   (    0.58 ms per token,  1725.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   849.90 ms /     7 runs   (  121.41 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =   870.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    41.64 ms /    72 runs   (    0.58 ms per token,  1729.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8830.83 ms /    72 runs   (  122.65 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  9047.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n",
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     5.23 ms /     9 runs   (    0.58 ms per token,  1721.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1114.61 ms /     9 runs   (  123.85 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  1140.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.05 ms /     7 runs   (    0.58 ms per token,  1727.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   850.48 ms /     7 runs   (  121.50 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =   870.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    82.67 ms /   143 runs   (    0.58 ms per token,  1729.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 17476.85 ms /   143 runs   (  122.22 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time = 17916.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd} Catarata.'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.07 ms /     7 runs   (    0.58 ms per token,  1718.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   836.11 ms /     7 runs   (  119.44 ms per token,     8.37 tokens per second)\n",
      "llama_print_timings:       total time =   856.86 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    97.20 ms /   168 runs   (    0.58 ms per token,  1728.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 20698.62 ms /   168 runs   (  123.21 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 21221.48 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.96 ms /    12 runs   (    0.58 ms per token,  1722.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1494.45 ms /    12 runs   (  124.54 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =  1530.58 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Catarata.'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.26 ms /    16 runs   (    0.58 ms per token,  1727.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1953.23 ms /    16 runs   (  122.08 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  2002.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 71: \n",
      "Language: english\n",
      "Question: \n",
      "Mark the alternative that correctly correlates the columns. I - Compound hypermetropic astigmatism in favor of the rule. II - Compound myopic astigmatism against the rule. III- Astigmatic anisometropia. IV - Antimetropic anisometropia. A - Right eye: -1.00 spherical diopters -1.50 cylindrical diopters x 90° Left eye: -1.00 spherical diopters -1.50 cylindrical diopters x 90°. B - Right eye: +5.00 spherical diopters Left eye: -1.00 spherical diopters. C - right eye: +5.00 spherical diopters -5.00 cylindrical diopters x 90° left eye: +1.00 spherical diopters -1.00 cylindrical diopters x 90°. D - right eye: +5.00 spherical diopters +2.50 cylindrical diopters x 90° left eye: +5.00 spherical diopters +2.50 cylindrical diopters x 90°.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    50.28 ms /    82 runs   (    0.61 ms per token,  1630.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4937.72 ms /   285 tokens (   17.33 ms per token,    57.72 tokens per second)\n",
      "llama_print_timings:        eval time = 10096.24 ms /    81 runs   (  124.64 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time = 15292.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    46.05 ms /    82 runs   (    0.56 ms per token,  1780.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10085.98 ms /    82 runs   (  123.00 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 10334.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    47.24 ms /    82 runs   (    0.58 ms per token,  1735.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10177.94 ms /    82 runs   (  124.12 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time = 10430.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    46.68 ms /    82 runs   (    0.57 ms per token,  1756.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10120.14 ms /    82 runs   (  123.42 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 10366.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    49.66 ms /    82 runs   (    0.61 ms per token,  1651.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10080.22 ms /    82 runs   (  122.93 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 10337.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    51.55 ms /    82 runs   (    0.63 ms per token,  1590.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10175.65 ms /    82 runs   (  124.09 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time = 10440.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    51.27 ms /    82 runs   (    0.63 ms per token,  1599.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10123.23 ms /    82 runs   (  123.45 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 10385.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.84 ms /    27 runs   (    0.55 ms per token,  1819.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3338.35 ms /    27 runs   (  123.64 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  3418.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #8: \n",
      "{'response': 'B'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    46.42 ms /    82 runs   (    0.57 ms per token,  1766.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10108.77 ms /    82 runs   (  123.28 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 10357.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    46.21 ms /    82 runs   (    0.56 ms per token,  1774.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10239.68 ms /    82 runs   (  124.87 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time = 10486.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Assinale a alternativa que correlaciona corretamente as colunas. I - Astigmatismo hipermetrópico composto a favor da regra. II - Astigmatismo miópico composto contra a regra. III- Anisometropia astigmática. IV - Anisometropia antimetrópica. A - Olho direito: -1,00 dioptrias esfericas -1,50 dioptrias cilindricas x 90° Olho esquerdo: -1,00 dioptrias esfericas -1,50 dioptrias cilindricas x 90°. B - Olho direito: +5,00 dioptrias esfericas Olho esquerdo: -1,00 dioptrias esfericas. C - olho direito: +5,00 dioptrias esfericas -5,00 dioptrais cilindricas x 90° olho esquerdo: +1,00 dioptrias esfericas -1,00 dioptrias cilindricas x 90°.  D - olho direito: +5,00 dioptrias esfericas +2,50 dioptrias cilindricas x 90° olho esquerdo: +5,00 dioptrias esfericas +2,50 dioptrias cilindricas x 90°.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.30 ms /     7 runs   (    0.61 ms per token,  1629.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5067.91 ms /   337 tokens (   15.04 ms per token,    66.50 tokens per second)\n",
      "llama_print_timings:        eval time =   731.45 ms /     6 runs   (  121.91 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  5821.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.57 ms /    27 runs   (    0.58 ms per token,  1734.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3327.19 ms /    27 runs   (  123.23 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  3409.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.45 ms /    27 runs   (    0.57 ms per token,  1747.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3353.83 ms /    27 runs   (  124.22 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =  3436.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.00 ms /     7 runs   (    0.57 ms per token,  1751.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   870.20 ms /     7 runs   (  124.31 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =   891.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.02 ms /     7 runs   (    0.57 ms per token,  1740.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   876.58 ms /     7 runs   (  125.23 ms per token,     7.99 tokens per second)\n",
      "llama_print_timings:       total time =   898.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #5: \n",
      "{'response': 'C'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     6.29 ms /    11 runs   (    0.57 ms per token,  1747.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1382.62 ms /    11 runs   (  125.69 ms per token,     7.96 tokens per second)\n",
      "llama_print_timings:       total time =  1415.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.46 ms /    27 runs   (    0.57 ms per token,  1746.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3335.59 ms /    27 runs   (  123.54 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =  3416.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #7: \n",
      "{'response': 'C'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1746.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   871.14 ms /     7 runs   (  124.45 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =   892.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.36 ms /    27 runs   (    0.57 ms per token,  1758.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3354.75 ms /    27 runs   (  124.25 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time =  3436.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.56 ms /    28 runs   (    0.59 ms per token,  1691.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3490.03 ms /    28 runs   (  124.64 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time =  3576.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'C'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 72: \n",
      "Language: english\n",
      "Question: \n",
      "Classify the astigmatisms below according to the central keratometry. Consider insignificant internal astigmatism.\n",
      "\n",
      "I- 44.00 @ 90° x 42.00 @ 180°.\n",
      "II- 44.00 @ 180° x 42.00 @ 90°.\n",
      "III- 44.00 @ 45° x 42.00 @ 135°.\n",
      "\n",
      "A- Astigmatism in favor of the rule.\n",
      "B- Astigmatism against the rule.\n",
      "C- Oblique astigmatism.\n",
      "\n",
      "a) I: A, II: B, III: C.\n",
      "b) I: A, II: C, III: B.\n",
      "c) I: B, II: A, III: C.\n",
      "d) I: B, II: C, III: A.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    51.80 ms /    91 runs   (    0.57 ms per token,  1756.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3790.71 ms /   214 tokens (   17.71 ms per token,    56.45 tokens per second)\n",
      "llama_print_timings:        eval time = 11128.85 ms /    90 runs   (  123.65 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 15202.49 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    85.38 ms /   153 runs   (    0.56 ms per token,  1791.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 19010.41 ms /   153 runs   (  124.25 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:       total time = 19486.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    72.11 ms /   126 runs   (    0.57 ms per token,  1747.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 15500.29 ms /   126 runs   (  123.02 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 15892.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.74 ms /    27 runs   (    0.55 ms per token,  1832.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3287.09 ms /    27 runs   (  121.74 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3367.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    73.73 ms /   130 runs   (    0.57 ms per token,  1763.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16040.89 ms /   130 runs   (  123.39 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 16440.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n",
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    37.27 ms /    66 runs   (    0.56 ms per token,  1771.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8166.25 ms /    66 runs   (  123.73 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  8367.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.54 ms /    20 runs   (    0.53 ms per token,  1897.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2477.22 ms /    20 runs   (  123.86 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  2535.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    50.41 ms /    89 runs   (    0.57 ms per token,  1765.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10934.79 ms /    89 runs   (  122.86 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 11203.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    87.36 ms /   153 runs   (    0.57 ms per token,  1751.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 18909.90 ms /   153 runs   (  123.59 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 19381.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    10.60 ms /    20 runs   (    0.53 ms per token,  1886.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2442.04 ms /    20 runs   (  122.10 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  2500.85 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Classifique os astigmatismos abaixo de acordo com a ceratometria central. Considere o astigmatismo interno insignificante.\n",
      "\n",
      "I- 44,00 @ 90° x 42,00 @ 180°.\n",
      "II- 44,00 @ 180° x 42,00 @ 90°.\n",
      "III- 44,00 @ 45° x 42,00 @ 135°.\n",
      "\n",
      "A- Astigmatismo a favor da regra.\n",
      "B- Astigmatismo contra a regra.\n",
      "C- Astigmatismo oblíquo.\n",
      "\n",
      "a)I: A, II: B, III: C.\n",
      "b)I: A, II: C, III: B.\n",
      "c)I: B, II: A, III: C.\n",
      "d)I: B, II: C, III: A.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    55.35 ms /    97 runs   (    0.57 ms per token,  1752.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3683.07 ms /   228 tokens (   16.15 ms per token,    61.90 tokens per second)\n",
      "llama_print_timings:        eval time = 11787.56 ms /    96 runs   (  122.79 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 15766.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.52 ms /     7 runs   (    0.50 ms per token,  1986.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   876.52 ms /     7 runs   (  125.22 ms per token,     7.99 tokens per second)\n",
      "llama_print_timings:       total time =   897.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.48 ms /     7 runs   (    0.50 ms per token,  2013.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   857.85 ms /     7 runs   (  122.55 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =   878.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.62 ms /     9 runs   (    0.51 ms per token,  1948.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1114.18 ms /     9 runs   (  123.80 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  1140.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    40.88 ms /    73 runs   (    0.56 ms per token,  1785.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9045.96 ms /    73 runs   (  123.92 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  9265.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.72 ms /    27 runs   (    0.55 ms per token,  1833.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3300.69 ms /    27 runs   (  122.25 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3379.63 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    51.90 ms /    92 runs   (    0.56 ms per token,  1772.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11492.31 ms /    92 runs   (  124.92 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time = 11771.03 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.51 ms /     7 runs   (    0.50 ms per token,  1996.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   880.90 ms /     7 runs   (  125.84 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:       total time =   901.24 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #8: \n",
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.50 ms /     7 runs   (    0.50 ms per token,  1997.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   867.87 ms /     7 runs   (  123.98 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =   888.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.55 ms /     7 runs   (    0.51 ms per token,  1973.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   871.60 ms /     7 runs   (  124.51 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =   891.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 73: \n",
      "Language: english\n",
      "Question: \n",
      "A patient moves away from the Greens so as not to fog up his lenses due to the use of a mask during the exam. Which of the following is correct if he stays away from the Greens and the ophthalmologist doesn't notice? a) If he is myopic, there will be undercorrection in his prescription. b) If he is myopic, there will be overcorrection in his prescription. c) If he is nearsighted, there will be no change in his prescription. d) If he is farsighted, he will need greater accommodative effort.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    64.31 ms /   110 runs   (    0.58 ms per token,  1710.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2744.23 ms /   140 tokens (   19.60 ms per token,    51.02 tokens per second)\n",
      "llama_print_timings:        eval time = 13343.75 ms /   109 runs   (  122.42 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time = 16427.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    56.57 ms /    98 runs   (    0.58 ms per token,  1732.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11984.62 ms /    98 runs   (  122.29 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time = 12283.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.00 ms /    31 runs   (    0.58 ms per token,  1722.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3781.17 ms /    31 runs   (  121.97 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3873.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.06 ms /     7 runs   (    0.58 ms per token,  1724.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   868.02 ms /     7 runs   (  124.00 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =   888.60 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n",
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    64.37 ms /   111 runs   (    0.58 ms per token,  1724.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13735.88 ms /   111 runs   (  123.75 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time = 14077.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.88 ms /    24 runs   (    0.58 ms per token,  1729.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2900.55 ms /    24 runs   (  120.86 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  2972.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.28 ms /    44 runs   (    0.57 ms per token,  1740.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5376.72 ms /    44 runs   (  122.20 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  5511.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    61.63 ms /   107 runs   (    0.58 ms per token,  1736.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13096.12 ms /   107 runs   (  122.39 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time = 13425.38 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    87.74 ms /   152 runs   (    0.58 ms per token,  1732.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 18649.18 ms /   152 runs   (  122.69 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 19124.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    43.44 ms /    75 runs   (    0.58 ms per token,  1726.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9180.79 ms /    75 runs   (  122.41 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  9408.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Um paciente se afasta do Greens para não embaçar as lentes devido ao uso de máscara durante o exame. Qual das alternativas abaixo é correta caso ele permaneça afastado do Greens e o oftalmologista não perceba? a) Se ele for míope, haverá hipocorreção em sua prescrição. b) Se ele for míope, haverá hipercorreção em sua prescrição. c) Se ele for míope, não haverá mudança em sua prescrição. d) Se ele for hipermetrope, necessitará de maior esforço acomodativo.\n",
      "Test #0: \n",
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.22 ms /     7 runs   (    0.60 ms per token,  1657.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2921.76 ms /   156 tokens (   18.73 ms per token,    53.39 tokens per second)\n",
      "llama_print_timings:        eval time =   739.29 ms /     6 runs   (  123.22 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  3682.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    66.21 ms /   110 runs   (    0.60 ms per token,  1661.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13485.26 ms /   110 runs   (  122.59 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time = 13833.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.08 ms /     7 runs   (    0.58 ms per token,  1716.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   849.42 ms /     7 runs   (  121.35 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =   870.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    49.15 ms /    85 runs   (    0.58 ms per token,  1729.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10443.75 ms /    85 runs   (  122.87 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 10701.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n",
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1736.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   871.17 ms /     7 runs   (  124.45 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time =   891.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.97 ms /     7 runs   (    0.57 ms per token,  1762.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   845.68 ms /     7 runs   (  120.81 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =   866.48 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1737.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   859.22 ms /     7 runs   (  122.75 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =   879.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.98 ms /    26 runs   (    0.58 ms per token,  1735.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3158.04 ms /    26 runs   (  121.46 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  3235.45 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.00 ms /     7 runs   (    0.57 ms per token,  1748.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   845.09 ms /     7 runs   (  120.73 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =   865.24 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.06 ms /     7 runs   (    0.58 ms per token,  1722.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   851.15 ms /     7 runs   (  121.59 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =   871.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 74: \n",
      "Language: english\n",
      "Question: \n",
      "What is the function of the \"P\" setting on the Greens refractor? a) Increase positive spherical power to test near refraction. b) Increase the negative spherical power to discount the working distance in skiascopy. c) Insert a 6 DP prism and allow the evaluation of the refractometric balance. d) Separate the images and allow evaluation of refractometric balance and stereopsis.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.76 ms /    27 runs   (    0.58 ms per token,  1713.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2820.43 ms /   100 tokens (   28.20 ms per token,    35.46 tokens per second)\n",
      "llama_print_timings:        eval time =  3187.01 ms /    26 runs   (  122.58 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  6089.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n",
      "{'response': 'd)'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.47 ms /    46 runs   (    0.58 ms per token,  1737.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5728.98 ms /    46 runs   (  124.54 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =  5868.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    42.01 ms /    73 runs   (    0.58 ms per token,  1737.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8929.04 ms /    73 runs   (  122.32 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  9149.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    48.75 ms /    85 runs   (    0.57 ms per token,  1743.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10321.30 ms /    85 runs   (  121.43 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time = 10579.53 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.23 ms /    28 runs   (    0.58 ms per token,  1724.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3435.11 ms /    28 runs   (  122.68 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  3518.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    37.00 ms /    64 runs   (    0.58 ms per token,  1729.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7823.95 ms /    64 runs   (  122.25 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  8022.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.29 ms /    44 runs   (    0.57 ms per token,  1740.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5343.10 ms /    44 runs   (  121.43 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  5478.75 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.01 ms /    27 runs   (    0.59 ms per token,  1686.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3296.81 ms /    27 runs   (  122.10 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  3380.61 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'D'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.58 ms /    27 runs   (    0.58 ms per token,  1732.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3301.48 ms /    27 runs   (  122.28 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3386.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.08 ms /    28 runs   (    0.57 ms per token,  1740.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3393.46 ms /    28 runs   (  121.19 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =  3481.12 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Qual a função do ajuste \"P\" no refrator de Greens? a) Aumentar o poder esférico positivo para testar a refração para perto. b) Aumentar o poder esférico negativo para descontar a distância de trabalho na esquiascopia. c) Inserir um prisma de 6 DP e possibilitar a avaliação do balanço refratométrico. d) Separar as imagens e possibilitar avaliação do balanço refratométrico e da estereopsia.\n",
      "Test #0: \n",
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    33.68 ms /    57 runs   (    0.59 ms per token,  1692.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2784.45 ms /   140 tokens (   19.89 ms per token,    50.28 tokens per second)\n",
      "llama_print_timings:        eval time =  6879.24 ms /    56 runs   (  122.84 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  9838.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1732.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   844.82 ms /     7 runs   (  120.69 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =   865.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n",
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.54 ms /    27 runs   (    0.58 ms per token,  1738.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3300.44 ms /    27 runs   (  122.24 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3380.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1745.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   916.33 ms /     7 runs   (  130.90 ms per token,     7.64 tokens per second)\n",
      "llama_print_timings:       total time =   936.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    46.63 ms /    80 runs   (    0.58 ms per token,  1715.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9841.66 ms /    80 runs   (  123.02 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 10085.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.05 ms /     7 runs   (    0.58 ms per token,  1727.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   846.24 ms /     7 runs   (  120.89 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =   866.92 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    51.33 ms /    87 runs   (    0.59 ms per token,  1694.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10735.57 ms /    87 runs   (  123.40 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time = 11014.26 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #7: \n",
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    53.14 ms /    92 runs   (    0.58 ms per token,  1731.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11337.62 ms /    92 runs   (  123.24 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 11618.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.65 ms /    27 runs   (    0.58 ms per token,  1724.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3290.22 ms /    27 runs   (  121.86 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3371.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    29.94 ms /    52 runs   (    0.58 ms per token,  1736.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6336.92 ms /    52 runs   (  121.86 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  6493.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 75: \n",
      "Language: english\n",
      "Question: \n",
      "An undercorrected myopic person often tilts his glasses to see better. What effect does this maneuver have on your corrective lenses? a) Increased optical divergence and positive cylinder induction on the tilt axis. b) Increased optical convergence and negative cylinder induction on the tilt axis. c) Increased optical divergence and negative cylinder induction on the tilt axis. d) Increased optical convergence and positive cylinder induction on the tilt axis.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    75.83 ms /   128 runs   (    0.59 ms per token,  1687.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2478.21 ms /   113 tokens (   21.93 ms per token,    45.60 tokens per second)\n",
      "llama_print_timings:        eval time = 15498.57 ms /   127 runs   (  122.04 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time = 18378.67 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n",
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.49 ms /    45 runs   (    0.57 ms per token,  1765.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5480.99 ms /    45 runs   (  121.80 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  5614.54 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.26 ms /    27 runs   (    0.57 ms per token,  1769.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3266.27 ms /    27 runs   (  120.97 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  3345.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    84.95 ms /   143 runs   (    0.59 ms per token,  1683.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 17490.43 ms /   143 runs   (  122.31 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time = 17942.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    64.16 ms /   112 runs   (    0.57 ms per token,  1745.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13727.72 ms /   112 runs   (  122.57 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time = 14073.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.20 ms /    27 runs   (    0.56 ms per token,  1776.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3279.05 ms /    27 runs   (  121.45 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  3358.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n",
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.69 ms /    28 runs   (    0.56 ms per token,  1785.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3446.01 ms /    28 runs   (  123.07 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  3529.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    58.18 ms /   102 runs   (    0.57 ms per token,  1753.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12494.38 ms /   102 runs   (  122.49 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time = 12805.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    66.03 ms /   115 runs   (    0.57 ms per token,  1741.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14157.95 ms /   115 runs   (  123.11 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 14516.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    45.49 ms /    79 runs   (    0.58 ms per token,  1736.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9828.67 ms /    79 runs   (  124.41 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:       total time = 10071.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Um míope hipocorrigido frequentemente inclina seus óculos para enxergar melhor. Qual o efeito desta manobra em suas lentes corretoras? a) Aumento da divergência óptica e indução de cilindro positivo no eixo da inclinação. b) Aumento da convergência óptica e indução de cilindro negativo no eixo da inclinação. c) Aumento da divergência óptica e indução de cilindro negativo no eixo da inclinação. d) Aumento da convergência óptica e indução de cilindro positivo no eixo da inclinação.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    46.34 ms /    80 runs   (    0.58 ms per token,  1726.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4015.87 ms /   172 tokens (   23.35 ms per token,    42.83 tokens per second)\n",
      "llama_print_timings:        eval time =  9759.86 ms /    79 runs   (  123.54 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 14019.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.99 ms /     7 runs   (    0.57 ms per token,  1753.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   849.70 ms /     7 runs   (  121.39 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =   870.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    46.80 ms /    72 runs   (    0.65 ms per token,  1538.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  8936.38 ms /    72 runs   (  124.12 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  9436.38 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   101.75 ms /   159 runs   (    0.64 ms per token,  1562.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 19661.06 ms /   159 runs   (  123.65 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 20696.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1746.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   872.51 ms /     7 runs   (  124.64 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time =   893.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.00 ms /     7 runs   (    0.57 ms per token,  1748.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   852.65 ms /     7 runs   (  121.81 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =   872.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.97 ms /     7 runs   (    0.57 ms per token,  1764.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   842.24 ms /     7 runs   (  120.32 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:       total time =   862.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n",
      "{'response': 'A'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    43.90 ms /    76 runs   (    0.58 ms per token,  1731.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9418.75 ms /    76 runs   (  123.93 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  9650.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.00 ms /     7 runs   (    0.57 ms per token,  1751.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   855.66 ms /     7 runs   (  122.24 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =   876.58 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.94 ms /     7 runs   (    0.56 ms per token,  1774.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   857.55 ms /     7 runs   (  122.51 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =   877.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 76: \n",
      "Language: english\n",
      "Question: \n",
      "During the refraction outpatient clinic, the first-year resident instilled a cycloplegic eye drop in the patients, but lost the bottle. His preceptor decided to measure the accommodation effect to find out which drops were used and noticed that the patients had good mydriasis and had a maximum reduction in accommodation 20 to 30 minutes after instillation, with a fleeting effect. What eye drops did the resident instill? a) Atropine. b) Cyclopentolate. c) Phenylephrine. d) Tropicamide.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.77 ms /    53 runs   (    0.58 ms per token,  1722.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3192.84 ms /   136 tokens (   23.48 ms per token,    42.60 tokens per second)\n",
      "llama_print_timings:        eval time =  6434.45 ms /    52 runs   (  123.74 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =  9790.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.01 ms /     7 runs   (    0.57 ms per token,  1746.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   842.01 ms /     7 runs   (  120.29 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:       total time =   862.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.13 ms /    52 runs   (    0.58 ms per token,  1726.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6399.82 ms /    52 runs   (  123.07 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  6558.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     5.16 ms /     9 runs   (    0.57 ms per token,  1744.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1093.06 ms /     9 runs   (  121.45 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  1120.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.85 ms /    32 runs   (    0.59 ms per token,  1697.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3923.86 ms /    32 runs   (  122.62 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  4022.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.98 ms /     7 runs   (    0.57 ms per token,  1756.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   883.04 ms /     7 runs   (  126.15 ms per token,     7.93 tokens per second)\n",
      "llama_print_timings:       total time =   904.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n",
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.93 ms /     7 runs   (    0.56 ms per token,  1779.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   857.93 ms /     7 runs   (  122.56 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =   879.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    29.86 ms /    52 runs   (    0.57 ms per token,  1741.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6371.88 ms /    52 runs   (  122.54 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  6531.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Tropicamide'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.20 ms /    23 runs   (    0.57 ms per token,  1742.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2831.69 ms /    23 runs   (  123.12 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  2900.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd} Tropicamide.'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.96 ms /     7 runs   (    0.57 ms per token,  1765.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   851.44 ms /     7 runs   (  121.63 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =   872.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Durante o ambulatório de refração o residente do primeiro ano instilou um colírio cicloplégico nos pacientes, mas perdeu o frasco. Seu preceptor resolveu medir o efeito de acomodação para descobrir qual colírio foi usado e notou que os pacientes estavam com boa midríase e tiveram redução máxima na acomodação 20 a 30 minutos após a instilação, com efeito fugaz. Qual colírio o residente instilou? a) Atropina. b) Ciclopentolato. c) Fenilefrina. d) Tropicamida.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.13 ms /     7 runs   (    0.59 ms per token,  1694.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3556.32 ms /   167 tokens (   21.30 ms per token,    46.96 tokens per second)\n",
      "llama_print_timings:        eval time =   732.07 ms /     6 runs   (  122.01 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  4310.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.12 ms /     7 runs   (    0.59 ms per token,  1697.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   854.91 ms /     7 runs   (  122.13 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =   877.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.05 ms /     7 runs   (    0.58 ms per token,  1728.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   854.49 ms /     7 runs   (  122.07 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =   875.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.07 ms /     7 runs   (    0.58 ms per token,  1719.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   864.07 ms /     7 runs   (  123.44 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =   885.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.03 ms /     7 runs   (    0.58 ms per token,  1735.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   858.63 ms /     7 runs   (  122.66 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =   879.58 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.06 ms /     7 runs   (    0.58 ms per token,  1724.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   850.81 ms /     7 runs   (  121.54 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =   871.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n",
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.00 ms /     7 runs   (    0.57 ms per token,  1748.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   849.33 ms /     7 runs   (  121.33 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =   870.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1733.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   866.21 ms /     7 runs   (  123.74 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =   887.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.09 ms /     7 runs   (    0.58 ms per token,  1710.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   846.76 ms /     7 runs   (  120.97 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =   868.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.07 ms /     7 runs   (    0.58 ms per token,  1718.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   854.34 ms /     7 runs   (  122.05 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =   875.93 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 77: \n",
      "Language: english\n",
      "Question: \n",
      "During a patient's skiascopy, which of the following characteristics indicates that the examiner is closest to the point of neutrality? a) Fast beam speed, high brightness, wide beam. b) Fast beam speed, low brightness, narrow beam. c) Slow beam speed, high brightness, wide beam. d) Slow beam speed, low brightness, narrow beam.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.62 ms /    34 runs   (    0.58 ms per token,  1733.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1655.95 ms /    88 tokens (   18.82 ms per token,    53.14 tokens per second)\n",
      "llama_print_timings:        eval time =  4029.29 ms /    33 runs   (  122.10 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  5789.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    23.82 ms /    42 runs   (    0.57 ms per token,  1763.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5153.27 ms /    42 runs   (  122.70 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5280.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    21.94 ms /    39 runs   (    0.56 ms per token,  1777.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4746.72 ms /    39 runs   (  121.71 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  4864.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.50 ms /    28 runs   (    0.55 ms per token,  1807.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3398.72 ms /    28 runs   (  121.38 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3482.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.02 ms /    34 runs   (    0.56 ms per token,  1788.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4148.78 ms /    34 runs   (  122.02 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  4250.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.45 ms /    40 runs   (    0.56 ms per token,  1782.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4928.89 ms /    40 runs   (  123.22 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  5049.95 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.04 ms /    32 runs   (    0.56 ms per token,  1773.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3895.02 ms /    32 runs   (  121.72 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  3991.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.44 ms /    40 runs   (    0.56 ms per token,  1782.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4908.55 ms /    40 runs   (  122.71 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5028.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.86 ms /     7 runs   (    0.55 ms per token,  1812.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   861.97 ms /     7 runs   (  123.14 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =   882.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.84 ms /    41 runs   (    0.56 ms per token,  1794.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5058.09 ms /    41 runs   (  123.37 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  5181.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Durante a esquiascopia de um paciente, qual das características abaixo indica que o examinador está mais próximo do ponto de neutralidade? a) Velocidade do feixe rápida, brilho alto, feixe largo. b) Velocidade do feixe rápida, brilho baixo, feixe estreito. c) Velocidade do feixe lenta, brilho alto, feixe largo. d) Velocidade do feixe lenta, brilho baixo, feixe estreito.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.11 ms /     7 runs   (    0.59 ms per token,  1703.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3228.10 ms /   140 tokens (   23.06 ms per token,    43.37 tokens per second)\n",
      "llama_print_timings:        eval time =   734.12 ms /     6 runs   (  122.35 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3983.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.98 ms /     7 runs   (    0.57 ms per token,  1758.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   850.00 ms /     7 runs   (  121.43 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =   871.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.31 ms /    20 runs   (    0.57 ms per token,  1768.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2448.23 ms /    20 runs   (  122.41 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  2509.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.93 ms /     7 runs   (    0.56 ms per token,  1782.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   843.96 ms /     7 runs   (  120.57 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =   864.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    36.05 ms /    63 runs   (    0.57 ms per token,  1747.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7706.99 ms /    63 runs   (  122.33 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  7899.86 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    29.66 ms /    52 runs   (    0.57 ms per token,  1753.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6397.77 ms /    52 runs   (  123.03 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  6554.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.85 ms /     7 runs   (    0.55 ms per token,  1819.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   880.81 ms /     7 runs   (  125.83 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:       total time =   901.12 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.85 ms /     7 runs   (    0.55 ms per token,  1818.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   868.41 ms /     7 runs   (  124.06 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =   888.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.76 ms /    21 runs   (    0.56 ms per token,  1785.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2631.41 ms /    21 runs   (  125.31 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:       total time =  2693.99 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.08 ms /    23 runs   (    0.57 ms per token,  1758.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2785.65 ms /    23 runs   (  121.12 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:       total time =  2854.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 78: \n",
      "Language: english\n",
      "Question: \n",
      "On examination of a phakic patient, the closest point of accommodation was found in the right eye at 30 cm and in the left eye at 50 cm. In this case, it can be stated: a) The left eye must be more myopic than the right eye. b) More likely there is a need to adjust the patient's refraction. c) It is a common condition, as accommodation is rarely similar in both eyes. d) A disease that moves the retina forward, such as central serous chorioretinopathy, is the cause of this shorter distance in the right eye.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.89 ms /    25 runs   (    0.60 ms per token,  1679.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2952.49 ms /   141 tokens (   20.94 ms per token,    47.76 tokens per second)\n",
      "llama_print_timings:        eval time =  2950.35 ms /    24 runs   (  122.93 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  5978.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'B'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.34 ms /    28 runs   (    0.58 ms per token,  1713.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3401.18 ms /    28 runs   (  121.47 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  3485.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.56 ms /    25 runs   (    0.58 ms per token,  1717.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3061.79 ms /    25 runs   (  122.47 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3136.88 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n",
      "{'response': 'B'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.65 ms /    20 runs   (    0.58 ms per token,  1717.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2440.63 ms /    20 runs   (  122.03 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  2501.57 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     9.84 ms /    17 runs   (    0.58 ms per token,  1728.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2068.35 ms /    17 runs   (  121.67 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =  2118.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.00 ms /     7 runs   (    0.57 ms per token,  1748.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   845.24 ms /     7 runs   (  120.75 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =   865.57 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.02 ms /     7 runs   (    0.57 ms per token,  1740.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   855.01 ms /     7 runs   (  122.14 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =   875.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    43.88 ms /    74 runs   (    0.59 ms per token,  1686.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9259.07 ms /    74 runs   (  125.12 ms per token,     7.99 tokens per second)\n",
      "llama_print_timings:       total time =  9486.80 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    50.12 ms /    87 runs   (    0.58 ms per token,  1735.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 10604.68 ms /    87 runs   (  121.89 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time = 10868.66 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.42 ms /    25 runs   (    0.58 ms per token,  1733.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3086.75 ms /    25 runs   (  123.47 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  3161.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "No exame de um paciente fácico, foi encontrado o ponto próximo de acomodação no olho direito a 30 cm e no olho esquerdo a 50 cm. Neste caso, pode-se afirmar: a) O olho esquerdo deve ser mais míope que o olho direito. b) Mais provavelmente há necessidade de se ajustar a refração do paciente. c) É uma condição comum, já que a acomodação é raramente semelhante nos dois olhos. d) Uma doença que desloque a retina para frente, como a coriorretinopatia central serosa, é causa dessa menor distância no olho direito.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    66.92 ms /   115 runs   (    0.58 ms per token,  1718.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2765.37 ms /   177 tokens (   15.62 ms per token,    64.01 tokens per second)\n",
      "llama_print_timings:        eval time = 14130.72 ms /   114 runs   (  123.95 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time = 17252.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n",
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    54.26 ms /    94 runs   (    0.58 ms per token,  1732.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 11543.52 ms /    94 runs   (  122.80 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 11827.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.56 ms /    27 runs   (    0.58 ms per token,  1735.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3316.91 ms /    27 runs   (  122.85 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  3396.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    64.80 ms /   111 runs   (    0.58 ms per token,  1712.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13732.81 ms /   111 runs   (  123.72 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time = 14072.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.70 ms /    27 runs   (    0.58 ms per token,  1720.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3303.12 ms /    27 runs   (  122.34 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3384.05 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #5: \n",
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.15 ms /    28 runs   (    0.58 ms per token,  1733.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3433.22 ms /    28 runs   (  122.62 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  3516.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    65.32 ms /   113 runs   (    0.58 ms per token,  1729.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 13888.60 ms /   113 runs   (  122.91 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time = 14233.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n",
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.09 ms /     7 runs   (    0.58 ms per token,  1710.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   852.24 ms /     7 runs   (  121.75 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =   873.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    11.50 ms /    20 runs   (    0.58 ms per token,  1738.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2502.25 ms /    20 runs   (  125.11 ms per token,     7.99 tokens per second)\n",
      "llama_print_timings:       total time =  2561.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.09 ms /     7 runs   (    0.58 ms per token,  1710.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   854.24 ms /     7 runs   (  122.03 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =   874.75 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 79: \n",
      "Language: english\n",
      "Question: \n",
      "If the patient's refraction is +1.50 spherical diopters and -0.50 cylindrical diopters x 180° and his keratometry is 42.00 diopters x 180° and 44.00 diopters x 90°, we can say that the internal astigmatism is : a) +1.50 cylindrical diopters x 90°. b) +2.50 cylindrical diopters x 180°. c) -1.50 cylindrical diopters x 90°. d) -2.50 cylindrical diopters x 180°.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.77 ms /    46 runs   (    0.56 ms per token,  1784.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3167.71 ms /   169 tokens (   18.74 ms per token,    53.35 tokens per second)\n",
      "llama_print_timings:        eval time =  5535.88 ms /    45 runs   (  123.02 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  8841.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    24.41 ms /    44 runs   (    0.55 ms per token,  1802.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5372.01 ms /    44 runs   (  122.09 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  5502.38 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.10 ms /    45 runs   (    0.56 ms per token,  1793.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5522.88 ms /    45 runs   (  122.73 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5656.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.56 ms /    46 runs   (    0.56 ms per token,  1799.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5646.66 ms /    46 runs   (  122.75 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  5782.93 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.87 ms /    46 runs   (    0.56 ms per token,  1778.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5704.64 ms /    46 runs   (  124.01 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =  5841.80 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.17 ms /    46 runs   (    0.57 ms per token,  1757.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5599.77 ms /    46 runs   (  121.73 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  5737.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    24.64 ms /    44 runs   (    0.56 ms per token,  1786.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5417.28 ms /    44 runs   (  123.12 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  5548.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.86 ms /    45 runs   (    0.57 ms per token,  1739.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5534.30 ms /    45 runs   (  122.98 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time =  5671.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.64 ms /    28 runs   (    0.56 ms per token,  1790.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3422.89 ms /    28 runs   (  122.25 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3506.98 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n",
      "{'response': 'b'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Se a refração do paciente é +1,50 dioptrias esfericas e -0,50 dioptrias cilindricas x 180° e sua ceratometria é 42,00 Dioptrias x 180° e 44,00 Dioptrias x 90°, podemos afirmar que o astigmatismo interno é: a) +1,50 dioptrias cilindricas x 90°. b) +2,50 dioptrias cilindricas x 180°. c) -1,50 dioptrias cilindricas x 90°. d) -2,50 dioptrias cilindricas x 180°.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    26.00 ms /    46 runs   (    0.57 ms per token,  1769.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5630.20 ms /    46 runs   (  122.40 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  5769.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     5.18 ms /     9 runs   (    0.58 ms per token,  1737.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3068.61 ms /   187 tokens (   16.41 ms per token,    60.94 tokens per second)\n",
      "llama_print_timings:        eval time =  1006.87 ms /     8 runs   (  125.86 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:       total time =  4103.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    14.72 ms /    26 runs   (    0.57 ms per token,  1766.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3206.22 ms /    26 runs   (  123.32 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  3284.68 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    30.89 ms /    51 runs   (    0.61 ms per token,  1651.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6256.57 ms /    51 runs   (  122.68 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =  6414.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     3.73 ms /     7 runs   (    0.53 ms per token,  1876.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   856.11 ms /     7 runs   (  122.30 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =   876.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    96.10 ms /   157 runs   (    0.61 ms per token,  1633.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 19348.19 ms /   157 runs   (  123.24 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 19853.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    29.59 ms /    47 runs   (    0.63 ms per token,  1588.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5804.39 ms /    47 runs   (  123.50 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  5949.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.89 ms /    26 runs   (    0.61 ms per token,  1635.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3168.63 ms /    26 runs   (  121.87 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  3253.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n",
      "{'response': 'a'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.87 ms /     9 runs   (    0.54 ms per token,  1848.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1101.10 ms /     9 runs   (  122.34 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  1127.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.22 ms /    28 runs   (    0.58 ms per token,  1726.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3414.19 ms /    28 runs   (  121.94 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  3502.98 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    19.88 ms /    26 runs   (    0.76 ms per token,  1307.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3206.02 ms /    26 runs   (  123.31 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =  3295.60 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 80: \n",
      "Language: english\n",
      "Question: \n",
      "A pseudophakic patient with a refraction of +1.00 spherical diopters and -3.00 cylindrical diopters x 180° is likely to notice less blurry vision at: a) 0.5 m. b) 1 m. c) 2 m. d) 4 m.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    89.08 ms /   150 runs   (    0.59 ms per token,  1683.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1864.02 ms /    82 tokens (   22.73 ms per token,    43.99 tokens per second)\n",
      "llama_print_timings:        eval time = 18222.07 ms /   149 runs   (  122.30 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time = 20568.24 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #1: \n",
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.36 ms /    32 runs   (    0.57 ms per token,  1743.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3867.14 ms /    32 runs   (  120.85 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =  3962.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.07 ms /    28 runs   (    0.57 ms per token,  1742.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3422.12 ms /    28 runs   (  122.22 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3505.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A'}\n",
      "Test #3: \n",
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    93.11 ms /   162 runs   (    0.57 ms per token,  1739.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 19761.38 ms /   162 runs   (  121.98 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time = 20262.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.39 ms /    27 runs   (    0.57 ms per token,  1753.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3276.97 ms /    27 runs   (  121.37 ms per token,     8.24 tokens per second)\n",
      "llama_print_timings:       total time =  3355.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n",
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   105.66 ms /   180 runs   (    0.59 ms per token,  1703.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 22098.73 ms /   180 runs   (  122.77 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 22664.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    72.43 ms /   126 runs   (    0.57 ms per token,  1739.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 15451.41 ms /   126 runs   (  122.63 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 15840.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    74.86 ms /   129 runs   (    0.58 ms per token,  1723.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 15861.68 ms /   129 runs   (  122.96 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 16262.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    91.31 ms /   159 runs   (    0.57 ms per token,  1741.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 19454.09 ms /   159 runs   (  122.35 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time = 19948.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    85.19 ms /   145 runs   (    0.59 ms per token,  1702.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 17869.03 ms /   145 runs   (  123.23 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time = 18320.17 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Um paciente pseudofácico com refração de +1,00 dioptrias esfericas e -3,00 dioptrias cilindricas x 180° provavelmente notará um visão menos borrada a: a) 0,5 m. b) 1 m. c) 2 m. d) 4 m.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =   172.68 ms /   278 runs   (    0.62 ms per token,  1609.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2180.92 ms /    89 tokens (   24.50 ms per token,    40.81 tokens per second)\n",
      "llama_print_timings:        eval time = 34085.51 ms /   277 runs   (  123.05 ms per token,     8.13 tokens per second)\n",
      "llama_print_timings:       total time = 37190.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    85.12 ms /   132 runs   (    0.64 ms per token,  1550.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 16163.91 ms /   132 runs   (  122.45 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time = 16606.95 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    74.89 ms /   130 runs   (    0.58 ms per token,  1735.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 15871.68 ms /   130 runs   (  122.09 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time = 16268.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.75 ms /    33 runs   (    0.57 ms per token,  1759.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4028.94 ms /    33 runs   (  122.09 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =  4126.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.53 ms /    27 runs   (    0.58 ms per token,  1738.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3259.56 ms /    27 runs   (  120.72 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =  3340.01 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #5: \n",
      "{'response': 'b'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    59.30 ms /   103 runs   (    0.58 ms per token,  1737.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12566.05 ms /   103 runs   (  122.00 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time = 12882.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    57.44 ms /   100 runs   (    0.57 ms per token,  1741.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12193.37 ms /   100 runs   (  121.93 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time = 12499.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    59.17 ms /   103 runs   (    0.57 ms per token,  1740.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12614.15 ms /   103 runs   (  122.47 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time = 12927.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    60.82 ms /   106 runs   (    0.57 ms per token,  1742.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 12879.12 ms /   106 runs   (  121.50 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time = 13201.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n",
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 81: \n",
      "Language: english\n",
      "Question: \n",
      "Patient returns complaining of diplopia after making eyeglasses. Has anisometropia, but previously wore glasses without problems. There was no change in the prescription. Optical centers are properly mounted. The most likely explanation is: a) Change in the base curve of one of the lenses. b) Change in frame format. c) Disappearance of the central suppression scotoma. d) Loss of cerebral compensation capacity for the difference in aniseikonia.\n",
      "\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    22.56 ms /    39 runs   (    0.58 ms per token,  1728.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4831.83 ms /    39 runs   (  123.89 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  4948.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    18.03 ms /    31 runs   (    0.58 ms per token,  1719.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2036.32 ms /   118 tokens (   17.26 ms per token,    57.95 tokens per second)\n",
      "llama_print_timings:        eval time =  3703.91 ms /    30 runs   (  123.46 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =  5834.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    13.85 ms /    24 runs   (    0.58 ms per token,  1732.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2927.31 ms /    24 runs   (  121.97 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  2998.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.67 ms /    45 runs   (    0.57 ms per token,  1752.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5484.03 ms /    45 runs   (  121.87 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  5618.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd} Loss of cerebral compensation capacity for the difference in aniseikonia.'}\n",
      "Test #3: \n",
      "{'response': 'd) Loss of cerebral compensation capacity for the difference in aniseikonia.'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    15.66 ms /    27 runs   (    0.58 ms per token,  1724.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3300.35 ms /    27 runs   (  122.24 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  3381.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.86 ms /    45 runs   (    0.57 ms per token,  1740.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5467.26 ms /    45 runs   (  121.49 ms per token,     8.23 tokens per second)\n",
      "llama_print_timings:       total time =  5602.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.92 ms /    45 runs   (    0.58 ms per token,  1736.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5508.96 ms /    45 runs   (  122.42 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  5642.95 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Loss of cerebral compensation capacity for the difference in aniseikonia.'}\n",
      "Test #6: \n",
      "{'response': 'b'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    66.57 ms /   115 runs   (    0.58 ms per token,  1727.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14170.28 ms /   115 runs   (  123.22 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time = 14525.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.83 ms /    45 runs   (    0.57 ms per token,  1742.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5488.75 ms /    45 runs   (  121.97 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =  5624.15 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd} Loss of cerebral compensation capacity for the difference in aniseikonia.'}\n",
      "Test #8: \n",
      "{'response': 'd) Loss of cerebral compensation capacity for the difference in aniseikonia.'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.88 ms /    45 runs   (    0.57 ms per token,  1739.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5525.05 ms /    45 runs   (  122.78 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:       total time =  5662.42 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    25.39 ms /    44 runs   (    0.58 ms per token,  1732.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5449.56 ms /    44 runs   (  123.85 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =  5582.82 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Loss of cerebral compensation capacity for the difference in aniseikonia.'}\n",
      "Language: portuguese\n",
      "Question: \n",
      "Paciente retorna com queixa de diplopia após confeccionar óculos. Tem anisometropia, mas usava óculos anteriormente sem problemas. Não houve modificação na prescrição. Os centros ópticos estão montados adequadamente. A explicação mais provável é: a) Mudança na curva base de uma das lentes. b) Mudança no formato da armação. c) Desaparecimento do escotoma central de supressão. d) Perda da capacidade de compensação cerebral da diferença da aniseiconia.\n",
      "Test #0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    17.08 ms /    29 runs   (    0.59 ms per token,  1697.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2830.97 ms /   146 tokens (   19.39 ms per token,    51.57 tokens per second)\n",
      "llama_print_timings:        eval time =  3429.80 ms /    28 runs   (  122.49 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =  6349.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd} Perda da capacidade de compensação cerebral da diferença da aniseiconia.'}\n",
      "Test #1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.09 ms /     7 runs   (    0.58 ms per token,  1711.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   857.44 ms /     7 runs   (  122.49 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =   879.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    16.68 ms /    29 runs   (    0.58 ms per token,  1739.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3548.24 ms /    29 runs   (  122.35 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  3636.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'd) Perda da capacidade de compensação cerebral da diferença da aniseiconia.'}\n",
      "Test #3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1734.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   858.42 ms /     7 runs   (  122.63 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time =   879.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.09 ms /     7 runs   (    0.58 ms per token,  1712.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   862.56 ms /     7 runs   (  123.22 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =   883.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1734.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   844.30 ms /     7 runs   (  120.61 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =   865.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'a'}\n",
      "Test #6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    24.73 ms /    43 runs   (    0.58 ms per token,  1738.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5295.28 ms /    43 runs   (  123.15 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =  5424.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'c'}\n",
      "Test #7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    88.94 ms /   154 runs   (    0.58 ms per token,  1731.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 19029.60 ms /   154 runs   (  123.57 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time = 19503.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =    45.42 ms /    79 runs   (    0.57 ms per token,  1739.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9668.12 ms /    79 runs   (  122.38 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =  9906.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "Test #9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3616.58 ms\n",
      "llama_print_timings:      sample time =     4.04 ms /     7 runs   (    0.58 ms per token,  1730.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   896.25 ms /     7 runs   (  128.04 ms per token,     7.81 tokens per second)\n",
      "llama_print_timings:       total time =   916.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'b'}\n",
      "**************************************************\n",
      "**************************************************\n",
      "Question 82: \n",
      "Language: english\n",
      "Question: \n",
      "The best alternative to including time-based prisms in eyeglass lenses for a -4.00 spherical diopters myopic patient with 4 prism diopters binocular diplopia is: a) Prescription of a lens with a neutral filter in one eye. b) Prescription of lenses with polarized filters in both eyes. c) Reduction of the distance between the optical centers of the lenses. d) Do not prescribe refraction in one eye to promote suppression of that eye.\n",
      "\n",
      "Test #0: \n"
     ]
    }
   ],
   "source": [
    "# Run evaluation:\n",
    "llm_language_evaluation(path=PATH, model=MODEL, temperature=TEMPERATURE, n_repetitions=N_REPETITIONS, reasoning=REASONING, languages=LANGUAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf99287",
   "metadata": {},
   "source": [
    "#### See the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72989a1d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "if N_REPETITIONS > 1:\n",
    "    df = pd.read_csv(f\"responses/{MODEL}_Temperature{str(TEMPERATURE).replace('.', '_')}_{N_REPETITIONS}Repetitions.csv\")\n",
    "else:\n",
    "    df = pd.read_csv(f\"responses/{MODEL}_Temperature{str(TEMPERATURE).replace('.', '_')}.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c09c0",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79913ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = str(TEMPERATURE).replace('.', '_')\n",
    "\n",
    "run_analysis(model=MODEL, temperature=TEMPERATURE, n_repetitions=N_REPETITIONS, languages=LANGUAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_REPETITIONS = 1 if N_REPETITIONS < 1 else N_REPETITIONS\n",
    "pd.read_csv(f'results/results_{MODEL}_Temperature{TEMPERATURE}_Repetitions{N_REPETITIONS}/matches_results_{MODEL}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9a317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_bias_vpython=3_8_15]",
   "language": "python",
   "name": "conda-env-nlp_bias_vpython_3_8_15-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
